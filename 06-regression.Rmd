# (PART) Regression Models {-}

# Simple and Multiple Regression

> *"You can lead a horse to water but you can't make him enter regional distribution codes in data field 97 to facilitate regression analysis on the back end."* 
>
>---John Cleese

## Learning objectives {#lo6}

- Identify and explain the components of a population or sample regression model
- Explain the difference between a deterministic equation of a line and a statistical, probabilistic equation of a line
- Given regression results, provide the predicted change in the outcome given a change in the explanatory variable(s)
- Given regression results, provide the predicted value of the outcome given a value of the explanatory variable(s)
- Explain what the error term in a regression model represents
- Interpret measures of fit in a regression model and explain their relative strengths and weaknesses

## Basic idea

The basic idea of regression is really quite simple. Regression calculates a line through a scatter plot of two variables so that we can summarize how much our variable on the y axis changes given a change in the variable on our x variable. Or, we can use a given value for our x variable to predict a value for our y variable. That's all it is--a line drawn to represent the association between two variables.

We all learned the equation for a line back in middle school, which probably looked something like the following:

\begin{equation}
y = mx + b
(\#eq:line)
\end{equation}

where $m$ is the slope of the line and $b$ is the y-intercept. If we know the slope and intercept for a line, then, given a value for $x$, we can compute $y$. Given a change in $x$, we can compute a change in $y$ by multiplying the change in $x$ by $m$. 

Consider the following equation for an arbitrary line:

$$y = 5x + 10$$

Here are some questions we can now answer:

- How much does $y$ change if $x$ increases by 1? Answer: 5
- How much does $y$ change if $x$ increases by 10? Answer: 50
- How much does $y$ change if $x$ decreases by 10? Answer: -50
- What does $y$ equal if $x$ equals 2? Answer: 20
- What would $y$ equal if $x$ were 0? Answer: 10

If you understand how to answer the above questions, then you can interpret regression results for any given context because interpreting regression results involves either predicting the *change* in $y$ given a *change* in $x$ or predicting the *value* of $y$ given a *value* of $x$.

What is different in regression is how the equation of the line is presented because there are population and sample versions of the relationship between $x$ and $y$. And, unlike a deterministic mathematical equation like the one above, because we generally use regression to measure relationships between social phenomena, there is inherent uncertainty in the line we calculate. This adds some complexity beyond solving a line's equation, but the process of running a regression to estimate the slope and intercept of a line to then predict changes or values of an outcome is fundamentally the same as the simple equation above.

## Simple linear regression

Equation \@ref(eq:simreg) presents the population regression model.

\begin{equation}
y=\beta_0+\beta_1x+\epsilon
(\#eq:simreg)
\end{equation}

Only one element differs between Equations \@ref(eq:line) and \@ref(eq:simreg). That is the symbol at the end, which is the Greek letter epsilon and is used to denote the aforementioned uncertainty of predicting real-world, particularly social, phenomena (more on that later).

The y-intercept denoted as $b$ in Equation \@ref(eq:line) has been moved to the front of the right-hand side in Equation \@ref(eq:simreg) and is denoted by $\beta_0$ (pronounced beta-naught). The slope denoted as $m$ in Equation \@ref(eq:line) is now denoted as $\beta_1$ in Equation \@ref(eq:simreg). These beta, $\beta$, symbols are simply the standard notation for **population parameters** in a statistical model and are used to signal that we intend to estimate these parameters using regression.

Recall that a parameter is a statistical measure of a population. In most cases, our research questions concern a population so large or inaccessible such that we do not observe all members. Instead, we take a sample of the population. From this sample, we calculate sample statistics, or **estimates** of the parameters and use methods of inference to decide if these estimates are valid guesses of the parameter (more on this later).

Equation \@ref(eq:simregsample) presents the sample regression equation.

\begin{equation}
\hat{y}=b_0+b_1x
(\#eq:simregsample)
\end{equation}

The carrot symbol atop our outcome variable $y$ is called a hat, and so the term on the left-hand side is commonly referred to as "y-hat." This is used to denote the fact that any value we calculate from Equation \@ref(eq:simregsample) is an *estimate* of what has been or will be observed. Similarly, the $b$ symbols are the sample estimate analogs of the $\beta$ population parameters in Equation \@ref(eq:simreg).

Equation \@ref(eq:simregsample) is the equation we use to interpret our regression results in the same way as was demonstrated using the mathematical equation of a line. Again, the only difference is that we are dealing with a statistical or probabilistic equation of a line--the outcome we calculate is a prediction based on observed data.

### Using regression

```{r importcounty, include=FALSE}
load('_bookdown_files/county_complete.rda')
```

Let's pause the theory to consider a simple example using data for U.S. counties. Table \@ref(tab:countydata) provides a preview of the data

```{r countydata, echo=FALSE}
selcounty <- county_complete %>% 
  select(name, state, fed_spend = fed_spending_2009, poverty = poverty_2010, homeownership = homeownership_2010, income = median_household_income_2010, pop2010) %>% 
  mutate(fed_spend = fed_spend/pop2010) %>% 
  filter(fed_spend < 50) %>% 
  select(-pop2010)

selcounty %>% 
  sample_n(7) %>% 
  kable(format = 'html', caption = 'Preview of county data')
```

where `fed_spending` is the amount of federal funds allocated to the county per capita, `poverty` is the percent of the population in poverty, `homeownership` is the percent of the population that owns a home, and `income` is per capita income. There are 3,143 observations in this dataset.

Suppose we wanted to examine the association between federal spending and poverty for U.S. counties such that poverty *explains* federal spending. After all, a substantial portion of federal dollars are dedicated to assist those in poverty. First, we might visualize the relationship between the two variables.

```{r povfedscatter, echo=FALSE, fig.cap='Federal spending and poverty among U.S. counties'}
ggplot(selcounty, aes(x = poverty, y = fed_spend)) +
  geom_point(color = 'steelblue', alpha = 0.4) +
  labs(y = 'Federal spending per capita',
       x = 'Percent population in poverty') +
  theme_minimal()
```

If we were to trace a line through these points, it would clearly slope upward. Thus, this suggests to us that as the percent of the population of a county increases, the amount of federal spending it receives increases. But by how much? That is precisely what regression estimates for us.

Equation \@ref(eq:simregexample) represents the relationship between federal spending and poverty using a simple linear regression population model. Note that we have chosen to model the two variables such that poverty explains or predicts federal spending. This aligns with the choice to plot poverty on the x axis and federal spending on the y axis in Figure \@ref(fig:povfedscatter). This is a critical choice in every regression and one that computers still need humans to help with (more on that later).

\begin{equation}
FedSpend = \beta_0+\beta_1Poverty + \epsilon
(\#eq:simregexample)
\end{equation}

We are going to use observed values of poverty and federal spending to estimate $\beta_0$ and $\beta_1$. Then, once we have those estimates, we can provide succinct answers regarding how federal spending tends to change given a change in poverty or a predicted level of federal spending given a particular level of poverty in a county. 

The $\epsilon$ represents all the other factors that explain or predict federal spending that are not in our model. If our world were such that the points in \@ref(fig:povfedscatter) literally formed a straight line, we would not need an $\epsilon$, but this is never the case with interesting questions of complex phenomena. This may or may not be a problem for whatever story we intend to tell about the the relationship between poverty and federal spending (more on that later).

Running the regression as represented in Equation \@ref(eq:simregexample) produces Table \@ref(tab:simpregextab) of results.

```{r simpregextab, echo=FALSE}
fedpov <- lm(fed_spend ~ poverty, data = selcounty)

get_regression_table(fedpov) %>% 
  kable(format = 'html', caption = 'Regression results of poverty on federal spending')
```

With the exception of Chapter @\ref(causation-and-bias), this section on regression models focuses on understanding the methods used to generate the values in the `estimate` column immediately to the right of the variable names as well as how to interpret and apply these values to any question. The remaining columns in the above table pertain to inference. The values in the `estimate` column are commonly referred to as **coefficients**, which were first mentioned in Chapter \@ref(descriptive-statistics). Regression coefficients measure the direction and magnitude of association between an explanatory variable and an outcome variable.

Now that we have our results, we can plug them into our sample regression equation like so

\begin{equation}
\hat{FedSpend}=7.95+0.11 \times Poverty
(\#eq:simregresults)
\end{equation}

and we are back to the first section of this chapter. Given a change in poverty we can predict the change in federal spending. Given any particular poverty level, we can predict a level of federal spending.

As for a standard template to interpret regression results, we generally say or write the following,

> On average, as the percent of the population in poverty increases by 1 percentage point, federal spending per capita increases approximately 11 cents.

A couple points about the above template:

- We always qualify using "on average" because that is exactly what regression does. Drawing a line through a scatter plot results in points above and below that line. The line drawn by regression traces how $y$ responds to $x$ *on average*.
- The standard change in $x$ to use when reporting results is one unit. Poverty is in units of percent. Therefore, a one-unit change in a variable measured in percentages is one percentage point (e.g. 10% to 11%).

However, we could report these results using any change or particular value in poverty germane to our original research question. For example, if we expected a county's poverty rate to be 30%, then we could report a predicted level of federal spending per capita equal to

```{r}
7.95+0.11*30
```

dollars per capita. Similarly, if we expected a decrease in the poverty rate of 5 percentage points, then we could predict a change in the level of federal spending per capita equal to 

```{r}
0.11*-5
```

dollars per capita.

### The error term

Back to theory. We need to address this $\epsilon$ that is present in the population regression model but disappears in the sample regression model and results. What gives?

The $\epsilon$ term is commonly referred to as the **error term** or, for those who don't like to insinuate some error was made in the regression, **statistical noise**. I prefer error term if for no other reason than to remind us to consider the myriad of errors we *may* be making in our regression model. 

As mentioned, the error term represents the inherent uncertainty of modeling an outcome based on a necessarily finite number of explanatory factors. Other factors affect our outcome. As a matter of principle, this does not prohibit our attempt to estimate the effect of a variable we care about or can alter with policy or programs on the outcome. 

Could we account for multiple factors (i.e. multiple regression)? Absolutely. Can we control for *everything* that affects our outcome? Definitely not if you subscribe to chaos theory or David Hume's thoughts on causality. Even if not so extreme as to say our world is too complex to ever make decisions concerning one variable's effect on another, the plausibility for us to collect data on every relevant factor is highly unlikely.

So, where did the error term go? It never really left; it simply is not used when calculating the predicted outcome based on our regression results. Like our $\beta$ terms, the error term is a population parameter. However, unlike the $\beta$s, we do not have observed data that corresponds to its estimation. In fact, the concept of the error term exists on the basis that we do not observe it. Therefore, it is necessarily excluded when we predict an outcome based on observed data, all the while we are careful to remind readers that the numbers we report are estimates subject to error. 

If the error term never left, where is it? Its sample analog exists as the difference between our estimated regression line and the observed data. Figure \@ref(fig:povfedscatter2) below is the same as Figure \@ref(fig:povfedscatter) except that it displays the line based on our regression results in Table \@ref(tab:simpregextab).

```{r povfedscatter2, echo=FALSE, message=FALSE, fig.cap='Federal spending and poverty among U.S. counties'}
ggplot(selcounty, aes(x = poverty, y = fed_spend)) +
  geom_point(color = 'steelblue', alpha = 0.4) +
  geom_smooth(method = 'lm', se = FALSE, linetype = 'dashed', color = 'black') +
  labs(y = 'Federal spending per capita',
       x = 'Percent population in poverty') +
  theme_minimal()
```

Surely, it is apparent that our regression line does not intersect all points perfectly; many points lie above and others below it. Pick any point in Figure \@ref(fig:povfedscatter2) and draw a vertical line to the regression line. The length of that line, which in this case is in units of dollars of federal spending per capita, and all other vertical distances between observed points and the regression line, is our sample version of the error term. 

Table \@ref(tab:simregexresid) quantifies the error for each of a select few observations.

```{r simregexresid, echo=FALSE}
get_regression_points(fedpov) %>% 
  sample_n(7) %>% 
  kable(format = 'html', caption = 'Comparing observed and predicted federal spending')
```

Our regression model uses observed values of poverty and federal spending to estimate the parameters of the regression line, which produced Equation \@ref(eq:simregresults). We can then plug the observed values of poverty into the equation to compute a predicted level of federal spending--`fed_spend_hat`. But, we know this prediction is not perfect in most cases. The right-most column of Table \@ref(tab:simregexresid) shows the difference between *predicted* federal spending and *observed* federal spending. This difference is called the **residual**.

Using the first row of Table \@ref(tab:simregexresid) as an example, our regression predicts a county with 16.9 percent poverty to receive 9.77 dollars per capita in federal spending. However, this county actually received 7.59 dollars per capita. Our regression over-estimates federal spending for this county by 2.18 dollars. Thus, the residual for this county is -2.18 because the observed outcome is 2.18 less than the estimated outcome. 

The residual is represented mathematically by Equation \@ref(eq:simregresid)

\begin{equation}
e = y - \hat{y}
(\#eq:simregresid)
\end{equation}

where $e$ is the sample analog of $\epsilon$. This is simply the equation behind the process of differencing the observed and predicted values of our outcome just described.

### Goodness of fit

Armed with an understanding of error and its sample analog, the residual, we can now consider goodness-of-fit. We must accept there will be error in our regression, but that does not mean we do not seek to minimize that error as much as possible. 

#### Assessing fit {-}

Table \@ref(tab:simregexfit) provides a standard set of three goodness-of-fit measures often used to assess regression.

```{r simregexfit, echo=FALSE}
get_regression_summaries(fedpov) %>% 
  select(r_squared, adj_r_squared, rmse) %>% 
  kable(format = 'html', caption = 'Goodness-of-fit measures')
```

The first column titled `r_squared` refers to the measure $R^2$, also known as the **coefficient of determination** defined in \@ref(descriptive-statistics). The $R^2$ measures the strength of association between a set of one or more explanatory variables and an outcome variable. Specifically, it quantifies the percent of total variation in the outcome explained by our regression model. In this case, our regression using poverty explains 2.1% of the total variation in federal spending.

The column titled `rmse` refers to **root mean squared error** (RMSE). The RMSE quantifies the typical deviation of the observed data points from the regression line and is particularly useful when predicting a value for our outcome. For example, if after predicting that a county with 30% poverty will receive 11.25 dollars of federal spending per capita, someone asks us how far off that prediction is likely to be, the RMSE suggests our prediction will tend to be off by plus or minus 4.65 dollars.

Regression involves choices. We choose which variables to use to explain or predict an outcome and how to model their effect on the outcome. This menu of choices will become increasingly evident as we build our regression toolbox. As we make choices, competing regression models emerge from which we must choose the one we prefer to report for decision-making.

The $R^2$ and RMSE provide us the basis for choosing our preferred model. In general, **we prefer the model with a _higher_ $R^2$ and/or a lower _RMSE_**. In virtually all cases, these two measures will agree with each other; the model with the higher $R^2$ will also have the lower RMSE.

#### Understanding fit {-}

```{block, type='announcement', purl=FALSE}
The material under this header is nonessential but is included because a deeper understanding of the mechanics of fit can be helpful.
```

Regression draws the *best* line through a set of data points of two or more variables. The best line in this case is the line with a slope and y-intercept that **minimizes the sum of squared residual** between the set of data points and said line. The procedure used to achieve such a line is called **ordinary least squares** (OLS). The type of regression covered in this book is sometimes referred to as OLS regression.

Recall that the variance of a variable is the sum of squared deviations from the mean, as depicted in Equation \@ref(eq:variance). This should sound familiar. Instead of deviations from the mean, fitting the best line in regression concerns the deviations from the regression line, which by definition are the residuals. As with variance, we square the deviations (i.e. residuals) for the data used to estimate the regression line, then we add these squared deviations together to obtain the sum of squared residual (SSR). Equation \@ref(eq:ssr) shows this process mathematically.

\begin{equation}
SSR=\sum _{i=1}^{n}(y_{i}-\hat{y})^2= (y_{1}-\hat{y})^2+(y_{2}-\hat{y})^2+\cdots +(y_{n}-\hat{y})^2
(\#eq:ssr)
\end{equation}

SSR quantifies the error in our regression and is what regression minimizes when predicting an outcome given the explanatory variables we have chosen to include.

The SSR also provides us what we need to compute the root mean squared error (RMSE). Recall that in order to compute the variance and standard deviation of a variable in Equations \@ref(eq:variance) and \@ref(eq:sd), respectively, we divide the sum of squared deviations by the number of observations (or $n-1$) then take the square root. The SSR is a sum of squared deviations. The deviations in this case represent error. If we divide SSR by the number of observations, we now have the mean of the sum of squared error. Then, if we take the square root, we have the root mean squared error. Note that this is the same process used to obtain the standard deviation. Thus, the RMSE is the regression version of a standard deviation. Just as the standard deviation tells us the average deviation from the mean, the RMSE tells us the average deviation from the regression line, or the average error in our regression.

We can also quantify the extent to which our regression *explains* the outcome. To do so, we need a benchmark against which to compare the reduction in error achieved by our regression. This benchmark is simply the average value of the outcome. If we had no explanatory variables to predict an outcome, the mean provides the typical value of the outcome. If we had to draw a random observation from a variable's distribution, the mean is our best guess of what that observation's value would be if we have no explanatory variables.

Figure \@ref(fig:povfedscatter3) adds a reference line of average federal spending to our scatter plot. Note that because average federal spending is a constant number, it does not change as poverty changes; the red line has no slope. Also, note that the red line does slightly worse fitting the data, particularly toward the left and right extremes of poverty. Compared to the red line representing the mean, the data appear to be more centered around our regression line. As a result, our regression line has less error than the mean.

```{r povfedscatter3, echo=FALSE, message=FALSE, fig.cap='Federal spending and poverty among U.S. counties'}
ggplot(selcounty, aes(x = poverty, y = fed_spend)) +
  geom_point(color = 'steelblue', alpha = 0.4) +
  geom_smooth(method = 'lm', se = FALSE, linetype = 'dashed', color = 'black') +
  geom_hline(yintercept = 9.63, color = 'red', linetype = 'dashed') +
  labs(y = 'Federal spending per capita',
       x = 'Percent population in poverty') +
  theme_minimal()
```

The mean of the outcome is our benchmark for assessing how much the explanatory variables included in our regression model explains the total variation in the outcome. The difference, if any, between the values our regression predicts, $\hat{y_i}$, and the mean, $\bar{y}$, serves as the basis for quantifying the extent to which our regression model explains the total variation in the outcome. Just like with the SSR, we square the difference between each predicted value and the mean, then add them together. The result is called the **sum of squared explained** (SSE) and is represented mathematically in Equation \@ref(eq:sse).

\begin{equation}
SSE=\sum _{i=1}^{n}(\hat{y}_{i}-\bar{y})^2= (\hat{y}_{1}-\bar{y})^2+(\hat{y}_{2}-\bar{y})^2+\cdots +(\hat{y}_{n}-\bar{y})^2
(\#eq:sse)
\end{equation}

We now have the sum of squared residuals (SSR) and sum of squared explained (SSE). Together, the SSR and SSE represent the **sum of squared total** (SST) variation in the outcome $y$.

\begin{equation}
SST = SSR + SSE
(\#eq:sst)
\end{equation}

Recall that the $R^2$ measures the percent of total variation in the outcome that is explained by our regression. To calculate any percent we take divide a proportion of the whole divided by the whole (e.g. $5/10 = 0.5$ or 50%). Thus, to obtain the percent of variation in the outcome explained by our regression, we divide the SSE by SST.

\begin{equation}
R^2 = {\frac{SSE}{SST}}
(\#eq:r2)
\end{equation}

The better you understand the mechanics of simple linear regression, the easier it will be to understand the next section and subsequent chapters on regression models because they are mere extensions of this basic model.

## Multiple regression

Of course, we are not limited to using only one variable to explain or predict an outcome. In fact, it is rather uncommon to use only one variable, but simple linear regression is useful for introducing the method of regression. Now, we can consider more realistic modeling method where we use multiple explanatory variables in our regression, which is aptly named multiple regression.

Equation \@ref(eq:multreg) provides the population model for multiple regression

\begin{equation}
y=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_kx_k+\epsilon
(\#eq:multreg)
\end{equation}

The only difference in this equation compared to Equation \@ref(eq:simreg) is the inclusion of multiple explanatory variables. Each explanatory is numbered and has a corresponding parameter $\beta$ representing the marginal effect it has on the outcome. In theory, we can add however many explanatory variables we deem worth including, represented by the arbitrary $k$. 

Equation \@ref(eq:multregsample) presents the sample equation for multiple regression.

\begin{equation}
\hat{y}=b_0+b_1x_1+b_2x_2+\cdots +b_kx_k
(\#eq:multregsample)
\end{equation}

Again, nothing is different from before except for more explanatory variables and sample estimates of the parameters.

### Using multiple regression

Let's return to our example of federal spending per capita in U.S. counties. Previously, we used only the percent of the population in poverty to explain or predict federal spending per capita. Let's add the percent of the population that owns a home and per capita income to our model. Thus, our model can be written as such

\begin{equation}
FedSpend = \beta_0 + \beta_1Poverty + \beta_2HomeOwn + \beta_3Income + \epsilon
(\#eq:multregex)
\end{equation}

which generates the following results

```{r multregextab, echo=FALSE}
fedpov2 <- lm(fed_spend ~ poverty + homeownership + income, data = selcounty)

get_regression_table(fedpov2) %>% 
  kable(format = 'html', caption = 'Multiple regression results')
```

and the following goodness-of-fit measures

```{r multregexfit, echo=FALSE}
get_regression_summaries(fedpov2) %>% 
  select(r_squared, adj_r_squared, rmse) %>% 
  kable(format = 'html', caption = 'Fit of multiple regression')
```

Now we can discuss what is different with multiple regression. First, note that our coefficient or estimate for poverty has changed from 0.108 to 0.105. A small difference to be sure, but that is specific to the example used; sometimes the estimate can change dramatically. Why the change? Because we are **controlling for other factors**. A slight amount of the marginal effect we reported poverty had on federal spending in our simple regression model was misattributed from the marginal effects of homeownership and/or income on federal spending.

This is a key feature of multiple regression:  it estimates the marginal effect of a variable on an outcome, holding all other explanatory variables equal to their respective means. In other words, if we were omnipotent beings who could take each county in our data and set homeownership and income to the mean of homeownership and income according to the observed data, then pull some lever that makes poverty change and nothing else, the estimate for poverty in our multiple regression reports how much each percentage point in poverty changes federal spending. This is how we isolate the effect of one variable on an outcome despite knowing other variables simultaneously affect our outcome.

The interpretation of multiple regression estimates is essentially the same as simple regression. In our example, we can interpret the homeownership estimate like so:

> On average, our results indicate that a one percentage point increase in the percent of the population that owns a home is associated with a decrease in federal spending per capita of approximately 9 cents, **holding other factors constant**.

The part in bold is to point out the small difference between the two interpretations. Here, we are simply reminding a reader that we have controlled for other factors that presumably we have already explained, and our estimate for poverty accounts for those factors by holding them constant. Other common word choices for this part of the interpretation include "all else equal" or its Latin translation "ceteris paribus."

Again, we can answer any sort of question relevant to our original research question concerning the predicted change or level of federal spending by plugging in the numbers to our regression equation.

\begin{equation}
\hat{FedSpend} = 13.50 + 0.105\times Poverty - 0.093\times HomeOwn + 0Income
\end{equation}

If we wanted to predict the change in federal spending given an 3 percentage point increase in poverty and a decline in home ownership of 4 percentage points, the our answer would be 

```{r}
0.105*3+(-0.093)*(-4)
```

dollars per capita (on average and all else equal, of course). If we wanted to predict the level of federal spending per capita for a county with 12% poverty, a 80% home ownership rate, and $31,000 income per capita, then we would predict

```{r}
13.50+0.105*12-0.093*80+0*31000
```

dollars per capita.

Not so fast! This example provides a good opportunity to consider another aspect of the units our variables are in. Per capita income is in dollars. This means the estimate for income represents the effect of a *one dollar* change in per capita income on federal spending per capita. That's a very small change that we would expect to have a very small effect on federal spending. This effect is so small that statistical software may round to 0. But what if we changed the units of income to *thousands* of dollars per capita instead of dollars per capita? Then we get the following results.

```{r, include=FALSE}
selcounty <- selcounty %>% 
  mutate(income = income/1000)
```

```{r, echo=FALSE}
fedpov3 <- lm(fed_spend ~ poverty + homeownership + income, data = selcounty)

get_regression_table(fedpov3) %>% 
  kable(format = 'html')
```

Now we see the effect of a *one thousand* dollar change in per capita income on federal spending per capita. Note that the estimates for poverty and homeownership are the same. Therefore, the predicted level of federal spending for our county is actually

```{r}
13.50+0.105*12-0.093*80+0.057*31
```

### Fit and adjusted R squared

In addition to doing a better job isolating the marginal effect of one variable on an outcome, including additional explanatory variables can reduce the error in our regression, thus achieve more accurate and/or precise predictions of the outcome. 

We can assess this improvement in fit by comparing the results in Table \@ref(tab:multregexfit) to those in Table \@ref(tab:simregexfit). We have gone from an RMSE of 4.65 dollars to an RMSE 4.59 dollars. This means our predictions from the multiple regression model tend to be off by 6 cents fewer than the predictions of our simple regression model.

The previous discussion on fit conspicuously skipped over the column titled `adj_r_square` because **adjusted-$R^2$** applies when comparing two or more models with a different number of explanatory variables. One caveat to using $R^2$ to choose a preferred model is that it mechanically increases as the number of explanatory variables increases whether those additional variables improve the extent to which our regression explains the total variation in the outcome or not. Therefore, it is unfair to compare a model with one explanatory variable to a model with more than one explanatory variable.

The adjusted-$R^2$ accounts for this unfairness by applying a penalty to each additional explanatory variable. We can fairly compare models with different numbers of explanatory variables using their respective adjusted-$R^2$. In our example, we have gone from explaining 2.1% of the total variation in federal spending to explaining 4.7% of its total variation. Adding home ownership and income has more than doubled the explanatory power of our model of federal spending.

### Explanatory penalty

Each explanatory variable we add to our regression model imposes a type of penalty on our results. Basically, for each explanatory variable included, we lose an observation in our data (not literally). This will be discussed further in the section on inference, but we need at least 33 observations to make valid inferences about a population based on sample estimates. If we had, say 50 observations in a dataset, and wanted to run a regression with 25 explanatory variables, then it is as though our regression model is based on only 25 observations (50 observations - 25 variables = 25 degrees of freedom). We will obtain results from such a model, but we should not use those results to make inferences.

In case you were wondering why not simply add all the variables we can to a model rather than carefully consider which variables to include and exclude in a model, this penalty is one of the primary reasons. Fewer degrees of freedom jeopardizes our ability to make valid inference. It can also reduce the precision of our predictions. The goal is to maximize the explanatory or predictive power of our regression model at minimal cost (i.e. excluding superfluous variables). Choosing good regression models is where subject matter expertise plays a crucial role. Experience and knowledge within the context of the research question informs our choices. Statistics is the method by which we apply our expertise to data to make evidence-based decisions.

<br>

> **To learn how to run regression in R, proceed to Chapter \@ref(r-regression).**

## Key terms and concepts {#kt6}

- Line concepts
  - y-intercept
  - slope
  - change in y versus value of y
- Regression model components
  - outcome/dependent/response variable
  - independent/explanatory variable
  - error term/statistical noise
  - residual
  - population parameter
  - sample coefficients/estimates
- Goodness of fit
  - R-squared
  - Adjusted R-squared
  - root mean squared error (RMSE)
- Controlling for other factors in multiple regression
