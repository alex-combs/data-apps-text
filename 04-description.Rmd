# Descriptive Statistics

> *"Just the facts, ma'am.*
>
>---Joe Friday, Dragnet

## Learning objectives {#lo4}

- Explain the difference between descriptive and inferential statistics
- Explain the difference between a population and sample; parameter and statistic
- Understand a distribution of a random variable
- Explain and apply the descriptive measures of center, spread, and association
- Choose the preferable measures of center and spread given a distribution and explain why
- Determine the direction and strength of association given a scatterplot or correlation coefficient
- Explain the possible shortcomings of correlation

## Two goals of statistics

The discipline of statistics has one or both of the following goals:

> 
- **Descriptive statistics:** summarizes the qualities of observed data in a sample or population, describing distributions of variables or the relationship between two or more variables.
- **Inferential statistics:** uses observed data in a sample to make inferences/conclusions about an unobserved population.

The descriptive or inferential statistics we produce or consume concern one or both of the following groups:

>
- **Population:** all members of a specified group pertaining to a research question
- **Sample:** a subset of that population

Descriptive statistics provides information about the data we have. Inferential statistics uses that information to make educated, scientific guesses about a larger group of people, places, or things we do not directly observe. In many cases, we cannot study an entire population because of logistics or cost. Instead, we take a sample of that population to make inferences about it. 

If my research question was, "What is the average GPA of all MPA students in the United States?", then the population is all MPA students in the United States. It is unlikely I could obtain the GPA of every MPA student. Therefore, I may take a sample of MPA students instead and use inferential statistics to make conclusions about the population of all MPA students.

Sometimes the population is small or accessible enough to observe, though it is probably uninteresting as a result. If my research question was instead, "What is the average GPA of students in my class?", then the population is the students in my class. In this case, I could easily compute the average for the entire population.

> We can describe a sample or a population as long as we have the data to do so. Inference is specifically using a sample we observe to describe a population we do not observe. 

When we compute statistical measures of a population or sample, those measures have specific names:

>
- **Parameter:** a measure pertaining to a population
- **Statistic:** a measure pertaining to a sample

Whether my statistical measure is a population parameter or sample statistic depends on whether my measure is computed from a population or a sample. If my population is students in my class, and I compute the average GPA for all of the students in my class, that measure is a population parameter. If my population is all MPA students in the U.S., and I use the students in my class as a sample, then the average GPA of the students in my class is a sample statistic. 

In inference, a sample statistic is often referred to as an **estimate** because it is used to estimate a population parameter. The parameter in this example would be the actual average GPA of all MPA students in the U.S.; a value I cannot directly calculate.

## Distributions

The goal of descriptive statistics is to summarize characteristics of variable distributions. Before reviewing the measures used to summarize distributions, we should understand what a distribution is.

> A **distribution** tells us the (possible) values of a variable and the frequency at which those values occur.

The values of a variable are the result of some random data-generating process. If it wasn't random, and instead deterministic, then there would be no uncertainty in the world. You do not know if you will get a job that requires your degree before you get the degree. An HR department does not know if an implicit bias workshop will reduce the number of racial insensitivity complaints before providing the workshop and measuring the number of complaints, nor does it know how many complaints occur at all before they are made. All of these are variables with some range of possible values, each of which occurs at some frequency. These frequencies are revealed to us when we take measures of the variable.

Sometimes we know all the possible values of a variable, or at least the range of possible values. We know a variable for biological sex has possible values of male or female. We know a variable for GPA has a possible range of 0 to 4, in most cases.

Sometimes we know what the frequency of values for a variable should be. Genetics tells us to expect 50% males and females. Most of the time we don't know the function that determines frequency, or it is too complex. For example, we have some idea of the factors that influence GPAs, but there will always be some randomness that goes unaccounted.

This somewhat esoteric exposition underlies the main focus here: the distribution of a variable. To make this as concrete as possible, let's consider a variable of something that is simple and familiar to all of us: a roll of a six-sided die.

We know a roll of a six-sided die can take on a range of integers between 1 and 6. We also know the frequency of each value is the same at 1 in 6, or about 17%. Therefore, we *know* the distribution of this variable, which is depicted below in Figure \@ref(fig:diedist).

```{r, include=FALSE}
diedist <- tibble(roll = c(1,2,3,4,5,6), value = c(1,2,3,4,5,6), dieprob = c(0.166,0.166,0.166,0.166,0.166,0.166))
```


```{r diedist, echo=FALSE, fig.cap='Probability distribution of a six-sided die'}
ggplot(diedist, aes(value, dieprob)) +
  geom_col(fill = 'steelblue') +
  labs(y = 'Probability') +
  scale_x_continuous(breaks = c(1,2,3,4,5,6)) +
  theme_classic() +
  theme(axis.title.x = element_blank())
```


Therefore, if we were to roll the die six times, we would *expect* the following data in Table \@ref(tab:dietable), though not necessarily in this order.

```{r dietable, echo=FALSE}
diedist %>% 
  select(-dieprob) %>% 
  kable(caption = 'Expected results of 6 rolls')
```

And we could represent this distribution by counting the number of times each value occurred using a histogram as shown in Figure \@ref(fig:diehisto), which is the essentially the samce as Figure \@ref(fig:diedist).

```{r diehisto, echo=FALSE, fig.cap='Expected distribution of 6 rolls'}
ggplot(diedist, aes(value)) +
  geom_histogram(fill = 'steelblue', bins = 6, color = 'white') +
  labs(y = 'Count') +
  scale_x_continuous(breaks = c(1,2,3,4,5,6)) +
  scale_y_continuous(breaks = c(0,1)) +
  theme_classic() +
  theme(axis.title.x = element_blank())
```

Of course, this is just what is expected to happen, on average, given many rolls of a die. Anyone who has played a board game knows streaks can occur. Given a number or rolls, we probably will not observe a uniform number of values.

Suppose we roll 12 times and record the value of each roll, as is shown in Table \@ref(tab:obsdietable). 

```{r, include=FALSE}
set.seed(551)
dierolls <- tibble(roll = 1:12, value = round(runif(12, 1, 6),0))
```

```{r obsdietable, echo=FALSE}
dierolls %>% 
  kable(caption = 'Observed results of 12 rolls')
```

We can visualize the distribution of these 12 rolls, as is done in Figure \@ref(fig:obsdieroll).

```{r obsdieroll, echo=FALSE, fig.cap='Observed distribution of 12 die rolls'}
ggplot(dierolls, aes(value)) +
  geom_bar(fill = 'steelblue', color = 'white') +
  labs(y = 'Count') +
  scale_x_continuous(breaks = c(1,2,3,4,5,6)) +
  theme_classic() +
  theme(axis.title.x = element_blank())
```

Here we can see the randomness of the variable. Values 1, 2, and 5 occur more frequently than 3 and 4, and 6 does not occur at all. If we were to roll the die many more times, it would look more like the distribution we would expect. But for *this* sample of die rolls, the distribution is unique.

This is exactly the point of descriptive statistics: whether or not we know what to expect in terms of a variable's distribution, we want to know the characteristics of the distribution for a variable from a particular sample or population. When we ask for, say, a variable's average, we are asking for the approximate midpoint of that variable's distribution. 

Descriptive measures help us summarize characteristics of distributions and some serve as the building blocks for other descriptive measures as well as inferential statistics.

## Descriptive Measures

A die roll is uninteresting and unimportant. In our review of descriptive measures, let us consider them with respect to the distribution of the infant mortality rate across 222 countries in 2012. Infant mortality is the number deaths of infants under one year old per 1,000 live births. It is used as a measure of health in a country.

```{r infmort, echo=FALSE, fig.cap='Infant Mortality Rates'}
ggplot(infmortrate, aes(inf_mort_rate)) +
  geom_histogram(fill = 'steelblue', color = 'white', bins = 30) +
  labs(x = 'Infant deaths per 1,000 live births') +
  theme_classic() 
```

We could simply show the entire distribution of values, but it is usually helpful to summarize key characteristics of it. We can describe distributions along three dimensions:

>
- **Center:** what is the typical value of this variable?
- **Spread:** how far away are values typically from the center?
- **Association:** what is the typical value or spread of the distribution given a value of another variable?

Multiple descriptive measures can answer the three questions above. Which measure is more appropriate to use largely depends on the shape of the distribution.

### Measures of center

#### Mean {-}

The mean or average takes the values of a variable, adds them, then divides that sum by the total count of values.

\begin{equation}
{\displaystyle \bar{x}={\frac {1}{n}}\sum _{i=1}^{n}x_{i}={\frac {x_{1}+x_{2}+\cdots +x_{n}}{n}}}
(\#eq:mean)
\end{equation}

Infant mortality has 222 values, so $n$ in equation \@ref(eq:mean) above would equal 222 in this case. If we were to pluck one country out of our pool of 222 countries at random, the mean tells us the infant mortality rate to expect. In other words, the mean tells us the typical infant mortality rate in our observed data. In this case, the average infant mortality rate is `r round(mean(infmortrate$inf_mort_rate),1)` per 1,000 live births.

#### Median {-}

If we took the 222 infant mortality rates and listed them in ascending or descending numerical order, the median is the value that sits at the middle of the ordered list. The median is also referred to as the 50th percentile because half of the values fall below it and half of the values fall above it. In the case of an even number of values, there is no naturally occurring middle value. In that case, we take the average of the two values in the middle. The median infant mortality rate is `r round(median(infmortrate$inf_mort_rate),1)`.

#### Mode {-}

```{r, include=FALSE}
mode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
```

The mode is the value that occurs most frequently. If all values occur only once, then a variable has no mode. If two or more values occur an equal number of times and it is more than other values, then a variable has two or more modes. For instance, the modes for our 12 die rolls in Figure \@ref(fig:obsdieroll) are 1, 2, and 5. The mode for infant mortality rates is 11.6.

#### Choosing a center {-}

As Figure \@ref(fig:infmort) shows, three measures of center have provided us three different typical infant mortality rates. The mean is represented by the red line, the median by the purple line, and the mode by the green line. 

```{r mortcenters, echo=FALSE, fig.cap='Centers of infant mortality rates'}
ggplot(infmortrate, aes(inf_mort_rate)) +
  geom_histogram(fill = 'steelblue', color = 'white', bins = 30) +
  geom_vline(xintercept = 26.7, color = 'red', linetype = 'dashed') +
  geom_vline(xintercept = 15.6, color = 'purple', linetype = 'dashed') +
  geom_vline(xintercept = 11.6, color = 'green', linetype = 'dashed') +
  labs(x = 'Infant deaths per 1,000 live births') +
  theme_classic() 
```

Which should we choose to report? We could report all three, but we should apply some professional judgment to this dilemma. After all, people generally do not like nuance and prefer one best number instead of deciphering the meaning from three, assuming they understand all three measures in the first place.

For continuous variables reported at several decimal places, a value may not occur more than once because of the precision. More importantly, because of this low probability of repeat values, the mode is not guaranteed to represent a typical value. If it only takes two occurrences to qualify as the mode, that second occurrence could be an extreme value. 

> The mode is rarely used to describe continuous variables. The mode is commonly used to report the most frequent value of categorical or discrete variables with relatively few possible values, such as race, sex, political party, or number of TVs in a household.

When choosing between mean and median, it usually comes down to the presence of extreme values or the extent to which a distribution is **skewed**. Skew pertains to the tails of a distribution--the taper to the left and right of its center. If the right or left tail extends far out from the center, we consider the distribution to be right- or left-skewed. Our distribution of infant mortality rates is right-skewed.

> When a distribution is skewed, the median is generally a better choice for reporting its center than the mean. This is because the mean is sensitive to extreme values. 

Note in Figure \@ref(fig:mortcenters) that the mean is pulled to the right by the right-skew of extreme values. The red line representing the mean is to the right of the cluster of frequent values and may not be a good answer for the typical value of this distribution. The median is not sensitive to extreme values. No matter how far the values above the median were to stretch to the right, the median of the distribution would not change.

### Measures of spread

Measures of center convey the typical value of a distribution. The typical infant mortality rate is `r round(mean(infmortrate$inf_mort_rate),1)` or `r round(median(infmortrate$inf_mort_rate),1)` depending on whether we choose to use mean or median, respectively. If we only had this measure, we would have no idea how far away the values are from the center. Are the infant mortality rates of most countries close to this center, or is the typical value not representative of most countries' infant mortality rates? Measures of spread provide us this information.

#### Variance {-}

Almost all values of a numerical variable, especially a continuous variable, do not equal the mean. The difference between a particular value and the mean of the variable is often referred to as **deviation from the mean**. The variance squares each observation's deviation from the mean, sums all the deviations, and divides this sum by the total count of observations minus one. Equation \@ref(eq:variance) displays this process using mathematical notation.

\begin{equation}
{\displaystyle S^2={\frac {1}{n-1}}\sum _{i=1}^{n}(x_{i}-\bar{x})^2={\frac {(x_{1}-\bar{x})^2+(x_{2}-\bar{x})^2+\cdots +(x_{n}-\bar{x})^2}{n-1}}}
(\#eq:variance)
\end{equation}

The mean infant mortality rate is 26.7. If we subtract this mean from each country's rate, we have each country's deviation from the mean, some of which is shown in Table \@ref(tab:varcalcs). Then, we square these deviations as is also shown in the table. We then sum the 222 squared deviations and divide by 221. The variance for our infant mortality rates is `r round(var(infmortrate$inf_mort_rate),1)`.

```{r varcalcs, echo=FALSE}
infmortrate %>% 
  head(n=5) %>% 
  mutate(deviate = inf_mort_rate - 26.7, sq_deviate = deviate^2) %>% 
  kable(caption = 'Excerpt of variance calculations')
```

#### Standard deviation {-}

Variance is an important building block for inference, but it is essentially useless as a descriptive measure because it is in squared units. If someone asks how far values are spread out from the mean, it would not help much to report values deviate from the mean by 672 squared-deaths.

The standard deviation is simply the square root of variance, which returns our units to their original meaning.

\begin{equation}
{\displaystyle s = \sqrt{S^2}}
(\#eq:sd)
\end{equation}

The standard deviation in the infant mortality rates data is `r round(sd(infmortrate$inf_mort_rate),1)`. This tells us that, on average, infant mortality rates are about 26 deaths above or below the mean.

> The inuition of the mean and standard deviation is useful for understanding probability and inferential statistics. The mean is the typical value of a variable. The standard deviation is the typical deviation from the mean. If we had to guess a value drawn randomly from a variable, our best guess is the mean as long as the variable is not highly skewed. If we had to guess how far off that randomly drawn value will be from the mean, our best guess is the standard deviation.

#### Interquartile range {-}

Recall that the median is the 50th percentile of a distribution--half of the values fall below the median and half fall above it. Two additional percentiles sometimes reported are the 75th and 25th percentiles. The 75th percentile is the value at which 75% of values fall below and 25% fall above it, while the 25th percentile is the value at which 25% of values fall below and 75% fall above it. 

The IQR is equal to the 75th percentile minus the 25th percentile, thus providing the range that captures the middle 50% of the values in the distribution. The IQR for infant mortality rates is `r round(IQR(infmortrate$inf_mort_rate),1)`. Alternatively, the IQR can be reported by specifying the 75th and 25th percentiles, leaving the consumer to compute the difference between the two. The 75th percentile for infant mortality rates is 42.1, while the 25th percentile is 6.5. 

#### Range {-}

The range is the maximum value in a distribution minus the minimum value of a distribution. Usually, the range is left implied in a table of summary statistics by reporting the maximum and minimum without differencing the two. The minimum of infant mortality rates is `r min(infmortrate$inf_mort_rate)`, and the maximum is `r max(infmortrate$inf_mort_rate)`. Therefore, the range of the distribution is `r round(max(infmortrate$inf_mort_rate)-min(infmortrate$inf_mort_rate),1)`.

#### Choosing a measure of spread {-}

The same logic applies to choosing a measure of spread as choosing a measure of center. The standard deviation is based on the mean, and so it is also sensitive to extreme values that, if present, could exaggerate the typical spread of the distribution. The IQR is based on percentiles just like the median. Therefore, IQR is not sensitive to extreme values.

Figure \@ref(fig:mortspread) displays the mean and plus-and-minus one standard deviation using red dashed and solid lines, respectively. The median and the IQR (25th and 75th percentiles) are represented by the purple dashed and solid lines, respectively. Note how wide the area contained by the standard deviation is--it contains most of the distribution and the lower bound of 0.7 is lower than the minimum observed value of `r min(infmortrate$inf_mort_rate)`. 

Standard deviation is arguably not good for conveying the typical deviation from the center, as it contains plenty of values that are rather atypical deviations from the center. In the case of describing the distribution of infant mortality rates, the median and IQR are probably a better choice.

```{r mortspread, echo=FALSE, fig.cap='Center and spread of infant mortality rates'}
ggplot(infmortrate, aes(inf_mort_rate)) +
  geom_histogram(fill = 'steelblue', color = 'white', bins = 30) +
  geom_vline(xintercept = 26.7, color = 'red', linetype = 'dashed') +
  geom_vline(xintercept = 0.7, color = 'red') +
  geom_vline(xintercept = 52.7, color = 'red') +
  geom_vline(xintercept = 15.6, color = 'purple', linetype = 'dashed') +
  geom_vline(xintercept = 6.5, color = 'purple') +
  geom_vline(xintercept = 42.1, color = 'purple') +
  labs(x = 'Infant deaths per 1,000 live births') +
  theme_classic() 
```

In most cases, the range (or minimum and maximum values) should be reported along with either the mean and standard deviation or median and IQR (or 25th and 75th percentiles), especially when a distribution is skewed. In addition to signaling the skew of a distribution, the range helps convey what may be the possible values of a variable and how different the most different units in the data are with respect to that variable.

In the case of infant mortality rates, we know the minimum possible value is 0 by definition, but the minimum value in our distribution is `r min(infmortrate$inf_mort_rate)`. Perhaps 0 deaths is unrealistic for any country. Moreover, the maximum value is `r max(infmortrate$inf_mort_rate)`. This range, along with the median and IQR, tells us the most different countries are *very* different.

### Outliers

Outliers refer to weird observations in our data whose inclusion may affect the conclusions we draw from our analysis. There are at least two points about outliers that can cause confusion and mistakes:

- There is no definitive threshold at which an observation can becomes an outlier. 
- Outliers should not necessarily be excluded from an analysis. It depends on the context. We may only care about making conclusions for typical cases. If so, removing outliers may be warranted. However, atypical cases may be an important part of the story.

While there is no single definition of an outlier, the most common definition uses $1.5 \times IQR$. Values that are more than $1.5 \times IQR$ below the 25th percentile are outliers on the left side of the distribution. Values that are more than $1.5 \times IQR$ above the 75th percentile are outliers on the right side of the distribution. 

Recall that the IQR of infant mortality rates depicted in Figure \@ref(fig:mortspread) was `r round(IQR(infmortrate$inf_mort_rate),1)` with 25th and 75th percentiles of 6.5 and 42.1, respectively. Applying the above definition, 1.5 times `r round(IQR(infmortrate$inf_mort_rate),1)` equals `r round(1.5*IQR(infmortrate$inf_mort_rate),1)`. Therefore, values that fall below -46.9 ( $6.5 - 53.4$ ) are outliers, but this is impossible given our variable is infant mortality rates. Values above 95.5 ( $42.1 + 53.4$ ) are outliers on the right end of the distribution. Figure \@ref(fig:mortspread) indicates there are a few outliers in our data based on this definition.

### The normal distribution

As a brief aside, it should be mentioned that if a distribution is **normal**, then measures of center and spread will be similar to each other. This is one of several desirable features of the normal distribution. 

Figure \@ref(fig:normrates) shows a simulated scenario in which the infant mortality rates in our 222 countries exhibit a normal distribution. Note the peaks in the center and symmetry. There is no skew. 

```{r normrates, echo=FALSE, fig.cap='Simulated normal distribution of infant mortality rates'}
set.seed(123)
normdf <- tibble(rates = rnorm(222, 26, 7))

ggplot(normdf, aes(rates)) +
  geom_histogram(fill = 'steelblue', color = 'white', bins = 30) +
  labs(x = 'Simulated infant deaths per 1,000 live births') +
  theme_classic() 
```

Table \@ref(tab:normratesum) confirms the similarity between measures of center and spread for this simulated distribution. This is one reason it is important to visualize your distribution. If it appears approximately normal, then you should report the mean and standard deviation (along with minimum and maximum values), as the are more widely understood.

```{r normratesum, echo=FALSE}
normdf %>% 
  summarise(Mean = round(mean(rates),1), Median = round(median(rates),1), Mode = round(mode(rates),1), SD = round(sd(rates),1), IQR = round(IQR(rates),1)) %>% 
  kable(caption = 'Center and spread measures of simulated data')
```

Again, the normal distribution has several desirable features that will be discussed further in the chapters pertaining to inference. One is that if a distribution is approximately normal, then extreme values are not a concern and the mean and standard deviation are good measures of center and spread, respectively. Besides making our choice of measures convenient, why is this worth repeating? Because the mean and standard deviation are building blocks to the next category of descriptive measures: association. If mean and standard deviation are bad choices of center and spread, then our measures of association will be negatively affected.

### Measures of association

```{r, include=FALSE}
gap07 <- gapminder %>% 
  filter(year==2007) %>% 
  select(country, lifeExp, gdpPercap)

gapdeath <- left_join(infmortrate, gap07, by = 'country')
```

With association, we now consider the distributions of two variables at a time. That is, given the value within one variable's distribution, what does the distribution of another variable look like?

We need a second variable to continue our example involving infant mortality rates. Table \@ref(tab:gapdeathtab) shows a preview of a dataset that adds two more variables to our previous infant mortality data.

```{r gapdeathtab, echo=FALSE}
gapdeath %>% 
  head(n=5) %>% 
  kable(caption = 'First five rows of country data')
```

Recalling that the mean infant mortality rate is about 26, the five countries included are in the right tail of the distribution. Also, you probably know enough about life expectancy to know that the values for these countries are quite low. Perhaps these two variables share a relationship?

In fact, we know they do. Life expectancy in a given year is the average age at which people in that country died. If a country has a high frequency of infants dying, then that will pull the mean downward. A common misunderstanding of life expectancy is that people in that country tend to die at the age of the country's life expectancy. This is certainly not the case if a country has a high infant mortality rate. While adults in countries with low life expectancy may die somewhat younger (or much younger if it is a war-torn country), adults tend to live longer than the average life expectancy. The key is making it out of infancy alive.

#### Visual association {-}

As was the case with one variable, we want to visualize the distributions of two variables. When working with two continuous variables, the **scatter plot** is the most common choice to visualize association between two variables. Figure \@ref(fig:scatterlifedeath) plots the paired values of infant mortality rate and life expectancy for each country.

```{r scatterlifedeath, echo=FALSE, warning=FALSE, fig.cap='Visualizing association between two continuous variables'}
ggplot(gapdeath, aes(x = inf_mort_rate, y = lifeExp)) +
  geom_point(color = 'steelblue', size = 2, alpha = 0.8) +
  labs(title = 'Infant mortality rates and life expectancy',
       x = 'Infant mortality rate',
       y = 'Life expectancy') +
  theme_classic()
```

Note that I plotted infant mortality rate along the x axis and life expectancy on the y axis. This choice was deliberate. If we suspect that one variable influences or affects the value of another variable, then the variable doing the influencing should be plotted on the x axis. Plotting a variable on the y axis implies to the viewer that it responds to the variable on the x axis.

Figure \@ref(fig:scatterlifedeath) confirms our suspicion that the two variables are associated. There appears to be a rather strong association such that as infant mortality rate increases, the lower a country's life expectancy.

#### Quantified association {-}

As was the case with one variable, we want to describe the association between two variables using quantitative measures. The association between two or more variables can be described in terms of

- **Direction:** when one variable increases, does the other variable increase or decrease?
- **Strength:** how much do the variables seem to be tied together?
- **Magnitude:** given an specific increase or decrease in one variable, by how much does the other variable increase or decrease?

There are several measures one can use to answer the above question.

>
- **Covariance:** measures direction of association between between two variables
- **Correlation coefficient:** measures direction and strength of association between two variables
- **Regression coefficient:** measures the direction and magnitude of association between an explanatory variable and an outcome variable
- **Coefficient of determination ( $R^2$ ):** measures the strength of association between a set of one or more explanatory variables and an outcome variable

The regression coefficient and coefficient of determination are discussed in Chapter \@ref(simple-and-multiple-regression) involving regression models. Let us briefly consider covariance and correlation.

#### Covariance {-}

Covariance tells us when one variable, $X$, is above or below its mean, whether another variable, $Y$, tends to be above or below its mean. If $Y$ tends to be above (below) its mean when $X$ is above (below) its mean, then the two have a positive covariance and are positively associated. If $Y$ tends to be below (above) its mean when $X$ is above (below) its mean, then the two have a negative covariance and are negatively associated. If the two variables exhibit no tendencies, they have a covariance of 0 and thus no association.

Figure \@ref(fig:scattercovar) adds references lines for the mean of each variable. Note that when infant mortality is above its mean (to the right of the red line), life expectancy is below its mean (below the purple line) in almost all cases. When infant mortality is below its mean, life expectancy is above its mean in almost all cases. Therefore, these two variables have a negative covariance and are negatively associated. In fact, the covariance between infant mortality rate and life expectancy is `r round(cov(gapdeath$inf_mort_rate, gapdeath$lifeExp, use='pairwise.complete.obs'),1)`

```{r scattercovar, echo=FALSE, warning=FALSE, fig.cap='Visualizing covariance'}
ggplot(gapdeath, aes(x = inf_mort_rate, y = lifeExp)) +
  geom_point(color = 'steelblue', size = 2, alpha = 0.8) +
  geom_vline(xintercept = 26.7, color = 'red', linetype = 'dashed') +
  geom_hline(yintercept = 66.9, color = 'purple', linetype = 'dashed') +
  labs(title = 'Infant mortality rates and life expectancy',
       x = 'Infant mortality rate',
       y = 'Life expectancy') +
  theme_classic()
```

Covariance is the association analog of variance. It is an important building block of other measures of association, but it is essentially useless for description because it only tells us the direction of association. Correlation tells us the direction and strength of association. Therefore, covariance is never used for description because correlation provides us twice as much information.

#### Correlation {-}

If $Y$ tends to increase (decrease) as $X$ increases (decreases), then the two are positively correlated. That is, the two variables tend to move in the same direction. If $Y$ tends to increase (decrease) as $X$ decreases (increases), then the two are negatively correlated. That is, the two variables tend to move in opposite directions. Correlation tells us how much the paired values of two variables in a scatterplot exhibit a straight line and whether that straight line is positively or negatively sloped. 

The correlation coefficient ranges between -1 and 1. If it is negative, then the two variables are negatively associated. If it is positive, the two variables are positively associated. The closer the correlation coefficient of two variables is to -1 or 1, the stronger their correlation and the more the two variables exhibit a straight line in a scatterplot. A correlation equal to -1 or 1 indicates the two variables form a perfect straight line. If two variables exhibit no shared tendencies and form what appears to be a random scattering of plot points, than their correlation will be close or equal to 0.

Based on the covariance and Figure \@ref(fig:scattercovar), we know to expect a negative correlation between infant mortality rate and life expectancy. We also know the correlation will not be -1 because the points do not form a perfect straight line. Nevertheless, they do form a fairly tight downward path, so we should expect a correlation closer to -1 than 0. It turns out that the correlation is equal to `r round(cor(gapdeath$inf_mort_rate, gapdeath$lifeExp, use='pairwise.complete.obs'),1)`. Infant mortality rate and life expectancy exhibit a strong, negative association.

If we imagined drawing a line through the data points on our scatterplot from left to right that could freely curve according to however the data are scattered, would that line be a straight line and would it slope upward or downward? Figure \@ref(fig:scattercorr) does exactly that with our data. Note that the data points lead the line to slope downward almost throughout the range of observed values. In the upper-left quadrant, the data points are tightly clustered around the line, indicating a strong correlation. In the bottom-right quadrant, the data points begin to spread further away from the line, indicating a weaker correlation. The line also begins to turn in the positive direction, which lowers the correlation coefficient. 

```{r scattercorr, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Drawing a free line through the data'}
ggplot(gapdeath, aes(x = inf_mort_rate, y = lifeExp)) +
  geom_point(color = 'steelblue', size = 2, alpha = 0.8) +
  geom_vline(xintercept = 26.7, color = 'red', linetype = 'dashed') +
  geom_hline(yintercept = 66.9, color = 'purple', linetype = 'dashed') +
  geom_smooth(se = FALSE, color = 'black', linetype = 'dashed') +
  labs(title = 'Infant mortality rates and life expectancy',
       x = 'Infant mortality rate',
       y = 'Life expectancy') +
  theme_classic()
```

The correlation coefficient has three qualities that can lead to misunderstandings or mistakes. First, **correlation is sensitive to extreme values**. A few points on a scatterplot can impose undue influence on the line that is drawn through the data, causing the correlation coefficient to increase or decrease dramatically. Second, **correlation measures only the linear association**. If two variables formed a perfect U-shape in a scatterplot, they are strongly associated. However, their correlation coefficient would suggest a weaker relationship because a straight line does not fit a U-shape well. Third, **correlation is a necessary but not sufficient condition for causality**. In order to validly claim that a change in the value of one variable *causes* the values of another variable to change, they must be correlated, but a few more conditions must also be met. Those conditions are discussed in Chapter \@ref(causation-and-bias).

<br>

> **To learn how to produce a summary table for a publication, proceed to Chapter \@ref(r-description).**

## Key terms and concepts {#kt4}

- Descriptive statistics
- Inferential statistics
- Population
- Sample
- Parameter
- Statistic
- Estimate
- Distribution
- Mean
- Median
- Mode
- Skewed distribution
- Standard deviation
- Interquartile range
- Range
- Correlation
