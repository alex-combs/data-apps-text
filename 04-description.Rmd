# Descriptive Statistics

> *"Just the facts, ma'am.*
>
>---Joe Friday, Dragnet

## Learning objectives {#lo4}

```{block, type='learncheck', purl=FALSE}
- Explain how descriptive and inferential statistics differ in purpose and the information they provide
- Identify and differentiate population, sample, parameter, statistic in a given research proposal or question
- Explain what is a distribution of a random variable
- Recall the descriptive measures of center, spread, and association
- Choose the preferable measures of center and spread given a distribution and explain their strengths and weaknesses
- Determine the direction and strength of association given a scatterplot or correlation coefficient
- Explain the possible shortcomings of correlation
```

## Two kinds of statistics

There are two kinds of statistics, each having a specific goal:

- **Descriptive statistics** summarizes the qualities of observed data, typically describing distributions of variables or the relationship between two variables.

- **Inferential statistics** uses observed data in a sample to make inferences/conclusions about an unobserved population.

Descriptive statistics concerns just the facts. Inferential statistics uses those facts to make educated, scientific guesses about a group of people, places, or things for which we don't have data.

The above definitions include some terms that warrant further explanation. 

- **Population:** all members of a specified group pertaining to a research question
- **Sample:** a subset of that population

In many cases, we cannot study an entire population because of logistics or cost. Instead, we take a sample of that population to make inferences about it.

Sometimes our population is small or accessible enough to observe. If I wanted to know the average GPA of students in my class, my population is the students in my class, and I could compute the exact average for the entire population. Or if I worked in HR for an agency and wanted to know the racial diversity of a department, I could calculate percentages of each race for the population.

We can describe a sample or a population. Inference is specifically using a sample to describe a population. When we compute measures of a population or sample, these measures have specific names:

- **Parameter:** a measure pertaining to a population
- **Statistic:** a measure pertaining to a sample

If my population is students in my class, and I compute the average GPA for all of the students in my class, that measure is a population parameter. If my population is all students at a university, and I use the students in my class as a sample, then the average GPA of the students in my class is a sample statistic. In inference, a statistic is often referred to as an **estimate** because it is used to estimate a population parameter. The parameter in this example would be the average GPA of all students at the university.

## Distributions

The goal of descriptive statistics is to summarize characteristics of variable distributions. Before reviewing the measures used to summarize distributions, we should understand what a distribution is.

A **distribution** tells us the (possible) values of a variable and the frequency at which those values occur.

The values of a variable are the result of some random data-generating process. If it wasn't random, and instead deterministic, then there would be no uncertainty in the world. You do not know if you will get a job that requires your degree before you get the degree. An HR department does not know if an implicit bias workshop will reduce the number of racial insensitivity complaints before providing the workshop and measuring the number of complaints, nor does it know how many complaints occur at all before they are made. All of these are variables with some range of possible values, each of which occurs at some frequency. These frequencies are revealed to us when we take measures of the variable.

Sometimes we know all the possible values of a variable, or at least the range of possible values. We know a variable for biological sex has possible values of male or female. We know a variable for GPA has a possible range of 0 to 4, in most cases.

Sometimes we know what the frequency of values for a variable should be. Genetics tells us to expect 50% males and females. Most of the time we don't know the function that determines frequency, or it is too complex. For example, we have some idea of the factors that influence GPAs, but there will always be some randomness that goes unaccounted.

This somewhat esoteric exposition underlies the main focus here: the distribution of a variable. To make this as concrete as possible, let's consider a variable of something that is simple and familiar to all of us: a roll of a six-sided die.

We know a roll of a six-sided die can take on a range of integers between 1 and 6. We also know the frequency of each value is the same at 1 in 6, or about 17%. Therefore, we *know* the distribution of this variable, which is depicted below in Figure 4.1.

```{r, include=FALSE}
diedist <- tibble(roll = c(1,2,3,4,5,6), value = c(1,2,3,4,5,6), dieprob = c(0.166,0.166,0.166,0.166,0.166,0.166))
```


```{r, echo=FALSE, fig.cap='Probability distribution of a six-sided die'}
ggplot(diedist, aes(value, dieprob)) +
  geom_col(fill = 'steelblue') +
  labs(y = 'Probability') +
  scale_x_continuous(breaks = c(1,2,3,4,5,6)) +
  theme_classic() +
  theme(axis.title.x = element_blank())
```


Therefore, if we were to roll the die six times, we would *expect* the following data in Table 4.1.

```{r, echo=FALSE}
diedist %>% 
  select(-dieprob) %>% 
  kable(format = 'html', caption = 'Expected results of 6 rolls')
```

And we could represent this distribution by counting the number of times each value occurred using a histogram as shown in Figure 4.2.

```{r, echo=FALSE, fig.cap='Expected distribution of 6 rolls'}
ggplot(diedist, aes(value)) +
  geom_histogram(fill = 'steelblue', bins = 6, color = 'white') +
  labs(y = 'Count') +
  scale_x_continuous(breaks = c(1,2,3,4,5,6)) +
  scale_y_continuous(breaks = c(0,1)) +
  theme_classic() +
  theme(axis.title.x = element_blank())
```

Of course, this is just what is expected to happen, on average, given many rolls of a die. Anyone who has played a board game knows streaks can occur. Given a number or rolls, we probably will not observe a uniform number of values.

Suppose we roll 12 times and record the value of each roll, as is shown in Table 4.2. 

```{r, include=FALSE}
set.seed(551)
dierolls <- tibble(roll = 1:12, value = round(runif(12, 1, 6),0))
```

```{r, echo=FALSE}
dierolls %>% 
  kable(format = 'html', caption = 'Observed results of 12 rolls')
```

We can visualize the distribution of these 12 rolls, as is done in Figure 4.3.

```{r, echo=FALSE, fig.cap='Observed distribution of 12 die rolls'}
ggplot(dierolls, aes(value)) +
  geom_bar(fill = 'steelblue', color = 'white') +
  labs(y = 'Count') +
  scale_x_continuous(breaks = c(1,2,3,4,5,6)) +
  theme_classic() +
  theme(axis.title.x = element_blank())
```

Here we can see the randomness of the variable. Values 1, 2, and 5 occur more frequently than 3 and 4, and 6 does not occur at all. If we were to roll the die many more times, it would look more like the distribution we would expect. But for *this* sample of die rolls, the distribution is unique.

This is exactly the point of descriptive statistics: whether or not we know what to expect in terms of a variable's distribution, we want to know the characteristics of the distribution for a variable from a particular sample or population. When we ask for, say, a variable's average, we are asking for the approximate midpoint of that variable's distribution. 

Descriptive measures help us summarize characteristics of distributions and some serve as the building blocks for other descriptive measures as well as inferential statistics.

## Descriptive Measures

A die roll is uninteresting and unimportant. In our review of descriptive measures, let us consider them with respect to the distribution of the infant mortality rate across 222 countries in 2012. Infant mortality is the number deaths of infants under one year old per 1,000 live births. It is used as a measure of health in a country.

```{r, echo=FALSE, fig.cap='Infant Mortality Rates'}
ggplot(infmortrate, aes(inf_mort_rate)) +
  geom_histogram(fill = 'steelblue', color = 'white', bins = 30) +
  labs(x = 'Infant deaths per 1,000 live births') +
  theme_classic() 
```

We could simply show this distribution, but it is usually helpful to summarize key characteristics of it, as doing so help answer some specific questions. Distributions are typically described in one of three ways:

- **Center:** what is the typical value of this variable?
- **Spread:** how far away are values typically from the center?
- **Association:** what is the typical value or spread of the distribution given a value of another variable?

The three types of descriptive measures above are defined using questions because there are multiple options for each, and which one is more appropriate to use depends on which one answers the question best given the shape of the distribution.

### Measures of center

#### Mean {-}

The mean or average takes the values of a variable, adds them, then divides that sum by the total count of values.

\begin{equation}
{\displaystyle \bar{x}={\frac {1}{n}}\sum _{i=1}^{n}x_{i}={\frac {x_{1}+x_{2}+\cdots +x_{n}}{n}}}
(\#eq:mean)
\end{equation}

Infant mortality has 222 values, so $n$ in equation \@ref(eq:mean) above would equal 222 in this case. If we were to pluck one country out of our pool of 222 countries at random, the mean tells us the infant mortality rate to expect. In other words, the mean tells us the typical infant mortality rate given our observed data. In this case, the average infant mortality rate is `r round(mean(infmortrate$inf_mort_rate),1)`.

#### Median {-}

If we took the 222 rates and listed them in ascending or descending numerical order, the median is the value that values exactly in the middle of that list. The median is also referred to as the 50th percentile because half of the values fall below it and half of the values fall above it. In the case of an even number of values, there is no naturally occurring middle value. In that case, we take the average of the two values in the middle. The median infant mortality rate is `r round(median(infmortrate$inf_mort_rate),1)`.

#### Mode {-}

```{r, include=FALSE}
mode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
```

The mode is the value that occurs most frequently. If all values occur only once, then a variable has no mode. If two or more values occur an equal number of times and it is more than other values, then a variable has two or more modes. For instance, the modes for our 12 die rolls from before are 1, 2, and 5. The mode for infant mortality rates is 11.6.

#### Choosing a center {-}

As Figure 4.5 shows, three measures of center have provided us three different typical infant mortality rates. The mean is represented by the red line, the median by the purple line, and the mode by the green line. 

```{r mortcenters, echo=FALSE, fig.cap='Centers of infant mortality rates'}
ggplot(infmortrate, aes(inf_mort_rate)) +
  geom_histogram(fill = 'steelblue', color = 'white', bins = 30) +
  geom_vline(xintercept = 26.7, color = 'red', linetype = 'dashed') +
  geom_vline(xintercept = 15.6, color = 'purple', linetype = 'dashed') +
  geom_vline(xintercept = 11.6, color = 'green', linetype = 'dashed') +
  labs(x = 'Infant deaths per 1,000 live births') +
  theme_classic() 
```

Which should we choose to report? We could report all three, but let us apply some professional judgment to this dilemma. After all, people generally do not like nuance and prefer one best number instead of deciphering the meaning from three, assuming they understand all three measures in the first place.

For continuous variables reported at several decimal places, a value may not occur more than once because of the precision. More importantly, because of this low probability of repeat values, the mode is not guaranteed to represent a typical value. If it only takes two occurrences to qualify as the mode, that second occurrence could be an extreme value. Therefore, the mode is more commonly used to report frequencies of categorical or discrete variables for which there are relatively few possible values.

When choosing between mean and median, it usually comes down to the presence of extreme values or the extent to which a distribution is **skewed**. Skew pertains to the tails of a distribution--the taper to the left and right of its center. If the right or left tail extends far out from the center, we consider the distribution to be right- or left-skewed. Our distribution of infant mortality rates is right-skewed.

When a distribution is skewed, the median is generally a better choice for reporting its center. This is because the mean is sensitive to extreme values. Note in Figure \@ref(fig:mortcenters) that the mean is being pulled to the right by the extreme values. As a result, the red line is to the right of the cluster of frequent values and may not be a good answer for the typical value of this distribution. The median is not sensitive to extreme values. No matter how far we stretch the values above the median to the right, the middle of the distribution stays put. If one were to use the median because of skew, one should also mention that in their report.

### Measures of spread

Reporting the center of a distribution does not tell us how tightly values are grouped around the mean. Put differently, if the center is a good guess of the value for a unit drawn randomly from our data, the measure of spread is a good guess of how far off that guess will be from the random draw.

#### Variance {-}

Almost all values of our variable will not exactly equal the mean. This is referred to as **deviation from the mean**. The variance squares each observation's deviation from the mean, sums all the deviations, and divides this sum by the total count of observations minus one. Equation \@ref(eq:variance) displays this process mathematically.

\begin{equation}
{\displaystyle S^2={\frac {1}{n-1}}\sum _{i=1}^{n}(x_{i}-\bar{x})^2={\frac {(x_{1}-\bar{x})^2+(x_{2}-\bar{x})^2+\cdots +(x_{n}-\bar{x})^2}{n-1}}}
(\#eq:variance)
\end{equation}

The mean infant mortality rate is 26.7. If we subtract this mean from each country's rate, we have each country's deviation from the mean, some of which is shown in Table \@ref(tab:varcalcs). Then, we square these deviations as is also shown in the table. We then sum the 222 squared deviations and divide by 221. The variance for our infant mortality rates is `r round(var(infmortrate$inf_mort_rate),1)`.

```{r varcalcs, echo=FALSE}
infmortrate %>% 
  head(n=5) %>% 
  mutate(deviate = inf_mort_rate - 26.7, sq_deviate = deviate^2) %>% 
  kable(format = 'html', caption = 'Excerpt of variance calculations')
```

#### Standard deviation {-}

Variance is an important building block for inference, but it is virtually useless as a descriptive measure because it is in squared units. If someone asks how far values are spread out from the mean, it would not help much to report values deviate from the mean by 672 squared-deaths.

The standard deviation is simply the square root of variance to return our units to their original meaning.

\begin{equation}
{\displaystyle s = \sqrt{S^2}}
(\#eq:sd)
\end{equation}

The standard deviation of our infant mortality rates data is `r round(sd(infmortrate$inf_mort_rate),1)`. This tells us that, on average, infant mortality rates are about 26 deaths above or below the mean.

#### Interquartile range {-}

Recall that the median is the 50th percentile of a distribution--half of the values fall below the median and half fall above it. The interquartile range (IQR) is equal to the 75th percentile minus the 25th percentile, thus providing the range that captures the middle 50% of the values in the distribution. The IQR is the spread analog of the median. The IQR for our distribution is `r round(IQR(infmortrate$inf_mort_rate),1)`.

#### Range {-}

The range is simply the maximum value in a distribution minus the minimum value of a distribution. Usually, the range is left implied in a table of summary statistics by reporting the maximum and minimum without differencing the two. The range of our distribution is `r round(max(infmortrate$inf_mort_rate)-min(infmortrate$inf_mort_rate),1)`.

#### Choosing a spread {-}

The same logic applies to choosing a measure of spread as choosing a measure of center. The standard deviation is based on the mean, and so it is also sensitive to extreme values that, if present, could exaggerate the typical spread of the distribution. The IQR is based on percentiles just like the median. Therefore, IQR is not sensitive to extreme values.

Figure \@ref(fig:mortspread) displays the mean and plus-and-minus one standard deviation using red dashed and solid lines, respectively. The median and the IQR (25th and 75th percentiles) are represented by the purple dashed and solid lines, respectively. Note how wide the area contained by the standard deviation is--it contains most of the distribution. It is arguably not ideal for conveying the typical deviation from the center, as it contains plenty of values that are rather atypical deviations from the center. In the case of describing the distribution of infant mortality rates, the median and IQR are likely a better choice.

```{r mortspread, echo=FALSE, fig.cap='Center and spread of infant mortality rates'}
ggplot(infmortrate, aes(inf_mort_rate)) +
  geom_histogram(fill = 'steelblue', color = 'white', bins = 30) +
  geom_vline(xintercept = 26.7, color = 'red', linetype = 'dashed') +
  geom_vline(xintercept = 0.7, color = 'red') +
  geom_vline(xintercept = 52.7, color = 'red') +
  geom_vline(xintercept = 15.6, color = 'purple', linetype = 'dashed') +
  geom_vline(xintercept = 6.5, color = 'purple') +
  geom_vline(xintercept = 42.1, color = 'purple') +
  labs(x = 'Infant deaths per 1,000 live births') +
  theme_classic() 
```

In most cases, the range (or minimum and maximum values) should be reported along with either the mean and standard deviation or median and IQR (or 25th and 75th percentiles), especially when a distribution is skewed. In addition to signaling the skew of a distribution, the range helps convey what may be the possible values of a variable and how different the most different units in the data are with respect to that variable.

In the case of infant mortality rates, we know the minimum possible value is 0 by definition, but the minimum value in our distribution is `r min(infmortrate$inf_mort_rate)`. Perhaps 0 deaths is unrealistic for any country. Moreover, the maximum value is `r max(infmortrate$inf_mort_rate)`. This range, along with the median and IQR, tells us the most different countries are *very* different.

### The normal distribution

As a brief aside, it should be mentioned that if a distribution is **normal**, then measures of center and spread will be similar to each other. This is one of several desirable features of the normal distribution. 

Figure \@ref(fig:normrates) shows a simulated scenario in which the infant mortality rates in our 222 countries exhibit a normal distribution. Note the peaks in the center and symmetry. There is no skew. 

```{r normrates, echo=FALSE, fig.cap='Simulated normal distribution of infant mortality rates'}
set.seed(123)
normdf <- tibble(rates = rnorm(222, 26, 7))

ggplot(normdf, aes(rates)) +
  geom_histogram(fill = 'steelblue', color = 'white', bins = 30) +
  labs(x = 'Simulated infant deaths per 1,000 live births') +
  theme_classic() 
```

Table \@ref(tab:normratesum) confirms the similarity between measures of center and spread for this simulated distribution. This is one reason it is important to visualize your distribution. If it appears approximately normal, then you should report the mean and standard deviation (along with minimum and maximum values), as the are more widely understood.

```{r normratesum, echo=FALSE}
normdf %>% 
  summarise(Mean = round(mean(rates),1), Median = round(median(rates),1), Mode = round(mode(rates),1), SD = round(sd(rates),1), IQR = round(IQR(rates),1)) %>% 
  kable(format = 'html', caption = 'Center and spread measures of simulated data')
```

Again, the normal distribution has several desirable features that will be discussed further in the chapters pertaining to inference. However, you now know one desirable feature. If a distribution is approximately normal, then extreme values are not a concern and the mean and standard deviation are good measures of center and spread, respectively. Besides making our choice of measures convenient, why is this worth repeating? Because the mean and standard deviation are building blocks to the next category of descriptive measures: association. 

### Measures of association

```{r, include=FALSE}
gap07 <- gapminder %>% 
  filter(year==2007) %>% 
  select(country, lifeExp, gdpPercap)

gapdeath <- left_join(infmortrate, gap07, by = 'country')
```

With association, we now consider the distributions of two variables at a time. That is, given the value within one variable's distribution, what does the distribution of another variable look like?

We need a second variable to continue our example involving infant mortality rates. Table \@ref(tab:gapdeathtab) shows a preview of a dataset that adds two more variables to our previous infant mortality data.

```{r gapdeathtab, echo=FALSE}
gapdeath %>% 
  head(n=5) %>% 
  kable(format = 'html', caption = 'First five rows of country data')
```

Recalling that the mean infant mortality rate is about 26, the five countries included are in the right tail of the distribution. Also, you probably know enough about life expectancy to know that the values for these countries are quite low. Perhaps these two variables share a relationship?

In fact, we know they do. Life expectancy in a given year is the average age at which people in that country died. If a country has a high frequency of infants dying, then that will pull the mean downward. A common misunderstanding of life expectancy is that people in that country tend to die at the age of the country's life expectancy. This is certainly not the case if a country has a high infant mortality rate. While adults in countries with low life expectancy may die somewhat younger (or much younger if it is a war-torn country), adults tend to live longer than the average life expectancy. The key is making it out of infancy alive.

#### Visual association {-}

As was the case with one variable, we want to visualize the distributions of two variables. When working with two continuous variables, the **scatter plot** is the most common choice to visualize association between two variables. Figure \@ref(fig:scatterlifedeath) plots the paired values of infant mortality rate and life expectancy for each country.

```{r scatterlifedeath, echo=FALSE, warning=FALSE, fig.cap='Visualizing association between two continuous variables'}
ggplot(gapdeath, aes(x = inf_mort_rate, y = lifeExp)) +
  geom_point(color = 'steelblue', size = 2, alpha = 0.8) +
  labs(title = 'Infant mortality rates and life expectancy',
       x = 'Infant mortality rate',
       y = 'Life expectancy') +
  theme_classic()
```

Note that I plotted infant mortality rate along the x axis and life expectancy on the y axis. This choice was deliberate. If we suspect that one variable influences or affects the value of another variable, then the variable doing the influencing should be plotted on the x axis. Plotting a variable on the y axis implies to the viewer that it responds to the variable on the x axis.

Figure \@ref(fig:scatterlifedeath) confirms our suspicion that the two variables are associated. There appears to be a rather strong association such that as infant mortality rate increases, the lower a country's life expectancy.

#### Quantified association {-}

As was the case with one variable, we want to describe the association between two variables using quantitative measures. The association between two or more variables can be described in terms of

- **Direction:** when one variable increases, does the other variable increase or decrease?
- **Strength:** how much do the variables seem to be tied together?
- **Magnitude:** given an specific increase or decrease in one variable, by how much does the other variable increase or decrease?

Again, we have several options to answer the above question.

- **Covariance:** measures direction of association between between two variables
- **Correlation:** measures direction and strength of association between two variables
- **Regression coefficient:** measures the direction and magnitude of association between an explanatory variable and an outcome variable
- **Coefficient of determination ($R^2$):** measures the strength of association between a set of one or more explanatory variables and an outcome variable

The regression coefficient and coefficient of determination are discussed in Chapter \@ref(r-simple-and-multiple-regression) involving regression models. Let us briefly consider covariance and correlation.

#### Covariance {-}

Covariance tells us when our x variable is above (below) its mean, whether our y variable tends to be above or below its mean. If our y variable tends to be above its mean when x is above its mean, then the two have a positive covariance and are positively associated. If our y variable tends to be below its mean when x is above its mean, then the two have a negative covariance and are negatively associated. A covariance of 0 indicates no association.

Figure \@ref(fig:scattercovar) adds references lines for the mean of each variable. Note that when infant mortality is above its mean (to the right of the red line), life expectancy is below its mean (below the purple line) in almost all cases. When infant mortality is below its mean, life expectancy is above its mean in almost all cases. Therefore, these two variables have a negative covariance and are negatively associated. In fact, the covariance between infant mortality rate and life expectancy is `r round(cov(gapdeath$inf_mort_rate, gapdeath$lifeExp, use='pairwise.complete.obs'),1)`

```{r scattercovar, echo=FALSE, warning=FALSE, fig.cap='Visualizing covariance'}
ggplot(gapdeath, aes(x = inf_mort_rate, y = lifeExp)) +
  geom_point(color = 'steelblue', size = 2, alpha = 0.8) +
  geom_vline(xintercept = 26.7, color = 'red', linetype = 'dashed') +
  geom_hline(yintercept = 66.9, color = 'purple', linetype = 'dashed') +
  labs(title = 'Infant mortality rates and life expectancy',
       x = 'Infant mortality rate',
       y = 'Life expectancy') +
  theme_classic()
```

Covariance is the association analog of variance. It is an important building block of other measures of association, but it is virtually useless for description because it only tells us direction. Correlation tells us direction and strength. Therefore, covariance is never used for description because correlation tells us twice as much information.

#### Correlation {-}

Correlation tells us how much the paired values of two variables exhibit a straight line and whether that straight line is sloped positively or negatively. Correlation ranges between -1 and 1. If it is negative, then the two variables are negatively associated. If it is positive, the two variables are positively associated. The closer correlation is to -1 or 1, the more the two variables exhibit a straight line. A correlation equal to -1 or 1 indicates the two variables form a perfect straight line. A correlation of 0 indicates no association.

Based on the covariance and the scatter plot, we know to expect a negative correlation between infant mortality rate and life expectancy. We also know the correlation will not be -1 because the points do not form a perfect straight line. Nevertheless they do form a fairly tight negative path, so we should expect a correlation closer to -1 than 0. It turns out that the correlation is equal to `r round(cor(gapdeath$inf_mort_rate, gapdeath$lifeExp, use='pairwise.complete.obs'),1)`. Infant mortality rate and life expectancy exhibit a very strong, negative association.

Another way to think about correlation is if we tried to draw a line through the data points on our scatter plot from left to right that could freely curve about however the data are scattered, would that line be a straight line and would it slope upward or downward? Figure \@ref(fig:scattercorr) does exactly that with our data. Note that the data lead the line to be essentially straight until the end when it appears the association turns positively. 

```{r scattercorr, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Drawing a free line through the data'}
ggplot(gapdeath, aes(x = inf_mort_rate, y = lifeExp)) +
  geom_point(color = 'steelblue', size = 2, alpha = 0.8) +
  geom_vline(xintercept = 26.7, color = 'red', linetype = 'dashed') +
  geom_hline(yintercept = 66.9, color = 'purple', linetype = 'dashed') +
  geom_smooth(se = FALSE, color = 'black', linetype = 'dashed') +
  labs(title = 'Infant mortality rates and life expectancy',
       x = 'Infant mortality rate',
       y = 'Life expectancy') +
  theme_classic()
```

Correlation has three qualities that can lead to misunderstandings or mistakes, some of which may have become apparent to you in the above discussion. First, correlation is sensitive to extreme values. A few dots on a scatter plot can have a substantial impact on the line that is drawn through the data. Second, correlation measures only the **linear** association, thus the repeated mentioning of **straight** lines. If two variables formed a perfect U-shape, they are almost certainly strongly associated. However, their correlation would suggest a weaker relationship because a straight line does not fit a U-shape well. Third, **correlation is a necessary but not sufficient condition of causation**. In order to validly claim that a change in the value of one variable *causes* the values of another variable to change, they must be correlated, but a few more conditions must also be met. Those conditions are discussed in Chapter \@ref(causation-and-bias).

<br>

> **To learn how to produce a summary table for a publication, proceed to Chapter \@ref(r-description).**

## Key terms and concepts {#kt4}

```{block, type='learncheck', purl=FALSE}
- Descriptive statistics
- Inferential statistics
- Population
- Sample
- Parameter
- Statistic
- Estimate
- Distribution
- Mean
- Median
- Mode
- Skewed distribution
- Standard deviation
- Interquartile range
- Range
- Correlation
```