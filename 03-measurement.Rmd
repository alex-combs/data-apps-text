# Measurement and Missing

> *"Will he not fancy that the shadows which he formerly saw are truer than the objects which are now shown to him?*
>
>---Plato

<br>

Once we understand the structure of our data and the types of variables contained within, we need to understand how the data relates to reality before trying to draw conclusions. Variables and their values are measured representations of reality. They are shadows on the allegorical cave wall. We should not assume these shadows are necessarily good representations of reality.

Measurement validity and reliability are the foundations of credible analysis, the components of which are depicted in Figure \@ref(fig:credfig). Without the two, we have little or no basis to make conclusions from data. As they say, garbage in, garbage out. No amount of fancy statistical tactics can compensate for starting with bad data.

<center>
```{r credfig, echo=FALSE, fig.cap="Components of credible analysis"}
include_graphics("images/credible.png")
```
</center>

<br>

## Learning objectives {#lo3}

- Assess the measurement validity of variables
- Assess the measurement reliability of variables
- Explain the difference between accuracy and precision

## Measurement validity

Data do not exist in nature. Rather, someone must set out to observe one or more phenomena, measure it, record it, and compile the recorded measures into a file or database. This process involves choices, limitations, and potential flaws. When evaluating the quality of data, the first quality to consider is the validity of the measure. Measurement validity can be considered from two similar yet distinct angles.

> **Measurement Validity**: Does the variable measure the concept/phenomenon it is intended or claims to measure? Are the recorded values of the variable accurate measures of the true values of the variable?

The first question above concerns conceptual accuracy. On matters of education policy, is GPA or standardized test scores a valid measure of our concept of intelligence? How about IQ? What do we even mean by intelligence? Perhaps we must clarify our concept as academic aptitude or a more observable concept like academic achievement. Are test scores a valid measure of teacher quality? Does the quality of a teacher amount to their students' academic achievement? On matters of public health, is body mass index (BMI; weight in kilograms divided by height in meters squared) a valid measure of a person's health? In public finance, are property values a valid measure of the quality of local public services, such as schools, parks, and police/fire departments? Is a city's bond rating a valid measure of its financial health? Is the unemployment rate a valid measure of economic performance or prosperity? Is the proportion of those below the federal poverty line a valid measure for the severity of poverty or financial stress?

The second question above concerns procedural accuracy. Regardless of whether we agree or disagree on what a variable actually measures, we must also consider whether the recorded values were accurately recorded. Is the number of sexual assaults in a dataset of city crime the actual number of sexual assaults that were committed? Is the recorded value of a property its actual fair market value? Is the recorded test score for a student their true test score? Procedural issues could be systemic or due to human error or manipulation. Sexual assaults along with almost all types of crime are systematically under-reported, thus recorded values are likely to be lower than the true value. Property value assessors may purposefully over- or under-value properties out of political or personal motivations. Or their inaccuracy could be accidental and due to a property being unique and difficult to assess. Data are also subject to input error depending on how they are recorded, receiving an errant decimal or 0.

## Measurement reliability

Measurement reliability can be also be considered from two similar yet distinct angles.

> **Measurement Reliability**: Provided no change in a subject's condition/reality, does the way the variable is measured generate the same value? Given identical conditions/realities between multiple subjects, does the measure generate identical values?

The first question above concerns a measure's consistency for a single subject. A student receives a score on their GRE. Provided the same student does not study before taking the GRE again, will they receive the same score (referred to as test-retest reliability)? A property value assessor assesses a property. A second assessor conducts an assessment of the same property. Will they arrive at the same property value (referred to as inter-rater reliability)?

The second question above concerns a measure's consistency for multiple subjects with identical conditions. Suppose two students are equally intelligent, academically apt, or whatever the correct concept should be. Will the two students receive the same GRE score? Will two identical properties receive the same property value? 

A measure can be valid or invalid and reliable or unreliable, resulting in one of four combinations. Let us consider an example where an agency needs to allocate resources to state governments according to the number of persons who are homeless in each state. Suppose the *true* count of homeless persons for two states is the same. Figure \@ref(fig:homeless) tries to visualize the scenario and the three potentially problematic combinations of validity and reliability along a number line.

<center>
```{r homeless, echo=FALSE, fig.cap="Comparing measurement validity and reliability"}
include_graphics("images/measure_lines.png")
```
</center>

Though techniques to count the number of homeless persons are improving, one method has been to designate a specific day of the year (e.g. January 1st) where government staff and volunteers attempt to conduct a census of homeless people. If these census takers in separate states successfully recorded the true count of homeless people, then our dataset would contain a valid and reliable measure. In the case where a valid and reliable measure is taken, the two states receive equal and appropriate amounts of resources.

If an invalid yet reliable measure is taken, as is depicted in the second number line from the top in Figure \@ref(fig:homeless), the two states receive equal resources, but the amount of resources is less (or more) than what it should be. If a valid and unreliable measure is taken, as is depicted by the third number line, then amount of resources provided is appropriate *on average*, but the two states receive different amounts. Lastly, if an invalid and unreliable measure is taken, the two states receive different amounts and the amount of resources provided is systematically less (or more) than what it should be.

Taking a count of homeless people on a designated day is known to have issues of measurement validity and reliability. Validity is an issue at least procedurally because it is unlikely that a government can count all of its homeless people. Whether or not it is conceptually invalid depends on what we claim it measures. Why is this measure unreliable? Temperature affects how easily and accurately the number of homeless people can be counted. In a cold state, most homeless people will stay in shelters. In a warm state, homeless people will be scattered and more difficult to count. Even if a cold state and a warm state truly had equal number of homeless people, it is unlikely that the same count would be recorded. 

The moral of this story is that when you collect data you did not generate yourself for your own purpose, take the time to consider if those data are valid and reliable measures for your intended purpose and what the consequences could be if they are not. Also, keep in mind that no measure is perfect. Our concern should not be so much whether a measure is valid vs. invalid or reliable vs. unreliable, but rather the degree to which a measure is invalid and unreliable.

## Missing data

It is not uncommon to encounter missing values in a data. Respondents skip or choose not to answer survey questions, administrators fail to contact respondents, entities that reported data last year may have dissolved or consolidated with another entity this year. Many reasons can lead to missing data. The key is to consider why data are missing and if it should affect your conclusions.

Using the previous example of self-reported income, suppose there are numerous missing values in the responses. Should we assume they are missing at random, or that there is some underlying reason or pattern? Perhaps those with no or low income do not wish to report. If we were to dismiss these missing values, and draw conclusions from the non-missing values, we may severely overestimate the income of the target population.

### Types of missing data

Missing data come in two varieties:

>
- **Explicit:** data that we can see are missing in the data; cells containing a value that denotes missing
- **Implicit:** data that we would expect to be included based on data structure but are not; no obvious sign of missing

Table 3.1 shows an example of data that are explicitly missing denoted by `NA`. Missing data is denoted in a variety of ways. For example, instead of `NA`, the cells could have been left empty, or filled with a period, or some other symbol. If data were obtained from an organization that regularly produces publicly available data, datasets are usually accompanied by a legend that explains what symbols denote missing. 

```{r, echo=FALSE}
crossgap <- gapminder %>% 
  filter(year == 2007 & continent == 'Americas') %>% 
  head(n=3)

crossgap[1,4] <- NA
crossgap[2,6] <- NA

kable(crossgap, caption = "Example of explicitly missing data")
```


Beware ambiguous missing values. For instance, some survey questions are dependent on previous questions. You do not want to conclude that a value is missing because a respondent chose not to answer when they were never asked the question. Or perhaps a value is missing when it should actually equal 0 or vice versa. If missing data are consequential to your analysis, then you may need to investigate further into how the data were collected or coded in order to eliminate such ambiguity.

Table 3.2 shows an example of implicitly missing data. Argentina is observed in 1997, 2002, and 2007, but Bolivia is observed only in 1997 and 2007. What happened to the 2002 observation for Bolivia? This sort of entry and exit from the dataset is common in panel data where the same units are observed over multiple time periods. 

```{r, echo=FALSE}
gapminder %>% 
  filter(continent == 'Americas', year >= 1997) %>% 
  head(n=6) %>%
  filter(country!='Bolivia' | year!=2002) %>% 
  kable(caption = 'Example of implicitly missing data')
```

Note that the missing Bolivia observation was easy to spot because the dataset is extremely small. If we were dealing with a large dataset, this would not have been so obvious. A quick way to check whether there may be implicitly missing observations is to check the number of observations in your data. If you are under the impression that your data contains all 50 states for 10 years, then you should have 500 observations. If not, some states or years must be missing.

> **To learn how to work with missing data in R, proceed to Chapter \@ref(r-missing-data).**

## Key terms and concepts {#kt2}

- Measurement validity
- Measurement reliability
- Measurement precision
- Implicitly missing data
- Explicitly missing data
