% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{book}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
  \usepackage{amssymb}
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Data Applications in Public Administration},
  pdfauthor={Alex Combs},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=Blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{bbm}
\usepackage[bf,singlelinecheck=off]{caption}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\usepackage{float}
\floatplacement{figure}{H}
\floatplacement{table}{H}
\setlength{\textfloatsep}{0.1cm}
\usepackage{array}
\usepackage{multirow}
%\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{kotex}

% Changed via Rob Hyndman's suggestions
% https://robjhyndman.com/hyndsight/latex-floats/
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}
% Force images to appear after their definition
\usepackage{flafter}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

%% Need to clean up
\newenvironment{rmdblock}[1]
  {\begin{shaded*}
  }
  {\end{shaded*}
  }

\newenvironment{learncheck}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}

% No widow lines
% Copied from https://github.com/hadley/adv-r/blob/master/latex/preamble.tex
\widowpenalty=10000
\clubpenalty=10000

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\listfiles

\frontmatter
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Data Applications in Public Administration}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Using R to Learn Concepts and Skills}
\author{Alex Combs}
\date{Last updated on 05 January 2021}

\begin{document}
\maketitle

\cleardoublepage\newpage
\thispagestyle{empty}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

This is a collection of lecture and presentation notes intended as a resource for students enrolled in my sections of PADP 7120: Data Applications in Public Administration. Distribution beyond that is discouraged. This resource is not peer-reviewed. All opinions and errors are my own. I do not benefit monetarily from this resource in any way.

The objective of this resource is to help students in PADP7120 become as competitive as possible in their desired job markets via competency in statistics and statistical programming software. It aims to teach students key concepts in statistics and applications of those concepts using R with a level of theoretical and technical detail that is accessible for MPA students.

\hypertarget{style-and-structure}{%
\section*{Style and Structure}\label{style-and-structure}}
\addcontentsline{toc}{section}{Style and Structure}

Rather than provide thorough coverage of complex statistical concepts, I take some liberty to present stylized facts for the benefit of the reader. When using R, multiple options exist to achieve an outcome. I provide what I consider or understand to be the best option. You may be aware of or discover what you consider a better approach. I welcome such suggestions for improvement!

Chapters are organized along two tracks. The first track covers statistical concepts and is self-contained. The second track applies the concepts in the first track using R. The chapters in the applied track are referred to as R chapters, each of which corresponds to a conceptual chapter in the first track. For example, the \texttt{R\ Data} chapter corresponds to the \texttt{Data} chapter in the first track.

The conceptual track is divided into four sections:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data and description
\item
  Regression models
\item
  Inference
\item
  Advanced topics
\end{enumerate}

Examples and exercises are presented using R. Students who intend to use a personal computer to complete exercises in the R chapters need to download and install the following software:

\begin{itemize}
\tightlist
\item
  \href{https://cloud.r-project.org}{R}
\item
  \href{https://rstudio.com/products/rstudio/download/}{RStudio}
\end{itemize}

\hypertarget{supplemental-resources}{%
\section*{Supplemental resources}\label{supplemental-resources}}
\addcontentsline{toc}{section}{Supplemental resources}

There are numerous free materials that teach statistics and R.

\begin{itemize}
\tightlist
\item
  \textbf{Traditional statistics texts}

  \begin{itemize}
  \tightlist
  \item
    \href{https://www.openintro.org/book/os/}{OpenIntro Statistics} by David Diez, Mine Cetinkaya-Rundel, and Christopher Barr
  \item
    \href{https://open.umn.edu/opentextbooks/textbooks/quantitative-research-methods-for-political-science-public-policy-and-public-administration-with-applications-in-r-3rd-edition}{Quantitative Research Methods for Political Science, Public Policy and Public Administration (With Applications in R) - 3rd Edition} by Hank Jenkins-Smith and Joseph Ripberger
  \end{itemize}
\item
  \textbf{R-centric statistics texts}

  \begin{itemize}
  \tightlist
  \item
    \href{https://moderndive.com/index.html}{Statistical Inference via Data Science} by Chester Ismay and Albert Y. Kim
  \item
    \href{https://r4ds.had.co.nz}{R for Data Science} by Garrett Grolemund and Hadley Wickham
  \item
    \href{https://socviz.co/index.html\#preface}{Data Visualization: A practical introduction} by Kieran Healy
  \item
    \href{https://otexts.com/fpp2/}{Forecasting: Principles and Practice} by Rob J. Hyndman and George Athanasopoulos
  \end{itemize}
\item
  \textbf{Other R resources}

  \begin{itemize}
  \tightlist
  \item
    \href{https://rmarkdown.rstudio.com/lesson-1.html}{R Markdown from RStudio}
  \item
    \href{https://bookdown.org/yihui/rmarkdown/}{R Markdown: The Definitive Guide} by Yihui Xie, J.J. Allaire, and Garrett Grolemund
  \item
    \href{https://holtzy.github.io/Pimp-my-rmd/\#text_formating}{Pimp my RMD} by Yan Holtz
  \item
    \href{https://datacarpentry.org/r-socialsci/}{Data Carpentry: R for Social Scientists}
  \end{itemize}
\end{itemize}

\hypertarget{r-package-references}{%
\section*{R Package References}\label{r-package-references}}
\addcontentsline{toc}{section}{R Package References}

The following list cites the creators and authors of the packages used in this resource.

\begin{itemize}
\tightlist
\item
  \texttt{arsenal} Ethan Heinzen {[}aut, cre{]}, Jason Sinnwell {[}aut{]}, Elizabeth Atkinson {[}aut{]}, Tina Gunderson {[}aut{]}, Gregory Dougherty {[}aut{]}
\item
  \texttt{broom} David Robinson and Simon Couch
\item
  \texttt{car} and \texttt{carData} John Fox {[}aut, cre{]}, Sanford Weisberg {[}aut{]}, Brad Price {[}aut{]}
\item
  \texttt{DAAG} John H. Maindonald and W. John Braun
\item
  \texttt{data.table} Matt Dowle {[}aut, cre{]}, Arun Srinivasan {[}aut{]}
\item
  \texttt{Ecdat} Yves Croissant and Spencer Graves
\item
  \texttt{fivethirtyeight} Albert Y. Kim {[}aut, cre{]}, Chester Ismay {[}aut{]}, Jennifer Chunn {[}aut{]}
\item
  \texttt{forecast} Rob J Hyndman
\item
  \texttt{fpp2} Hyndman, R.J., and Athanasopoulos, G. (2017). Forecasting: principles and practice, OTexts: Melbourne, Australia. \url{https://OTexts.org/fpp2/}
\item
  \texttt{gapminder} Jennifer Bryan
\item
  \texttt{gvlma} Pena, EA and Slate, EH (2006). ``Global validation of linear model assumptions,'' J.~Amer.~Statist.~Assoc., 101(473):341-354.
\item
  \texttt{knitr} Yihui Xie
\item
  \texttt{lubridate} Garrett Grolemund and Hadley Wickham
\item
  \texttt{moderndive} Albert Y. Kim {[}aut, cre{]}, Chester Ismay {[}aut{]}
\item
  \texttt{openintro} Mine Çetinkaya-Rundel {[}aut, cre{]}, David Diez {[}aut{]}, Andrew Bray {[}aut{]}, Albert Kim {[}aut{]}, Ben Baumer {[}aut{]}, Chester Ismay {[}aut{]}, Christopher Barr {[}aut{]}
\item
  \texttt{plm} Yves Croissant {[}aut, cre{]}, Giovanni Millo {[}aut{]}, Kevin Tappe {[}aut{]}
\item
  \texttt{readxl} Hadley Wickham
\item
  \texttt{Stat2Data} Ann Cannon, George Cobb, Bradley Hartlaub, Julie Legler, Robin Lock, Thomas Moore, Al- lan Rossman, Jeffrey Witmer
\item
  \texttt{tidyverse} Hadley Wickham
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

{Data Applications in Public Administration} by Alex Combs is licensed under CC BY-NC-ND 4.0

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

\begin{quote}
\emph{``Data, data everywhere, and not a thought to think.''}

---John Allen Paulos
\end{quote}

\hypertarget{why-statistics}{%
\section{Why statistics}\label{why-statistics}}

Statistics converts raw information (i.e.~data) into something useful. If we want to make evidence-based decisions, we need statistics. If we want to allow ourselves to be misled by nefarious or mistaken analyses of data, we should resist learning statistics.

\hypertarget{professional-standards}{%
\section{Professional standards}\label{professional-standards}}

The Network of Schools of Public Policy, Affairs, and Administration (NASPAA) is the accrediting authority for MPA programs. NASPAA promotes the following universal competencies:

\begin{itemize}
\tightlist
\item
  to lead and manage in the public interest;
\item
  to participate in, and contribute to, the policy process;
\item
  to analyze, synthesize, think critically, solve problems and make evidence-informed decisions in a complex and dynamic environment;
\item
  to articulate, apply, and advance a public service perspective;
\item
  to communicate and interact productively and in culturally responsive ways with a diverse and changing workforce and society at large.
\end{itemize}

Statistics will help you develop all of the above competencies. You would not sufficiently possess one or more of the above competencies without knowledge and skills in statistics.

\hypertarget{statistics-in-pa}{%
\section{Statistics in PA}\label{statistics-in-pa}}

The use of statistics is ubiquitous in public administration. Agencies and nonprofits use statistics to describe their clients and assess their needs. Agencies like the Government Accountability Office and watchdog organizations use statistics to monitor performance and guard against fraud. Service-oriented organizations like schools and hospitals use statistics to evaluate services and communicate to stakeholders. The Congressional Budget Office, Office of Management and Budget, and employees at every level of government use statistics to assess finances and forecast trends.

\hypertarget{using-r}{%
\section{Using R}\label{using-r}}

The goal of this course is not for you to become an expert in R or even a data scientist or analyst. Rather, the goal is to train you enough so R becomes a legitimate alternative to inferior spreadsheet software like Excel and enable you to perform statistical tasks that may be expected of a master in public administration.

MPA students may be reluctant to learn something referred to as a statistical computing language and its relevancy to their career goals may not be clear. I firmly believe that not training you to use statistical software in a course such as this would be doing you a disservice. Demand for those competent in statistical software like R continues to rise. Even if you plan to pursue a managerial role with minimal analytic tasks, chances are non-trivial that you will supervise or work with those who conduct analyses. You will need to interpret their findings, applying your own managerial and/or subject matter expertise toward making an evidence-based decision. People in both roles--consumers and producers of statistical analyses--need to be able to communicate with the other. The best way to become a competent consumer of statistical information is to learn the basics of producing it.

In addition, R is free! There are many free resources that teach R, and R is popular across many disciplines. If you study statistics and data applications for a semester, you might as well spend part of that semester learning software like R. There is only upside in doing so with respect to employment prospects.

\begin{quote}
\textbf{Proceed to Chapter \ref{r-chapter-introduction} for an orientation to R.}
\end{quote}

\hypertarget{part-data-and-description}{%
\part{Data and Description}\label{part-data-and-description}}

\hypertarget{data}{%
\chapter{Data}\label{data}}

\begin{quote}
\emph{Nothing exists except atoms and empty space; everything else is opinion.}

---Democritus
\end{quote}

We cannot effectively convert the raw material of knowledge into a useful product without first understanding the raw material. Therefore, learning statistics naturally begins with learning the types and structures of data.

\hypertarget{lo2}{%
\section{Learning objectives}\label{lo2}}

\begin{itemize}
\tightlist
\item
  Understand the organization of rectangular data
\item
  Identify the unit of analysis within a dataset
\item
  Identify and distinguish types of variables
\item
  Identify and distinguish types of dataset structures
\end{itemize}

\hypertarget{rectangular-data}{%
\section{Rectangular Data}\label{rectangular-data}}

\begin{table}

\caption{\label{tab:print-generic}Generic rectangular data}
\centering
\begin{tabular}[t]{l|l|l}
\hline
ID & Variable\_1 & Variable\_2\\
\hline
Unit of Analysis & Datum & Datum\\
\hline
Unit of Analysis & Datum & Datum\\
\hline
Unit of Analysis & Datum & Datum\\
\hline
\end{tabular}
\end{table}

Rectangular data organized by rows and columns. A rectangular dataset has three components. Not all datasets will match the below description because many datasets are not organized correctly.

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{Unit of analysis or observation:} The generic entity or subject a row of data refers to. The unit of analysis uniquely identifies each row of a dataset. If we have a dataset of 50 states and some variables measured in 2020, then our unit of analysis is states. If you were told a specific state, then you could find the row in the dataset. If we have 50 states measured in 2019 and 2020, then the unit of analysis is state-year because you will need to know the state and year to find a specific row.
\item
  \textbf{Variable:} A measured characteristic of the unit of analysis. State unemployment rate is a variable for a state unit of analysis.
\item
  \textbf{Datum:} The intersection of a variable (column) and a unit of analysis (row) resulting in a cell. The datum is a particular piece of information. A cell could contain something like 4.8 as the unemployment rate for Georgia in 2020.
\end{itemize}
\end{quote}

\hypertarget{types-of-variables}{%
\section{Types of variables}\label{types-of-variables}}

The variables in a given dataset can be of several types. Types of variables are important to learn because the types of variables one is dealing with has consequences for data applications, such as description, visualization, and inference.

A variable provides us raw information about the units of analysis. If statistics is a discipline to convert raw information into something useful, then it stands to reason that we should want to know what type of information a variable provides us, especially the specificity of that information.

For example, suppose you ask two strangers to report their annual income. What options do they have for answers? If virtually any value, then you know to a precise degree the income each earns and can compute the precise difference between the two incomes. What if their choices are either more or less than \$50,000? Then, you have a coarse understanding of how much they earn. If they provide different answers, you can only conclude whether one makes more than the other but not by how much. If they provide the same answer, then the two are grouped together even though it is highly unlikely they earn equal incomes. This makes a serious difference for statistical analysis.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/variables} 

}

\caption{Variable Types}\label{fig:vartypefig}
\end{figure}

All variables belong to one of two broad types: qualitative (or categorical) and quantitative (or numeric).

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{Qualitative} variables take on values that have no intrinsic numerical meaning. They are expressed in words.
\item
  \textbf{Quantitative} variables take on values that do have intrinsic numerical meaning.
\end{itemize}
\end{quote}

\hypertarget{qualitative-variables}{%
\subsection{Qualitative variables}\label{qualitative-variables}}

Qualitative variables can be further differentiated into two types: nominal and ordinal.

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{Nominal} variables take on values that differ in name only.
\item
  \textbf{Ordinal} variables take on values that can be ranked relative to each other but the difference between rankings has no numerical value.
\end{itemize}
\end{quote}

The values that categorical variables take on are commonly referred to as \textbf{levels}. Categorical variables can contain virtually any number of levels, though the number of levels is usually limited.

A variable such as sex contains two levels: male and female. The variable sex is nominal, as its values have no numerical meaning and the two levels have no ranking. Race, state, country, political party, and any variable coded as yes/no such as unemployed, married, and below the federal poverty line are all examples of nominal variables.

If you have ever participated in a customer satisfaction survey, then you have almost surely contributed data to an ordinal variable. Those scales that provide some number of options from ``disagree'' to ``agree'' are called Likert scales. Your answer has no intrinsic numerical value but it can be ranked against the answers of others. One respondent can be said to be more satisfied than another but not by how much. Moreover, one can only trust the results insofar as respondents have the same understanding or frame of reference--the service that satisfied one respondent may not have satisfied another. Other ordinal variables, such as education degree and income level do not have this issue.

\hypertarget{quantitative-variables}{%
\subsection{Quantitative variables}\label{quantitative-variables}}

Quantitative variables can be further differentiated into two types: discrete and continuous.

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{Discrete} variables take on countable or indivisible values.
\item
  \textbf{Continuous} variables take on infinitely divisible values (at least in theory).
\end{itemize}
\end{quote}

Any variable that is a count of persons, places, events, or things is a discrete variable, taking on integer values (e.g.~0, 1, 2, 3,\ldots). The count of homeless people in a city, students in a classroom, hospital beds, or nonprofit volunteers are all discrete variables. When should we care that a variable is discrete? Chapter \ref{descriptive-statistics} and chapters on inference will discuss how statistics relies heavily on the \textbf{normal distribution}, also referred to as a bell curve. If a discrete variable can take on integer values only, and especially if only a few values are possible, then that variable is unlikely to be normally distributed. Rare discrete events, such as plane crashes or government defaults are not normally distributed. Application of basic statistical procedures to such variables may be inappropriate, requiring more advanced methods outside to scope of this course.

In many cases, if a variable is numeric, then it is continuous or can be treated as such. Continuous variables can contain values with an infinite number of decimal places. Still, continuous variables are recorded in data with a limited number of decimal places because either we measure phenomena with finite precision or it simply becomes impractical to include so many decimal places. For example, age is usually recorded in discrete years, but we could record continuously to the zeptosecond (a trillionth of a billionth of a second).

Many discrete variables become continuous because we calculate averages, proportions, or rates from them. The number of students in the classroom is discrete (e.g.~20, 25, etc.), but the average number of students in classrooms (i.e.~total students/number of classrooms) is continuous. This is how we can have values such as 22.5 for pupil-to-teacher ratio. The number of homeless people in a city is discrete but the proportion of the city's population that is homeless (count of homeless/population) is continuous.

\hypertarget{index-variables}{%
\subsection{Index variables}\label{index-variables}}

Index variables are usually continuous but warrant separate discussion. An index variable is a composite measure of multiple variables. They can be used to make a continuous variable out of multiple categorical variables or simplify multiple quantitative variables into one quantitative measure. Purposes such as ranking colleges, measuring multidimensional poverty (i.e.~factors beyond income), and determining political ideology make use of index variables.

Index variables mask underlying information. This can be helpful or harmful. In either case, it is important to consider how an index variable is constructed. Doing so can offer insight or uncover problems. An instructive example familiar to readers is college rankings. U.S. News and World Report \href{https://www.usnews.com/education/best-colleges/articles/ranking-criteria-and-weights}{describes} how rankings are determined.

What makes a college good? According to these rankings, five percent of what makes a college good is the percent of undergraduate alumni giving a donation as a proxy of student satisfaction. Another 20\% is based on the opinions of administrators at peer institutions. Are these choices wise? This is difficult to say and besides the point. The point is that index variables involve choices made by people and are not data that are observed directly. They are synthetic materials of knowledge and warrant careful consideration.

\hypertarget{dataset-structures}{%
\section{Dataset structures}\label{dataset-structures}}

Just as the type of variable one is dealing with impacts the kinds of visualizations or analyses one should use, so too does the structure of a dataset. Datasets come in three varieties depending on their unit of analysis.

\begin{itemize}
\tightlist
\item
  Cross-sectional

  \begin{itemize}
  \tightlist
  \item
    Pooled cross-sectional
  \end{itemize}
\item
  Time series
\item
  Panel or longitudinal
\end{itemize}

\textbf{Cross-sectional} data is a snapshot in time measuring some size sample of units. One column serves as the identifier of the unit of analysis, such as the name or ID number of the unit. Notice in Table 2.2 that all one needs to know is the country in order to identify a specific row.

\begin{table}

\caption{\label{tab:cross-sec-fig}Cross-section example}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
country & continent & year & lifeExp & pop & gdpPercap\\
\hline
Argentina & Americas & 2007 & 75.320 & 40301927 & 12779.380\\
\hline
Bolivia & Americas & 2007 & 65.554 & 9119152 & 3822.137\\
\hline
Brazil & Americas & 2007 & 72.390 & 190010647 & 9065.801\\
\hline
\end{tabular}
\end{table}

\textbf{Pooled cross-sectional} data could be considered a fourth structure but is simply multiple cross-sections stacked atop each other. The critical quality of pooled cross-sectional data is that each cross-section contains \emph{different} units measured at different times, not the same units measured at different times. Notice in Table 2.3 that the countries included from 2002 are not the same as those included from 2007.

\begin{table}

\caption{\label{tab:pooled}Pooled cross-section example}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
country & continent & year & lifeExp & pop & gdpPercap\\
\hline
Algeria & Africa & 2002 & 70.994 & 31287142 & 5288.040\\
\hline
Angola & Africa & 2002 & 41.003 & 10866106 & 2773.287\\
\hline
Benin & Africa & 2002 & 54.406 & 7026113 & 1372.878\\
\hline
Botswana & Africa & 2002 & 46.634 & 1630347 & 11003.605\\
\hline
Argentina & Americas & 2007 & 75.320 & 40301927 & 12779.380\\
\hline
Bolivia & Americas & 2007 & 65.554 & 9119152 & 3822.137\\
\hline
Brazil & Americas & 2007 & 72.390 & 190010647 & 9065.801\\
\hline
\end{tabular}
\end{table}

\textbf{Time series} data measures one unit over multiple time periods. The unit of analysis in time series data is time, as it uniquely identifies each row. Notice in Table 2.4 that one country is tracked over multiple years.

\begin{table}

\caption{\label{tab:timeseries}Time series example}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
country & continent & year & lifeExp & pop & gdpPercap\\
\hline
Argentina & Americas & 1977 & 68.481 & 26983828 & 10079.027\\
\hline
Argentina & Americas & 1982 & 69.942 & 29341374 & 8997.897\\
\hline
Argentina & Americas & 1987 & 70.774 & 31620918 & 9139.671\\
\hline
Argentina & Americas & 1992 & 71.868 & 33958947 & 9308.419\\
\hline
Argentina & Americas & 1997 & 73.275 & 36203463 & 10967.282\\
\hline
Argentina & Americas & 2002 & 74.340 & 38331121 & 8797.641\\
\hline
Argentina & Americas & 2007 & 75.320 & 40301927 & 12779.380\\
\hline
\end{tabular}
\end{table}

\textbf{Panel} (or longitudinal) data measures the same units over multiple time periods. The unit of analysis is pair of unit and time period. Notice in Table 2.5 that in order to identify a specific row, you would need to know the country \emph{and} year. One could also think of panel data as numerous time series.

\begin{table}

\caption{\label{tab:panel}Panel example}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
country & continent & year & lifeExp & pop & gdpPercap\\
\hline
Argentina & Americas & 1997 & 73.275 & 36203463 & 10967.282\\
\hline
Argentina & Americas & 2002 & 74.340 & 38331121 & 8797.641\\
\hline
Argentina & Americas & 2007 & 75.320 & 40301927 & 12779.380\\
\hline
Bolivia & Americas & 1997 & 62.050 & 7693188 & 3326.143\\
\hline
Bolivia & Americas & 2002 & 63.883 & 8445134 & 3413.263\\
\hline
Bolivia & Americas & 2007 & 65.554 & 9119152 & 3822.137\\
\hline
\end{tabular}
\end{table}

\begin{quote}
\textbf{To learn how to examine data in R, proceed to Chapter \ref{r-data}.}
\end{quote}

\hypertarget{kt1}{%
\section{Key terms and concepts}\label{kt1}}

\begin{itemize}
\tightlist
\item
  Unit of analysis
\item
  Variable
\item
  Types of variables: qualitative, quantitative, nominal, ordinal, discrete, continuous, index
\item
  Data structures: cross-sectional, pooled cross-sectional, time series, panel
\end{itemize}

\hypertarget{measurement-and-missing}{%
\chapter{Measurement and Missing}\label{measurement-and-missing}}

\begin{quote}
\emph{"Will he not fancy that the shadows which he formerly saw are truer than the objects which are now shown to him?}

---Plato
\end{quote}

Once we understand the structure of our data and the types of variables contained within, we need to understand how the data relates to reality before trying to draw conclusions. Variables and their values are measured representations of reality. They are shadows on the allegorical cave wall. We should not assume these shadows are necessarily good representations of reality.

Measurement validity and reliability are the foundations of credible analysis, the components of which are depicted in Figure \ref{fig:credfig}. Without the two, we have little or no basis to make conclusions from data. As they say, garbage in, garbage out. No amount of fancy statistical tactics can compensate for starting with bad data.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/credible} 

}

\caption{Components of credible analysis}\label{fig:credfig}
\end{figure}

\hypertarget{lo3}{%
\section{Learning objectives}\label{lo3}}

\begin{itemize}
\tightlist
\item
  Assess the measurement validity of variables
\item
  Assess the measurement reliability of variables
\item
  Explain the difference between accuracy and precision
\end{itemize}

\hypertarget{measurement-validity}{%
\section{Measurement validity}\label{measurement-validity}}

Data do not exist in nature. Rather, someone must set out to observe one or more phenomena, measure it, record it, and compile the recorded measures into a file or database. This process involves choices, limitations, and potential flaws. When evaluating the quality of data, the first quality to consider is the validity of the measure. Measurement validity can be considered from two similar yet distinct angles.

\begin{quote}
\textbf{Measurement Validity}: Does the variable measure the concept/phenomenon it is intended or claims to measure? Are the recorded values of the variable accurate measures of the true values of the variable?
\end{quote}

The first question above concerns conceptual accuracy. On matters of education policy, is GPA or standardized test scores a valid measure of our concept of intelligence? How about IQ? What do we even mean by intelligence? Perhaps we must clarify our concept as academic aptitude or a more observable concept like academic achievement. Are test scores a valid measure of teacher quality? Does the quality of a teacher amount to their students' academic achievement? On matters of public health, is body mass index (BMI; weight in kilograms divided by height in meters squared) a valid measure of a person's health? In public finance, are property values a valid measure of the quality of local public services, such as schools, parks, and police/fire departments? Is a city's bond rating a valid measure of its financial health? Is the unemployment rate a valid measure of economic performance or prosperity? Is the proportion of those below the federal poverty line a valid measure for the severity of poverty or financial stress?

The second question above concerns procedural accuracy. Regardless of whether we agree or disagree on what a variable actually measures, we must also consider whether the recorded values were accurately recorded. Is the number of sexual assaults in a dataset of city crime the actual number of sexual assaults that were committed? Is the recorded value of a property its actual fair market value? Is the recorded test score for a student their true test score? Procedural issues could be systemic or due to human error or manipulation. Sexual assaults along with almost all types of crime are systematically under-reported, thus recorded values are likely to be lower than the true value. Property value assessors may purposefully over- or under-value properties out of political or personal motivations. Or their inaccuracy could be accidental and due to a property being unique and difficult to assess. Data are also subject to input error depending on how they are recorded, receiving an errant decimal or 0.

\hypertarget{measurement-reliability}{%
\section{Measurement reliability}\label{measurement-reliability}}

Measurement reliability can be also be considered from two similar yet distinct angles.

\begin{quote}
\textbf{Measurement Reliability}: Provided no change in a subject's condition/reality, does the way the variable is measured generate the same value? Given identical conditions/realities between multiple subjects, does the measure generate identical values?
\end{quote}

The first question above concerns a measure's consistency for a single subject. A student receives a score on their GRE. Provided the same student does not study before taking the GRE again, will they receive the same score (referred to as test-retest reliability)? A property value assessor assesses a property. A second assessor conducts an assessment of the same property. Will they arrive at the same property value (referred to as inter-rater reliability)?

The second question above concerns a measure's consistency for multiple subjects with identical conditions. Suppose two students are equally intelligent, academically apt, or whatever the correct concept should be. Will the two students receive the same GRE score? Will two identical properties receive the same property value?

A measure can be valid or invalid and reliable or unreliable, resulting in one of four combinations. Let us consider an example where an agency needs to allocate resources to state governments according to the number of persons who are homeless in each state. Suppose the \emph{true} count of homeless persons for two states is the same. Figure \ref{fig:homeless} tries to visualize the scenario and the three potentially problematic combinations of validity and reliability along a number line.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/measure_lines} 

}

\caption{Comparing measurement validity and reliability}\label{fig:homeless}
\end{figure}

Though techniques to count the number of homeless persons are improving, one method has been to designate a specific day of the year (e.g.~January 1st) where government staff and volunteers attempt to conduct a census of homeless people. If these census takers in separate states successfully recorded the true count of homeless people, then our dataset would contain a valid and reliable measure. In the case where a valid and reliable measure is taken, the two states receive equal and appropriate amounts of resources.

If an invalid yet reliable measure is taken, as is depicted in the second number line from the top in Figure \ref{fig:homeless}, the two states receive equal resources, but the amount of resources is less (or more) than what it should be. If a valid and unreliable measure is taken, as is depicted by the third number line, then amount of resources provided is appropriate \emph{on average}, but the two states receive different amounts. Lastly, if an invalid and unreliable measure is taken, the two states receive different amounts and the amount of resources provided is systematically less (or more) than what it should be.

Taking a count of homeless people on a designated day is known to have issues of measurement validity and reliability. Validity is an issue at least procedurally because it is unlikely that a government can count all of its homeless people. Whether or not it is conceptually invalid depends on what we claim it measures. Why is this measure unreliable? Temperature affects how easily and accurately the number of homeless people can be counted. In a cold state, most homeless people will stay in shelters. In a warm state, homeless people will be scattered and more difficult to count. Even if a cold state and a warm state truly had equal number of homeless people, it is unlikely that the same count would be recorded.

The moral of this story is that when you collect data you did not generate yourself for your own purpose, take the time to consider if those data are valid and reliable measures for your intended purpose and what the consequences could be if they are not. Also, keep in mind that no measure is perfect. Our concern should not be so much whether a measure is valid vs.~invalid or reliable vs.~unreliable, but rather the degree to which a measure is invalid and unreliable.

\hypertarget{missing-data}{%
\section{Missing data}\label{missing-data}}

It is not uncommon to encounter missing values in a data. Respondents skip or choose not to answer survey questions, administrators fail to contact respondents, entities that reported data last year may have dissolved or consolidated with another entity this year. Many reasons can lead to missing data. The key is to consider why data are missing and if it should affect your conclusions.

Using the previous example of self-reported income, suppose there are numerous missing values in the responses. Should we assume they are missing at random, or that there is some underlying reason or pattern? Perhaps those with no or low income do not wish to report. If we were to dismiss these missing values, and draw conclusions from the non-missing values, we may severely overestimate the income of the target population.

\hypertarget{types-of-missing-data}{%
\subsection{Types of missing data}\label{types-of-missing-data}}

Missing data come in two varieties:

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{Explicit:} data that we can see are missing in the data; cells containing a value that denotes missing
\item
  \textbf{Implicit:} data that we would expect to be included based on data structure but are not; no obvious sign of missing
\end{itemize}
\end{quote}

Table 3.1 shows an example of data that are explicitly missing denoted by \texttt{NA}. Missing data is denoted in a variety of ways. For example, instead of \texttt{NA}, the cells could have been left empty, or filled with a period, or some other symbol. If data were obtained from an organization that regularly produces publicly available data, datasets are usually accompanied by a legend that explains what symbols denote missing.

\begin{table}

\caption{\label{tab:unnamed-chunk-1}Example of explicitly missing data}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
country & continent & year & lifeExp & pop & gdpPercap\\
\hline
Argentina & Americas & 2007 & NA & 40301927 & 12779.380\\
\hline
Bolivia & Americas & 2007 & 65.554 & 9119152 & NA\\
\hline
Brazil & Americas & 2007 & 72.390 & 190010647 & 9065.801\\
\hline
\end{tabular}
\end{table}

Beware ambiguous missing values. For instance, some survey questions are dependent on previous questions. You do not want to conclude that a value is missing because a respondent chose not to answer when they were never asked the question. Or perhaps a value is missing when it should actually equal 0 or vice versa. If missing data are consequential to your analysis, then you may need to investigate further into how the data were collected or coded in order to eliminate such ambiguity.

Table 3.2 shows an example of implicitly missing data. Argentina is observed in 1997, 2002, and 2007, but Bolivia is observed only in 1997 and 2007. What happened to the 2002 observation for Bolivia? This sort of entry and exit from the dataset is common in panel data where the same units are observed over multiple time periods.

\begin{table}

\caption{\label{tab:unnamed-chunk-2}Example of implicitly missing data}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
country & continent & year & lifeExp & pop & gdpPercap\\
\hline
Argentina & Americas & 1997 & 73.275 & 36203463 & 10967.282\\
\hline
Argentina & Americas & 2002 & 74.340 & 38331121 & 8797.641\\
\hline
Argentina & Americas & 2007 & 75.320 & 40301927 & 12779.380\\
\hline
Bolivia & Americas & 1997 & 62.050 & 7693188 & 3326.143\\
\hline
Bolivia & Americas & 2007 & 65.554 & 9119152 & 3822.137\\
\hline
\end{tabular}
\end{table}

Note that the missing Bolivia observation was easy to spot because the dataset is extremely small. If we were dealing with a large dataset, this would not have been so obvious. A quick way to check whether there may be implicitly missing observations is to check the number of observations in your data. If you are under the impression that your data contains all 50 states for 10 years, then you should have 500 observations. If not, some states or years must be missing.

\begin{quote}
\textbf{To learn how to work with missing data in R, proceed to Chapter \ref{r-missing-data}.}
\end{quote}

\hypertarget{kt2}{%
\section{Key terms and concepts}\label{kt2}}

\begin{itemize}
\tightlist
\item
  Measurement validity
\item
  Measurement reliability
\item
  Measurement precision
\item
  Implicitly missing data
\item
  Explicitly missing data
\end{itemize}

\hypertarget{descriptive-statistics}{%
\chapter{Descriptive Statistics}\label{descriptive-statistics}}

\begin{quote}
\emph{"Just the facts, ma'am.}

---Joe Friday, Dragnet
\end{quote}

\hypertarget{lo4}{%
\section{Learning objectives}\label{lo4}}

\begin{itemize}
\tightlist
\item
  Explain the difference between descriptive and inferential statistics
\item
  Explain the difference between a population and sample; parameter and statistic
\item
  Understand a distribution of a random variable
\item
  Explain and apply the descriptive measures of center, spread, and association
\item
  Choose the preferable measures of center and spread given a distribution and explain why
\item
  Determine the direction and strength of association given a scatterplot or correlation coefficient
\item
  Explain the possible shortcomings of correlation
\end{itemize}

\hypertarget{two-goals-of-statistics}{%
\section{Two goals of statistics}\label{two-goals-of-statistics}}

The discipline of statistics has one or both of the following goals:

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{Descriptive statistics:} summarizes the qualities of observed data in a sample or population, describing distributions of variables or the relationship between two or more variables.
\item
  \textbf{Inferential statistics:} uses observed data in a sample to make inferences/conclusions about an unobserved population.
\end{itemize}
\end{quote}

The descriptive or inferential statistics we produce or consume concern one or both of the following groups:

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{Population:} all members of a specified group pertaining to a research question
\item
  \textbf{Sample:} a subset of that population
\end{itemize}
\end{quote}

Descriptive statistics provides information about the data we have. Inferential statistics uses that information to make educated, scientific guesses about a larger group of people, places, or things we do not directly observe. In many cases, we cannot study an entire population because of logistics or cost. Instead, we take a sample of that population to make inferences about it.

If my research question was, ``What is the average GPA of all MPA students in the United States?'', then the population is all MPA students in the United States. It is unlikely I could obtain the GPA of every MPA student. Therefore, I may take a sample of MPA students instead and use inferential statistics to make conclusions about the population of all MPA students.

Sometimes the population is small or accessible enough to observe, though it is probably uninteresting as a result. If my research question was instead, ``What is the average GPA of students in my class?'', then the population is the students in my class. In this case, I could easily compute the average for the entire population.

\begin{quote}
We can describe a sample or a population as long as we have the data to do so. Inference is specifically using a sample we observe to describe a population we do not observe.
\end{quote}

When we compute statistical measures of a population or sample, those measures have specific names:

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{Parameter:} a measure pertaining to a population
\item
  \textbf{Statistic:} a measure pertaining to a sample
\end{itemize}
\end{quote}

Whether my statistical measure is a population parameter or sample statistic depends on whether my measure is computed from a population or a sample. If my population is students in my class, and I compute the average GPA for all of the students in my class, that measure is a population parameter. If my population is all MPA students in the U.S., and I use the students in my class as a sample, then the average GPA of the students in my class is a sample statistic.

In inference, a sample statistic is often referred to as an \textbf{estimate} because it is used to estimate a population parameter. The parameter in this example would be the actual average GPA of all MPA students in the U.S.; a value I cannot directly calculate.

\hypertarget{distributions}{%
\section{Distributions}\label{distributions}}

The goal of descriptive statistics is to summarize characteristics of variable distributions. Before reviewing the measures used to summarize distributions, we should understand what a distribution is.

\begin{quote}
A \textbf{distribution} tells us the (possible) values of a variable and the frequency at which those values occur.
\end{quote}

The values of a variable are the result of some random data-generating process. If it wasn't random, and instead deterministic, then there would be no uncertainty in the world. You do not know if you will get a job that requires your degree before you get the degree. An HR department does not know if an implicit bias workshop will reduce the number of racial insensitivity complaints before providing the workshop and measuring the number of complaints, nor does it know how many complaints occur at all before they are made. All of these are variables with some range of possible values, each of which occurs at some frequency. These frequencies are revealed to us when we take measures of the variable.

Sometimes we know all the possible values of a variable, or at least the range of possible values. We know a variable for biological sex has possible values of male or female. We know a variable for GPA has a possible range of 0 to 4, in most cases.

Sometimes we know what the frequency of values for a variable should be. Genetics tells us to expect 50\% males and females. Most of the time we don't know the function that determines frequency, or it is too complex. For example, we have some idea of the factors that influence GPAs, but there will always be some randomness that goes unaccounted.

This somewhat esoteric exposition underlies the main focus here: the distribution of a variable. To make this as concrete as possible, let's consider a variable of something that is simple and familiar to all of us: a roll of a six-sided die.

We know a roll of a six-sided die can take on a range of integers between 1 and 6. We also know the frequency of each value is the same at 1 in 6, or about 17\%. Therefore, we \emph{know} the distribution of this variable, which is depicted below in Figure \ref{fig:diedist}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/diedist-1} 

}

\caption{Probability distribution of a six-sided die}\label{fig:diedist}
\end{figure}

Therefore, if we were to roll the die six times, we would \emph{expect} the following data in Table \ref{tab:dietable}, though not necessarily in this order.

\begin{table}

\caption{\label{tab:dietable}Expected results of 6 rolls}
\centering
\begin{tabular}[t]{r|r}
\hline
roll & value\\
\hline
1 & 1\\
\hline
2 & 2\\
\hline
3 & 3\\
\hline
4 & 4\\
\hline
5 & 5\\
\hline
6 & 6\\
\hline
\end{tabular}
\end{table}

And we could represent this distribution by counting the number of times each value occurred using a histogram as shown in Figure \ref{fig:diehisto}, which is the essentially the samce as Figure \ref{fig:diedist}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/diehisto-1} 

}

\caption{Expected distribution of 6 rolls}\label{fig:diehisto}
\end{figure}

Of course, this is just what is expected to happen, on average, given many rolls of a die. Anyone who has played a board game knows streaks can occur. Given a number or rolls, we probably will not observe a uniform number of values.

Suppose we roll 12 times and record the value of each roll, as is shown in Table \ref{tab:obsdietable}.

\begin{table}

\caption{\label{tab:obsdietable}Observed results of 12 rolls}
\centering
\begin{tabular}[t]{r|r}
\hline
roll & value\\
\hline
1 & 5\\
\hline
2 & 2\\
\hline
3 & 4\\
\hline
4 & 2\\
\hline
5 & 5\\
\hline
6 & 1\\
\hline
7 & 1\\
\hline
8 & 4\\
\hline
9 & 3\\
\hline
10 & 2\\
\hline
11 & 5\\
\hline
12 & 1\\
\hline
\end{tabular}
\end{table}

We can visualize the distribution of these 12 rolls, as is done in Figure \ref{fig:obsdieroll}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/obsdieroll-1} 

}

\caption{Observed distribution of 12 die rolls}\label{fig:obsdieroll}
\end{figure}

Here we can see the randomness of the variable. Values 1, 2, and 5 occur more frequently than 3 and 4, and 6 does not occur at all. If we were to roll the die many more times, it would look more like the distribution we would expect. But for \emph{this} sample of die rolls, the distribution is unique.

This is exactly the point of descriptive statistics: whether or not we know what to expect in terms of a variable's distribution, we want to know the characteristics of the distribution for a variable from a particular sample or population. When we ask for, say, a variable's average, we are asking for the approximate midpoint of that variable's distribution.

Descriptive measures help us summarize characteristics of distributions and some serve as the building blocks for other descriptive measures as well as inferential statistics.

\hypertarget{descriptive-measures}{%
\section{Descriptive Measures}\label{descriptive-measures}}

A die roll is uninteresting and unimportant. In our review of descriptive measures, let us consider them with respect to the distribution of the infant mortality rate across 222 countries in 2012. Infant mortality is the number deaths of infants under one year old per 1,000 live births. It is used as a measure of health in a country.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/infmort-1} 

}

\caption{Infant Mortality Rates}\label{fig:infmort}
\end{figure}

We could simply show the entire distribution of values, but it is usually helpful to summarize key characteristics of it. We can describe distributions along three dimensions:

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{Center:} what is the typical value of this variable?
\item
  \textbf{Spread:} how far away are values typically from the center?
\item
  \textbf{Association:} what is the typical value or spread of the distribution given a value of another variable?
\end{itemize}
\end{quote}

Multiple descriptive measures can answer the three questions above. Which measure is more appropriate to use largely depends on the shape of the distribution.

\hypertarget{measures-of-center}{%
\subsection{Measures of center}\label{measures-of-center}}

\hypertarget{mean}{%
\subsubsection*{Mean}\label{mean}}
\addcontentsline{toc}{subsubsection}{Mean}

The mean or average takes the values of a variable, adds them, then divides that sum by the total count of values.

\begin{equation}
{\displaystyle \bar{x}={\frac {1}{n}}\sum _{i=1}^{n}x_{i}={\frac {x_{1}+x_{2}+\cdots +x_{n}}{n}}}
\label{eq:mean}
\end{equation}

Infant mortality has 222 values, so \(n\) in equation \eqref{eq:mean} above would equal 222 in this case. If we were to pluck one country out of our pool of 222 countries at random, the mean tells us the infant mortality rate to expect. In other words, the mean tells us the typical infant mortality rate in our observed data. In this case, the average infant mortality rate is 26.7 per 1,000 live births.

\hypertarget{median}{%
\subsubsection*{Median}\label{median}}
\addcontentsline{toc}{subsubsection}{Median}

If we took the 222 infant mortality rates and listed them in ascending or descending numerical order, the median is the value that sits at the middle of the ordered list. The median is also referred to as the 50th percentile because half of the values fall below it and half of the values fall above it. In the case of an even number of values, there is no naturally occurring middle value. In that case, we take the average of the two values in the middle. The median infant mortality rate is 15.6.

\hypertarget{mode}{%
\subsubsection*{Mode}\label{mode}}
\addcontentsline{toc}{subsubsection}{Mode}

The mode is the value that occurs most frequently. If all values occur only once, then a variable has no mode. If two or more values occur an equal number of times and it is more than other values, then a variable has two or more modes. For instance, the modes for our 12 die rolls in Figure \ref{fig:obsdieroll} are 1, 2, and 5. The mode for infant mortality rates is 11.6.

\hypertarget{choosing-a-center}{%
\subsubsection*{Choosing a center}\label{choosing-a-center}}
\addcontentsline{toc}{subsubsection}{Choosing a center}

As Figure \ref{fig:infmort} shows, three measures of center have provided us three different typical infant mortality rates. The mean is represented by the red line, the median by the purple line, and the mode by the green line.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/mortcenters-1} 

}

\caption{Centers of infant mortality rates}\label{fig:mortcenters}
\end{figure}

Which should we choose to report? We could report all three, but we should apply some professional judgment to this dilemma. After all, people generally do not like nuance and prefer one best number instead of deciphering the meaning from three, assuming they understand all three measures in the first place.

For continuous variables reported at several decimal places, a value may not occur more than once because of the precision. More importantly, because of this low probability of repeat values, the mode is not guaranteed to represent a typical value. If it only takes two occurrences to qualify as the mode, that second occurrence could be an extreme value.

\begin{quote}
The mode is rarely used to describe continuous variables. The mode is commonly used to report the most frequent value of categorical or discrete variables with relatively few possible values, such as race, sex, political party, or number of TVs in a household.
\end{quote}

When choosing between mean and median, it usually comes down to the presence of extreme values or the extent to which a distribution is \textbf{skewed}. Skew pertains to the tails of a distribution--the taper to the left and right of its center. If the right or left tail extends far out from the center, we consider the distribution to be right- or left-skewed. Our distribution of infant mortality rates is right-skewed.

\begin{quote}
When a distribution is skewed, the median is generally a better choice for reporting its center than the mean. This is because the mean is sensitive to extreme values.
\end{quote}

Note in Figure \ref{fig:mortcenters} that the mean is pulled to the right by the right-skew of extreme values. The red line representing the mean is to the right of the cluster of frequent values and may not be a good answer for the typical value of this distribution. The median is not sensitive to extreme values. No matter how far the values above the median were to stretch to the right, the median of the distribution would not change.

\hypertarget{measures-of-spread}{%
\subsection{Measures of spread}\label{measures-of-spread}}

Measures of center convey the typical value of a distribution. The typical infant mortality rate is 26.7 or 15.6 depending on whether we choose to use mean or median, respectively. If we only had this measure, we would have no idea how far away the values are from the center. Are the infant mortality rates of most countries close to this center, or is the typical value not representative of most countries' infant mortality rates? Measures of spread provide us this information.

\hypertarget{variance}{%
\subsubsection*{Variance}\label{variance}}
\addcontentsline{toc}{subsubsection}{Variance}

Almost all values of a numerical variable, especially a continuous variable, do not equal the mean. The difference between a particular value and the mean of the variable is often referred to as \textbf{deviation from the mean}. The variance squares each observation's deviation from the mean, sums all the deviations, and divides this sum by the total count of observations minus one. Equation \eqref{eq:variance} displays this process using mathematical notation.

\begin{equation}
{\displaystyle S^2={\frac {1}{n-1}}\sum _{i=1}^{n}(x_{i}-\bar{x})^2={\frac {(x_{1}-\bar{x})^2+(x_{2}-\bar{x})^2+\cdots +(x_{n}-\bar{x})^2}{n-1}}}
\label{eq:variance}
\end{equation}

The mean infant mortality rate is 26.7. If we subtract this mean from each country's rate, we have each country's deviation from the mean, some of which is shown in Table \ref{tab:varcalcs}. Then, we square these deviations as is also shown in the table. We then sum the 222 squared deviations and divide by 221. The variance for our infant mortality rates is 672.6.

\begin{table}

\caption{\label{tab:varcalcs}Excerpt of variance calculations}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
country & inf\_mort\_rate & deviate & sq\_deviate\\
\hline
Afghanistan & 121.63 & 94.93 & 9011.705\\
\hline
Niger & 109.98 & 83.28 & 6935.558\\
\hline
Mali & 109.08 & 82.38 & 6786.464\\
\hline
Somalia & 103.72 & 77.02 & 5932.080\\
\hline
Central African Republic & 97.17 & 70.47 & 4966.021\\
\hline
\end{tabular}
\end{table}

\hypertarget{standard-deviation}{%
\subsubsection*{Standard deviation}\label{standard-deviation}}
\addcontentsline{toc}{subsubsection}{Standard deviation}

Variance is an important building block for inference, but it is essentially useless as a descriptive measure because it is in squared units. If someone asks how far values are spread out from the mean, it would not help much to report values deviate from the mean by 672 squared-deaths.

The standard deviation is simply the square root of variance, which returns our units to their original meaning.

\begin{equation}
{\displaystyle s = \sqrt{S^2}}
\label{eq:sd}
\end{equation}

The standard deviation in the infant mortality rates data is 25.9. This tells us that, on average, infant mortality rates are about 26 deaths above or below the mean.

\begin{quote}
The inuition of the mean and standard deviation is useful for understanding probability and inferential statistics. The mean is the typical value of a variable. The standard deviation is the typical deviation from the mean. If we had to guess a value drawn randomly from a variable, our best guess is the mean as long as the variable is not highly skewed. If we had to guess how far off that randomly drawn value will be from the mean, our best guess is the standard deviation.
\end{quote}

\hypertarget{interquartile-range}{%
\subsubsection*{Interquartile range}\label{interquartile-range}}
\addcontentsline{toc}{subsubsection}{Interquartile range}

Recall that the median is the 50th percentile of a distribution--half of the values fall below the median and half fall above it. Two additional percentiles sometimes reported are the 75th and 25th percentiles. The 75th percentile is the value at which 75\% of values fall below and 25\% fall above it, while the 25th percentile is the value at which 25\% of values fall below and 75\% fall above it.

The IQR is equal to the 75th percentile minus the 25th percentile, thus providing the range that captures the middle 50\% of the values in the distribution. The IQR for infant mortality rates is 35.6. Alternatively, the IQR can be reported by specifying the 75th and 25th percentiles, leaving the consumer to compute the difference between the two. The 75th percentile for infant mortality rates is 42.1, while the 25th percentile is 6.5.

\hypertarget{range}{%
\subsubsection*{Range}\label{range}}
\addcontentsline{toc}{subsubsection}{Range}

The range is the maximum value in a distribution minus the minimum value of a distribution. Usually, the range is left implied in a table of summary statistics by reporting the maximum and minimum without differencing the two. The minimum of infant mortality rates is 1.8, and the maximum is 121.63. Therefore, the range of the distribution is 119.8.

\hypertarget{choosing-a-measure-of-spread}{%
\subsubsection*{Choosing a measure of spread}\label{choosing-a-measure-of-spread}}
\addcontentsline{toc}{subsubsection}{Choosing a measure of spread}

The same logic applies to choosing a measure of spread as choosing a measure of center. The standard deviation is based on the mean, and so it is also sensitive to extreme values that, if present, could exaggerate the typical spread of the distribution. The IQR is based on percentiles just like the median. Therefore, IQR is not sensitive to extreme values.

Figure \ref{fig:mortspread} displays the mean and plus-and-minus one standard deviation using red dashed and solid lines, respectively. The median and the IQR (25th and 75th percentiles) are represented by the purple dashed and solid lines, respectively. Note how wide the area contained by the standard deviation is--it contains most of the distribution and the lower bound of 0.7 is lower than the minimum observed value of 1.8.

Standard deviation is arguably not good for conveying the typical deviation from the center, as it contains plenty of values that are rather atypical deviations from the center. In the case of describing the distribution of infant mortality rates, the median and IQR are probably a better choice.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/mortspread-1} 

}

\caption{Center and spread of infant mortality rates}\label{fig:mortspread}
\end{figure}

In most cases, the range (or minimum and maximum values) should be reported along with either the mean and standard deviation or median and IQR (or 25th and 75th percentiles), especially when a distribution is skewed. In addition to signaling the skew of a distribution, the range helps convey what may be the possible values of a variable and how different the most different units in the data are with respect to that variable.

In the case of infant mortality rates, we know the minimum possible value is 0 by definition, but the minimum value in our distribution is 1.8. Perhaps 0 deaths is unrealistic for any country. Moreover, the maximum value is 121.63. This range, along with the median and IQR, tells us the most different countries are \emph{very} different.

\hypertarget{outliers}{%
\subsection{Outliers}\label{outliers}}

Outliers refer to weird observations in our data whose inclusion may affect the conclusions we draw from our analysis. There are at least two points about outliers that can cause confusion and mistakes:

\begin{itemize}
\tightlist
\item
  There is no definitive threshold at which an observation can becomes an outlier.
\item
  Outliers should not necessarily be excluded from an analysis. It depends on the context. We may only care about making conclusions for typical cases. If so, removing outliers may be warranted. However, atypical cases may be an important part of the story.
\end{itemize}

While there is no single definition of an outlier, the most common definition uses \(1.5 \times IQR\). Values that are more than \(1.5 \times IQR\) below the 25th percentile are outliers on the left side of the distribution. Values that are more than \(1.5 \times IQR\) above the 75th percentile are outliers on the right side of the distribution.

Recall that the IQR of infant mortality rates depicted in Figure \ref{fig:mortspread} was 35.6 with 25th and 75th percentiles of 6.5 and 42.1, respectively. Applying the above definition, 1.5 times 35.6 equals 53.5. Therefore, values that fall below -46.9 ( \(6.5 - 53.4\) ) are outliers, but this is impossible given our variable is infant mortality rates. Values above 95.5 ( \(42.1 + 53.4\) ) are outliers on the right end of the distribution. Figure \ref{fig:mortspread} indicates there are a few outliers in our data based on this definition.

\hypertarget{the-normal-distribution}{%
\subsection{The normal distribution}\label{the-normal-distribution}}

As a brief aside, it should be mentioned that if a distribution is \textbf{normal}, then measures of center and spread will be similar to each other. This is one of several desirable features of the normal distribution.

Figure \ref{fig:normrates} shows a simulated scenario in which the infant mortality rates in our 222 countries exhibit a normal distribution. Note the peaks in the center and symmetry. There is no skew.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/normrates-1} 

}

\caption{Simulated normal distribution of infant mortality rates}\label{fig:normrates}
\end{figure}

Table \ref{tab:normratesum} confirms the similarity between measures of center and spread for this simulated distribution. This is one reason it is important to visualize your distribution. If it appears approximately normal, then you should report the mean and standard deviation (along with minimum and maximum values), as the are more widely understood.

\begin{table}

\caption{\label{tab:normratesum}Center and spread measures of simulated data}
\centering
\begin{tabular}[t]{r|r|r|r|r}
\hline
Mean & Median & Mode & SD & IQR\\
\hline
26 & 25.5 & 22.1 & 6.6 & 8.4\\
\hline
\end{tabular}
\end{table}

Again, the normal distribution has several desirable features that will be discussed further in the chapters pertaining to inference. One is that if a distribution is approximately normal, then extreme values are not a concern and the mean and standard deviation are good measures of center and spread, respectively. Besides making our choice of measures convenient, why is this worth repeating? Because the mean and standard deviation are building blocks to the next category of descriptive measures: association. If mean and standard deviation are bad choices of center and spread, then our measures of association will be negatively affected.

\hypertarget{measures-of-association}{%
\subsection{Measures of association}\label{measures-of-association}}

With association, we now consider the distributions of two variables at a time. That is, given the value within one variable's distribution, what does the distribution of another variable look like?

We need a second variable to continue our example involving infant mortality rates. Table \ref{tab:gapdeathtab} shows a preview of a dataset that adds two more variables to our previous infant mortality data.

\begin{table}

\caption{\label{tab:gapdeathtab}First five rows of country data}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
country & inf\_mort\_rate & lifeExp & gdpPercap\\
\hline
Afghanistan & 121.63 & 43.828 & 974.5803\\
\hline
Niger & 109.98 & 56.867 & 619.6769\\
\hline
Mali & 109.08 & 54.467 & 1042.5816\\
\hline
Somalia & 103.72 & 48.159 & 926.1411\\
\hline
Central African Republic & 97.17 & 44.741 & 706.0165\\
\hline
\end{tabular}
\end{table}

Recalling that the mean infant mortality rate is about 26, the five countries included are in the right tail of the distribution. Also, you probably know enough about life expectancy to know that the values for these countries are quite low. Perhaps these two variables share a relationship?

In fact, we know they do. Life expectancy in a given year is the average age at which people in that country died. If a country has a high frequency of infants dying, then that will pull the mean downward. A common misunderstanding of life expectancy is that people in that country tend to die at the age of the country's life expectancy. This is certainly not the case if a country has a high infant mortality rate. While adults in countries with low life expectancy may die somewhat younger (or much younger if it is a war-torn country), adults tend to live longer than the average life expectancy. The key is making it out of infancy alive.

\hypertarget{visual-association}{%
\subsubsection*{Visual association}\label{visual-association}}
\addcontentsline{toc}{subsubsection}{Visual association}

As was the case with one variable, we want to visualize the distributions of two variables. When working with two continuous variables, the \textbf{scatter plot} is the most common choice to visualize association between two variables. Figure \ref{fig:scatterlifedeath} plots the paired values of infant mortality rate and life expectancy for each country.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/scatterlifedeath-1} 

}

\caption{Visualizing association between two continuous variables}\label{fig:scatterlifedeath}
\end{figure}

Note that I plotted infant mortality rate along the x axis and life expectancy on the y axis. This choice was deliberate. If we suspect that one variable influences or affects the value of another variable, then the variable doing the influencing should be plotted on the x axis. Plotting a variable on the y axis implies to the viewer that it responds to the variable on the x axis.

Figure \ref{fig:scatterlifedeath} confirms our suspicion that the two variables are associated. There appears to be a rather strong association such that as infant mortality rate increases, the lower a country's life expectancy.

\hypertarget{quantified-association}{%
\subsubsection*{Quantified association}\label{quantified-association}}
\addcontentsline{toc}{subsubsection}{Quantified association}

As was the case with one variable, we want to describe the association between two variables using quantitative measures. The association between two or more variables can be described in terms of

\begin{itemize}
\tightlist
\item
  \textbf{Direction:} when one variable increases, does the other variable increase or decrease?
\item
  \textbf{Strength:} how much do the variables seem to be tied together?
\item
  \textbf{Magnitude:} given an specific increase or decrease in one variable, by how much does the other variable increase or decrease?
\end{itemize}

There are several measures one can use to answer the above question.

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{Covariance:} measures direction of association between between two variables
\item
  \textbf{Correlation coefficient:} measures direction and strength of association between two variables
\item
  \textbf{Regression coefficient:} measures the direction and magnitude of association between an explanatory variable and an outcome variable
\item
  \textbf{Coefficient of determination ( \(R^2\) ):} measures the strength of association between a set of one or more explanatory variables and an outcome variable
\end{itemize}
\end{quote}

The regression coefficient and coefficient of determination are discussed in Chapter \ref{simple-and-multiple-regression} involving regression models. Let us briefly consider covariance and correlation.

\hypertarget{covariance}{%
\subsubsection*{Covariance}\label{covariance}}
\addcontentsline{toc}{subsubsection}{Covariance}

Covariance tells us when one variable, \(X\), is above or below its mean, whether another variable, \(Y\), tends to be above or below its mean. If \(Y\) tends to be above (below) its mean when \(X\) is above (below) its mean, then the two have a positive covariance and are positively associated. If \(Y\) tends to be below (above) its mean when \(X\) is above (below) its mean, then the two have a negative covariance and are negatively associated. If the two variables exhibit no tendencies, they have a covariance of 0 and thus no association.

Figure \ref{fig:scattercovar} adds references lines for the mean of each variable. Note that when infant mortality is above its mean (to the right of the red line), life expectancy is below its mean (below the purple line) in almost all cases. When infant mortality is below its mean, life expectancy is above its mean in almost all cases. Therefore, these two variables have a negative covariance and are negatively associated. In fact, the covariance between infant mortality rate and life expectancy is -312.7

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/scattercovar-1} 

}

\caption{Visualizing covariance}\label{fig:scattercovar}
\end{figure}

Covariance is the association analog of variance. It is an important building block of other measures of association, but it is essentially useless for description because it only tells us the direction of association. Correlation tells us the direction and strength of association. Therefore, covariance is never used for description because correlation provides us twice as much information.

\hypertarget{correlation}{%
\subsubsection*{Correlation}\label{correlation}}
\addcontentsline{toc}{subsubsection}{Correlation}

If \(Y\) tends to increase (decrease) as \(X\) increases (decreases), then the two are positively correlated. That is, the two variables tend to move in the same direction. If \(Y\) tends to increase (decrease) as \(X\) decreases (increases), then the two are negatively correlated. That is, the two variables tend to move in opposite directions. Correlation tells us how much the paired values of two variables in a scatterplot exhibit a straight line and whether that straight line is positively or negatively sloped.

The correlation coefficient ranges between -1 and 1. If it is negative, then the two variables are negatively associated. If it is positive, the two variables are positively associated. The closer the correlation coefficient of two variables is to -1 or 1, the stronger their correlation and the more the two variables exhibit a straight line in a scatterplot. A correlation equal to -1 or 1 indicates the two variables form a perfect straight line. If two variables exhibit no shared tendencies and form what appears to be a random scattering of plot points, than their correlation will be close or equal to 0.

Based on the covariance and Figure \ref{fig:scattercovar}, we know to expect a negative correlation between infant mortality rate and life expectancy. We also know the correlation will not be -1 because the points do not form a perfect straight line. Nevertheless, they do form a fairly tight downward path, so we should expect a correlation closer to -1 than 0. It turns out that the correlation is equal to -0.9. Infant mortality rate and life expectancy exhibit a strong, negative association.

If we imagined drawing a line through the data points on our scatterplot from left to right that could freely curve according to however the data are scattered, would that line be a straight line and would it slope upward or downward? Figure \ref{fig:scattercorr} does exactly that with our data. Note that the data points lead the line to slope downward almost throughout the range of observed values. In the upper-left quadrant, the data points are tightly clustered around the line, indicating a strong correlation. In the bottom-right quadrant, the data points begin to spread further away from the line, indicating a weaker correlation. The line also begins to turn in the positive direction, which lowers the correlation coefficient.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/scattercorr-1} 

}

\caption{Drawing a free line through the data}\label{fig:scattercorr}
\end{figure}

The correlation coefficient has three qualities that can lead to misunderstandings or mistakes. First, \textbf{correlation is sensitive to extreme values}. A few points on a scatterplot can impose undue influence on the line that is drawn through the data, causing the correlation coefficient to increase or decrease dramatically. Second, \textbf{correlation measures only the linear association}. If two variables formed a perfect U-shape in a scatterplot, they are strongly associated. However, their correlation coefficient would suggest a weaker relationship because a straight line does not fit a U-shape well. Third, \textbf{correlation is a necessary but not sufficient condition for causality}. In order to validly claim that a change in the value of one variable \emph{causes} the values of another variable to change, they must be correlated, but a few more conditions must also be met. Those conditions are discussed in Chapter \ref{causation-and-bias}.

\begin{quote}
\textbf{To learn how to produce a summary table for a publication, proceed to Chapter \ref{r-description}.}
\end{quote}

\hypertarget{kt4}{%
\section{Key terms and concepts}\label{kt4}}

\begin{itemize}
\tightlist
\item
  Descriptive statistics
\item
  Inferential statistics
\item
  Population
\item
  Sample
\item
  Parameter
\item
  Statistic
\item
  Estimate
\item
  Distribution
\item
  Mean
\item
  Median
\item
  Mode
\item
  Skewed distribution
\item
  Standard deviation
\item
  Interquartile range
\item
  Range
\item
  Correlation
\end{itemize}

\hypertarget{data-visualization}{%
\chapter{Data Visualization}\label{data-visualization}}

\begin{quote}
\emph{``The pen is mightier than the sword, especially if it draws a graph.''}
\end{quote}

The world of data visualization is incredibly diverse and detailed. You could spend a substantial amount of time learning how to construct the best visualization given particular data and the the intended message. Such depth is far beyond the scope of this book. For more coverage on data visualization, I recommend the following resources:

\begin{itemize}
\tightlist
\item
  \href{https://flowingdata.com}{Flowing Data}
\item
  \href{https://socviz.co/index.html\#preface}{Data Viz by Kieran Healy}
\end{itemize}

At a basic level, most choices of visualization can be determined based on:

\begin{itemize}
\tightlist
\item
  The kind of description or comparison we want to visualize, and
\item
  the kinds of variables involved in the visualization.
\end{itemize}

Figure \ref{fig:vizflow} below provides a decision tree organized by according to these two considerations.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/vizflow} 

}

\caption{Basic data viz decisions}\label{fig:vizflow}
\end{figure}

\hypertarget{lo5}{%
\section{Learning objectives}\label{lo5}}

\begin{itemize}
\tightlist
\item
  Interpret the six common visualizations from Figure \ref{fig:vizflow}

  \begin{itemize}
  \tightlist
  \item
    Histogram
  \item
    Box plot
  \item
    Pie chart
  \item
    Bar chart or dot plot
  \item
    Line graph
  \item
    Scatter plot
  \end{itemize}
\item
  Recommend an appropriate type of visualization given the intended message and data
\end{itemize}

\hypertarget{distribution}{%
\section{Distribution}\label{distribution}}

\hypertarget{histogram}{%
\subsection{Histogram}\label{histogram}}

You have already seen several histograms. A histogram visualizes the distribution of a single variable by counting the number of occurrences for values that fall within a certain range. The frequency of occurrences within each range is represented by a vertical rectangle.

Figure \ref{fig:gradmedhist} shows the median earnings of those employed full time for different graduate degree majors. We can see that most graduate degrees result in a median pay for graduates of between 60 and 80 thousand dollars. There are a few graduate majors for which the median pay is above 100 thousand dollars.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/gradmedhist-1} 

}

\caption{Histogram of full-time median earnings for different graduate school majors}\label{fig:gradmedhist}
\end{figure}

These rectangles are called \texttt{bins} and the range each rectangle covers is called a \texttt{binwidth}. We can specify the number of bins and/or the binwidth. If we have more than 150 observations of a continuous variable, we may want to specify as many as 100 bins but should experiment with this number depending on the particular distribution of the variable. If we have less than 30 observations, we should not use a histogram. If we have more than 30 but less than 150 observations, we should experiment with some number of bins between 30 and 100. Regarding binwidth, if our variable is discrete, then our binwidth should equal the natural integer width. For example, if our variable is a count of weeks, then our binwidth should equal 1 so that each bin contains one week.

\hypertarget{box-plot}{%
\subsection{Box plot}\label{box-plot}}

A box plot (or box-and-whiskers plot) is similar to the histogram and density plot, but a box plot tries to combine a complete view of a distribution and several visual markers denoting some of the descriptive measures covered in Chapter \ref{descriptive-statistics}. Figure \ref{fig:gradmedbox} shows the median pay data.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/gradmedbox-1} 

}

\caption{Box plot of full-time median earnings for different graduate school majors}\label{fig:gradmedbox}
\end{figure}

The line in the middle of the box denotes the median of the variable's distribution. The top and bottom edges of the box denote the 75th and 25th percentiles, respectively. Therefore, the length of the box denotes the IQR of the variable's distribution. The whiskers of a boxplot extend 1.5 times the length of the box (IQR). This 1.5*IQR is a standard threshold to identify extreme values also known as outliers. If a variable contains values beyond this threshold, a box plot will single them out with dots beyond the end of the whisker.

\hypertarget{compostion-of-a-category}{%
\section{Compostion of a category}\label{compostion-of-a-category}}

Suppose we deemed a graduate degree for which 5\% or more of its graduates are unemployed to be a ``high'' unemployment degree, and those with an unemployment rate less than 5\% as a ``low'' unemployment degree. We have 173 graduate degree majors. Suppose we want to visualize the composition of this categorical unemployment variable.

\hypertarget{pie-charts}{%
\subsection{Pie charts}\label{pie-charts}}

Pie charts are much derided. This derision is due to the fact that pie charts are often misused. Pie charts are acceptable if you want to show the composition of one categorical variable for which there are no more than 3 levels, though preferably no more than 2 levels. We should \emph{never} use pie charts to compare the composition of a categorical variable between two groups or time periods. Figure \ref{fig:emppie} shows the composition of our unemployment variable.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/emppie-1} 

}

\caption{Graduate degrees with high/low unemployment}\label{fig:emppie}
\end{figure}

\hypertarget{bar-chart}{%
\subsection{Bar chart}\label{bar-chart}}

Bar charts can be used to present the same information as a pie chart. Moreover, bar charts are easier to interpret, can handle any number of levels, can present data as proportions or total counts, can be used to compare across groups or time, and are easier to make. In short, bar charts are better than pie charts, and we should choose bar charts unless someone forces us to use a pie chart for some reason.

The figures below show the three general types of bar charts. Figures \ref{fig:empbar} and \ref{fig:empbar2} show the composition of our unemployment variable in terms of absolute counts. Figure \ref{fig:empbar} is commonly referred to as a \texttt{stacked} bar chart, while Figure \ref{fig:empbar2} is referred to as \texttt{dodged}. Figure \ref{fig:empbar3} shows the composition in terms of proportions. That is, we can see that slightly over 75\% of graduate degrees have low unemployment.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/empbar-1} 

}

\caption{Graduate degrees with high/low unemployment}\label{fig:empbar}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/empbar2-1} 

}

\caption{Graduate degrees with high/low unemployment}\label{fig:empbar2}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/empbar3-1} 

}

\caption{Graduate degrees with high/low unemployment}\label{fig:empbar3}
\end{figure}

\hypertarget{comparing-between-units}{%
\section{Comparing between units}\label{comparing-between-units}}

\hypertarget{bar-chart-1}{%
\subsection{Bar chart}\label{bar-chart-1}}

If we want to compare one variable across multiple groups or units that are not time, then a bar chart is a good choice.

Suppose we wanted to compare the median pay between two or more graduate degrees.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/spiapay-1} 

}

\caption{Comparison of median pay between degrees in public and international affairs}\label{fig:spiapay}
\end{figure}

\hypertarget{dot-plot}{%
\subsection{Dot plot}\label{dot-plot}}

Dot plots serve the same purpose as bar charts, but are more appropriate for variables that measure something we would not naturally stack up for counting purposes. That is, money is stackable--we could imagine each bar as a stack of cash. By contrast, unemployment rates are not somethings we would stack on top of each other.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/spiaemp-1} 

}

\caption{Comparison of unemployment rates between degrees in public and international affairs}\label{fig:spiaemp}
\end{figure}

\hypertarget{line-graph}{%
\subsection{Line graph}\label{line-graph}}

If we want to compare values of a variable across units of time (i.e.~change over time), then a line graph is probably the most common choice, though a bar chart or dot plot can work too. The graduate degree data is cross-sectional, so there is no good way to make a line graph using these data. I trust you have seen a line graph before. We will cover how to make line graphs using panel data in Chapter \ref{r-visualization}.

\hypertarget{association}{%
\section{Association}\label{association}}

Associations involve two or more distributions. We can visualize multiple distributions using a scatter plot if both variables are continuous or discrete with many values, or we can use a histogram or box plot if we want to visualize how the distribution of a continuous variable changes for each level of a categorical variable.

\hypertarget{categorical-and-numerical}{%
\subsection{Categorical and numerical}\label{categorical-and-numerical}}

Suppose we wanted to visualize the association between attaining a graduate degree or not and median pay. Whether to attain a graduate degree is a categorical variable with two levels. Therefore, we can use a box plot to visualize the distribution of median pay for employees with undergraduate degrees in the 173 majors in our data and the distribution of median pay for employees with a graduate degree in those same majors. Figure \ref{fig:gradpaydiffbox} below does just that.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/gradpaydiffbox-1} 

}

\caption{Median pay for undergraduate and graduate degrees of the same group of majors}\label{fig:gradpaydiffbox}
\end{figure}

Overlaying histograms for each level could work too as is done in Figure \ref{fig:gradpaydiffhist}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/gradpaydiffhist-1} 

}

\caption{Median pay for undergraduate and graduate degrees of the same group of majors}\label{fig:gradpaydiffhist}
\end{figure}

\hypertarget{scatter-plot}{%
\subsection{Scatter plot}\label{scatter-plot}}

The most common visualization for associations is the scatter plot, which you saw several times in Chapter \ref{descriptive-statistics}. It is also common to overlay a simple regression line for the two variables, thus providing a reader the full scatter of the two distributions as well as a tracing of how the two variables move in tandem, \emph{on average}.

Suppose we wanted to visualize the relationship between median pay of graduate degrees and the total number of people with that graduate degree. Do more people tend to enroll in the programs that pay the most?

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/gradpayenroll-1} 

}

\caption{Graduate degree median pay and total number of people with degree}\label{fig:gradpayenroll}
\end{figure}

\begin{quote}
\textbf{The logic of visualization choice discussed in this chapter applies regardless of what particular software one uses. To learn how to generate most of these graphs in R, proceed to Chapter \ref{r-visualization}.}
\end{quote}

\hypertarget{kt5}{%
\section{Key terms and concepts}\label{kt5}}

\begin{itemize}
\tightlist
\item
  Uses of a histogram
\item
  Uses of a box plot
\item
  Uses of a bar chart
\item
  Uses of a scatter plot
\item
  Distribution of a numerical variable
\item
  Comparison of one variable between two or more units of analysis
\item
  Composition of a categorical variable
\end{itemize}

\hypertarget{part-regression-models}{%
\part{Regression Models}\label{part-regression-models}}

\hypertarget{simple-and-multiple-regression}{%
\chapter{Simple and Multiple Regression}\label{simple-and-multiple-regression}}

\begin{quote}
\emph{``You can lead a horse to water but you can't make him enter regional distribution codes in data field 97 to facilitate regression analysis on the back end.''}

---John Cleese
\end{quote}

\hypertarget{lo6}{%
\section{Learning objectives}\label{lo6}}

\begin{itemize}
\tightlist
\item
  Identify and explain the components of a population or sample regression model
\item
  Explain the difference between a deterministic equation of a line and a statistical, probabilistic equation of a line
\item
  Given regression results, provide the predicted change in the outcome given a change in the explanatory variable(s)
\item
  Given regression results, provide the predicted value of the outcome given a value of the explanatory variable(s)
\item
  Explain what the error term in a regression model represents
\item
  Interpret measures of fit in a regression model and explain their relative strengths and weaknesses
\end{itemize}

\hypertarget{basic-idea}{%
\section{Basic idea}\label{basic-idea}}

The basic idea of regression is really quite simple. Regression calculates a line through a scatter plot of two variables so that we can summarize how much our variable on the y axis changes given a change in the variable on our x variable. Or, we can use a given value for our x variable to predict a value for our y variable. That's all it is--a line drawn to represent the association between two variables.

We all learned the equation for a line back in middle school, which probably looked something like the following:

\begin{equation}
y = mx + b
\label{eq:line}
\end{equation}

where \(m\) is the slope of the line and \(b\) is the y-intercept. If we know the slope and intercept for a line, then, given a value for \(x\), we can compute \(y\). Given a change in \(x\), we can compute a change in \(y\) by multiplying the change in \(x\) by \(m\).

Consider the following equation for an arbitrary line:

\[y = 5x + 10\]

Here are some questions we can now answer:

\begin{itemize}
\tightlist
\item
  How much does \(y\) change if \(x\) increases by 1? Answer: 5
\item
  How much does \(y\) change if \(x\) increases by 10? Answer: 50
\item
  How much does \(y\) change if \(x\) decreases by 10? Answer: -50
\item
  What does \(y\) equal if \(x\) equals 2? Answer: 20
\item
  What would \(y\) equal if \(x\) were 0? Answer: 10
\end{itemize}

If you understand how to answer the above questions, then you can interpret regression results for any given context because

\begin{quote}
interpreting regression results involves either predicting the \emph{change} in \(y\) given a \emph{change} in \(x\) or predicting the \emph{value} of \(y\) given a \emph{value} of \(x\).
\end{quote}

What is different in regression is how the equation of the line is presented because there are population and sample versions of the relationship between \(x\) and \(y\). Also, regression is not a deterministic mathematical equation like the one above. Because we generally use regression to measure relationships between social phenomena, there is inherent uncertainty in the line we calculate. This adds some complexity beyond solving a line's equation, but the process of running a regression to estimate the slope and intercept of a line to then predict changes or values of an outcome is fundamentally the same as the simple equation above.

\hypertarget{simple-linear-regression}{%
\section{Simple linear regression}\label{simple-linear-regression}}

Equation \eqref{eq:simreg} presents the population regression model.

\begin{equation}
y=\beta_0+\beta_1x+\epsilon
\label{eq:simreg}
\end{equation}

Only one element differs between Equations \eqref{eq:line} and \eqref{eq:simreg}. That is the symbol at the end, which is the Greek letter epsilon and is used to denote the aforementioned uncertainty of predicting real-world, particularly social, phenomena.

The y-intercept denoted as \(b\) in Equation \eqref{eq:line} has been moved to the front of the right-hand side in Equation \eqref{eq:simreg} and is denoted by \(\beta_0\) (pronounced beta-naught). The slope denoted as \(m\) in Equation \eqref{eq:line} is now denoted as \(\beta_1\) in Equation \eqref{eq:simreg}. These beta, \(\beta\), symbols are simply the standard notation for \textbf{population parameters} in a statistical model and are used to signal that we intend to estimate these parameters using regression.

Recall that a parameter is a statistical measure of a population. In most cases, our research questions concern a population so large or inaccessible such that we do not observe all members. Instead, we take a sample of the population. From this sample, we calculate sample statistics, or \textbf{estimates} of the parameters and use methods of inference to decide if these estimates are valid guesses of the parameter (more on this in Chapters \ref{sampling} - \ref{regression-inference}).

Equation \eqref{eq:simregsample} presents the sample regression equation.

\begin{equation}
\hat{y}=b_0+b_1x
\label{eq:simregsample}
\end{equation}

The carrot symbol atop our outcome variable \(y\) is called a hat, and so the term on the left-hand side is commonly referred to as ``y-hat.'' This is used to denote the fact that any value we calculate from Equation \eqref{eq:simregsample} is an \emph{estimate} of what has been or will be observed. Similarly, the \(b\) symbols are the sample estimate analogs of the \(\beta\) population parameters in Equation \eqref{eq:simreg}.

Equation \eqref{eq:simregsample} is the equation we use to interpret our regression results in the same way as was demonstrated using the mathematical equation of a line. Again, the only difference is that we are dealing with a statistical or probabilistic equation of a line--the outcome we calculate is a prediction based on observed data.

\hypertarget{using-regression}{%
\subsection{Using regression}\label{using-regression}}

Let's pause the theory to consider a simple example using data for U.S. counties. Table \ref{tab:countydata} provides a preview of the data

\begin{table}

\caption{\label{tab:countydata}Preview of county data}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
name & state & fed\_spend & poverty & homeownership & income\\
\hline
Traverse County & Minnesota & 20.038786 & 9.3 & 80.3 & 41287\\
\hline
Wabash County & Illinois & 7.422533 & 13.0 & 80.1 & 46026\\
\hline
Pike County & Mississippi & 9.091897 & 25.3 & 72.9 & 30779\\
\hline
Greenbrier County & West Virginia & 9.029030 & 19.4 & 75.0 & 33732\\
\hline
Ray County & Missouri & 5.795480 & 9.4 & 78.7 & 53343\\
\hline
Hamilton County & Tennessee & 10.188056 & 14.7 & 65.5 & 45408\\
\hline
Ballard County & Kentucky & 11.907989 & 13.0 & 83.3 & 41228\\
\hline
\end{tabular}
\end{table}

where \texttt{fed\_spending} is the amount of federal funds allocated to the county per capita, \texttt{poverty} is the percent of the population in poverty, \texttt{homeownership} is the percent of the population that owns a home, and \texttt{income} is per capita income. There are 3,143 observations in this dataset.

Suppose we wanted to examine the association between federal spending and poverty for U.S. counties such that poverty \emph{explains} federal spending. After all, a substantial portion of federal dollars are dedicated to assist those in poverty. First, we might visualize the relationship between the two variables.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/povfedscatter-1} 

}

\caption{Federal spending and poverty among U.S. counties}\label{fig:povfedscatter}
\end{figure}

If we were to trace a line through these points, it would clearly slope upward. Thus, this suggests to us that as the percent of the population of a county increases, the amount of federal spending it receives increases. But by how much? That is precisely what regression estimates for us.

Equation \eqref{eq:simregexample} represents the relationship between federal spending and poverty using a simple linear regression population model. Note that we have chosen to model the two variables such that poverty explains or predicts federal spending. This aligns with the choice to plot poverty on the x axis and federal spending on the y axis in Figure \ref{fig:povfedscatter}. This is a critical choice in every regression and one that computers still need humans to help with (more on that later).

\begin{equation}
FedSpend = \beta_0+\beta_1Poverty + \epsilon
\label{eq:simregexample}
\end{equation}

We are going to use observed values of poverty and federal spending to estimate \(\beta_0\) and \(\beta_1\). Then, once we have those estimates, we can provide succinct answers regarding how federal spending tends to change given a change in poverty or a predicted level of federal spending given a particular level of poverty in a county.

The \(\epsilon\) represents all the other factors that explain or predict federal spending that are not in our model. If our world were such that the points in \ref{fig:povfedscatter} literally formed a straight line, we would not need an \(\epsilon\), but this is never the case with interesting questions of complex phenomena. This may or may not be a problem for whatever story we intend to tell about the the relationship between poverty and federal spending.

Running the regression as represented in Equation \eqref{eq:simregexample} produces Table \ref{tab:simpregextab} of results.

\begin{table}

\caption{\label{tab:simpregextab}Regression results of poverty on federal spending}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\hline
intercept & 7.950 & 0.219 & 36.294 & 0 & 7.520 & 8.379\\
\hline
poverty & 0.108 & 0.013 & 8.265 & 0 & 0.082 & 0.134\\
\hline
\end{tabular}
\end{table}

With the exception of Chapter \ref{causation-and-bias}, this section on regression focuses on understanding the methods used to generate the values in the \texttt{estimate} column immediately to the right of the variable names as well as how to interpret and apply these values to any question. The remaining columns in the above table pertain to inference and will be covered in the section on inference.

The values in the \texttt{estimate} column are commonly referred to as \textbf{coefficients}, which were first mentioned in Chapter \ref{descriptive-statistics}. Regression coefficients measure the direction and magnitude of association between an explanatory variable and an outcome variable.

Now that we have our results, we can plug them into our sample regression equation like so

\begin{equation}
\hat{FedSpend}=7.95+0.108 \times Poverty
\label{eq:simregresults}
\end{equation}

and we are back to the first section of this chapter. Note that we replaced the intercept, \(\beta_0\), with our estimate of the intercept, \(b_0=7.95\) and replaced the marginal effect of poverty on federal spending, \(\beta_1\), with our estimate of the marginal effect, \(b_1=0.108\).

\begin{quote}
The intercept of a regression does not always have a practical use. The intercept represents the predicted value of the outcome when the explanatory variable \(x\) equals 0.
\end{quote}

Note in Figure \ref{fig:povfedscatter} that it appears only two counties in the US have a poverty rate of 0, so it is not a very applicable scenario. Still, our regression model suggests that federal spending per capita is predicted to equal \$7.95 when the poverty is 0.

\begin{quote}
We are usually more interested in the estimates corresponding to our explanatory variable(s). The estimates for our explanatory variables represent their marginal effect on the outcome. That is, as \(x\) changes one unit, \(y\) changes by \(b\) units.
\end{quote}

There is a standard template for reporting the marginal effect or estimate of a explanatory variable in regression. It goes as follows:

\begin{quote}
On average, a one {[}unit{]} increase in \(x\) {[}is associated with{]} a \(b\) {[}unit{]} {[}increase/decrease{]} in \(y\).
\end{quote}

A couple points about the above template:

\begin{itemize}
\tightlist
\item
  We replace {[}unit{]} with the actual units of the \(x\) variable (e.g.~dollar, percentage point)
\item
  We replace \(x\) with what \(x\) is
\item
  We can replace {[}is associated with{]} with any combination of words to express the relationship (e.g.~causes, results in, tends to, etc.)
\item
  We replace \(b\) with the value of the estimate from our regression results
\item
  We replace {[}unit{]} with the actual units of the \(y\) variable, and
\item
  We replace \(y\) with what \(y\) is
\end{itemize}

Applying this template to our example, we can write the following:

\begin{quote}
On average, as the percent of the population in poverty increases by 1 percentage point, federal spending per capita tends to increase approximately 11 cents.
\end{quote}

The marginal effect of poverty on federal spending is 11 cents. A couple points about the above interpretation:

\begin{itemize}
\tightlist
\item
  We always qualify with ``on average'' because that is exactly what regression does. Drawing a line through a scatter plot results in points above and below that line. Will every instance of a one percentage point increase in poverty result in an increase of 11 cents in federal spending? No.~Sometimes it is more, and other times it is less. The line drawn by regression traces how \(y\) responds to \(x\) \emph{on average}.
\item
  The standard change in \(x\) to use when reporting results is one unit. Poverty is in units of percent. Therefore, a one-unit change in a variable measured in percentages is one percentage point (e.g.~10\% to 11\%).
\end{itemize}

\hypertarget{predicted-change}{%
\subsection{Predicted change}\label{predicted-change}}

Any hypothetical change in the explanatory variable can be used to predict the corresponding change in outcome. To do so, we only use the part of the regression equation that includes the explanatory variable that is changing:

\begin{equation}
\Delta \hat{y}=b_1x
\label{eq:predchange}
\end{equation}

where \(\Delta\) denotes change. We replace \(b_1\) with our estimate and \(x\) with the magnitude of the hypothetical change. This gives us the predicted change in \(\hat{y}\) given the particular change in \(x\).

Applying this to our example, what is the predicted change in federal spending per capita given a 10 percentage point increase in the poverty rate?

\[0.108 \times 10 = 1.08\]
\textgreater{} Federal spending per capita is predicted to increase by an average of \$1.08 given a 10 point increase in county poverty rate.

\hypertarget{predicted-value}{%
\subsection{Predicted value}\label{predicted-value}}

Any hypothetical value of the explanatory variable can be used to predict the corresponding value of outcome. To do so, we use the full regression equation.

For example, if we expected a county's poverty rate to be 30\%, then we could report a predicted level like so.

\[7.95 + 0.108 \times 30 = 11.19\]

\begin{quote}
Given a poverty rate of 30\%, federal spending per capita is predicted to be \$11.19, on average.
\end{quote}

\hypertarget{visualizing-predicted-change-and-value}{%
\subsection{Visualizing predicted change and value}\label{visualizing-predicted-change-and-value}}

Figure \ref{fig:povfedscatter2} visualizes the regression line from our running example. Note that when poverty equals 0, the regression line appears to intersect the y-axis of federal spending just below \$10, which we know is exactly \$7.95 from our regression results in Table \ref{tab:simpregextab}. Also, we know the slope of this line is 0.108.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/povfedscatter2-1} 

}

\caption{Federal spending and poverty among U.S. counties}\label{fig:povfedscatter2}
\end{figure}

The dashed red and green lines visualize how our regression line (or the equation from which it is drawn) is used to predict change in the outcome and/or the value of the outcome. It may not be clear why we do not use the intercept when predicting change. The vertical red line represents a poverty rate of 20\% and the vertical green line represents a poverty rate of 40\%. If we were to ask what the predicted change in federal spending per capita is if the poverty rate were to increase by 20 percentage points, the answer is the vertical distance between the horizontal red and green lines. In other words, if we were to move along the regression line from 20 to 40, how much distance will we cover along the y-axis? This only involves the slope of the line, not where the regression line intersects the y-axis. The vertical distance between the horizontal red and green lines is

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.108}\SpecialCharTok{*}\DecValTok{20}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.16
\end{verbatim}

or \$2.16. Federal spending per capita is predicted to increase \$2.16 given a 20 percentage point increase in poverty. Because this is a linear regression line, a 20 percentage point increase starting from any poverty rate would predict the same change because \(0.108 \times 20\) always equals 2.16.

Hopefully, it is clear why we use the entire regression equation to predict the value of the outcome given the value of the explanatory variable. In Figure \ref{fig:povfedscatter2}, the predicted value of federal spending per capita at poverty rates of 20\% and 40\% is where the horizontal red and green lines intersect the y-axis, respectively. Precisely, predicted federal spending per capita at 20\% poverty is

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{7.95} \SpecialCharTok{+} \FloatTok{0.108}\SpecialCharTok{*}\DecValTok{20}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 10.11
\end{verbatim}

and predicted federal spending per capita at 40\% poverty is

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{7.95} \SpecialCharTok{+} \FloatTok{0.108}\SpecialCharTok{*}\DecValTok{40}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 12.27
\end{verbatim}

Note that the difference between the predicted values at 20\% and 40\% is

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{12.27} \SpecialCharTok{{-}} \FloatTok{10.11}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.16
\end{verbatim}

which is a roundabout way of getting the predicted change in federal spending given a 20 point increase in poverty and demonstrates why we do not need to incorporate the intercept when predicting \emph{change}. When predicting the value of federal spending, not incorporating the intercept would be as if the regression line intersects the y-axis at 0, which is clearly not the case here. Without the intercept, our answer for predicted federal spending per capita at 20\% and 40\% poverty would be

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.108}\SpecialCharTok{*}\DecValTok{20}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.16
\end{verbatim}

and

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.108}\SpecialCharTok{*}\DecValTok{40}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.32
\end{verbatim}

each of which underestimates predicted federal spending by exactly the intercept

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{10.11{-}2.16}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.95
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{12.27} \SpecialCharTok{{-}} \FloatTok{4.32}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.95
\end{verbatim}

\hypertarget{the-error-term}{%
\subsection{The error term}\label{the-error-term}}

Back to theory. We need to address this \(\epsilon\) that is present in the population regression model but disappears in the sample regression model and results. What gives?

The \(\epsilon\) term is commonly referred to as the \textbf{error term} or, for those who don't like to insinuate some error was made in the regression, \textbf{statistical noise}. I prefer error term if for no other reason than to remind us to consider the myriad of errors we \emph{may} be making in our regression model.

As mentioned, the error term represents the inherent uncertainty of modeling an outcome based on a necessarily finite number of explanatory factors. Other factors affect our outcome. As a matter of principle, failure to account for all other factors that affect our outcome does not prohibit our attempt to estimate the effect of a variable we care about on the outcome.

Could we account for multiple factors (i.e.~multiple regression)? Absolutely. Can we control for \emph{everything} that affects our outcome? Definitely not if you subscribe to chaos theory or philosopher David Hume's thoughts on causality. Even if not so extreme as to say our world is too complex to ever make decisions concerning one variable's effect on another, the plausibility for us to collect data on every relevant factor is highly unlikely.

So, where did the error term go? It never really left; it simply is not used when calculating the predicted outcome based on our regression results. Like our \(\beta\) terms, the error term is a population parameter. However, unlike the \(\beta\)s, we do not have observed data that corresponds to the error term's estimation. In fact, the concept of the error term exists on the basis that we do not observe it. Therefore, it is necessarily excluded when we predict an outcome based on observed data, all the while we are careful to remind readers that the numbers we report are estimates subject to error and everything we report is based on an average.

If the error term never left, where is it? Its sample analog exists as the difference between our estimated regression line and the observed data. Figure \ref{fig:povfedscatterresid} below highlights each residual in our running example.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/povfedscatterresid-1} 

}

\caption{Federal spending and poverty among U.S. counties}\label{fig:povfedscatterresid}
\end{figure}

Surely, it is apparent that our regression line does not intersect all points perfectly; many points lie above or below our line. The vertical distance of any line between a plot point and the regression line in Figure \ref{fig:povfedscatterresid} is the values of a particular residual. In this example, our residuals are in units of dollars of federal spending per capita.

Table \ref{tab:simregexresid} quantifies the error/residual for select observations.

\begin{table}

\caption{\label{tab:simregexresid}Comparing observed and predicted federal spending}
\centering
\begin{tabular}[t]{r|r|r|r|r}
\hline
ID & fed\_spend & poverty & fed\_spend\_hat & residual\\
\hline
1961 & 7.592 & 16.9 & 9.773 & -2.181\\
\hline
2440 & 10.188 & 17.1 & 9.795 & 0.393\\
\hline
1738 & 15.784 & 8.8 & 8.899 & 6.885\\
\hline
2641 & 23.503 & 0.0 & 7.950 & 15.554\\
\hline
1471 & 8.152 & 25.0 & 10.647 & -2.495\\
\hline
1362 & 7.649 & 14.0 & 9.460 & -1.811\\
\hline
2792 & 7.933 & 13.5 & 9.406 & -1.474\\
\hline
\end{tabular}
\end{table}

Our regression model uses observed values of poverty and federal spending to estimate the parameters of the regression line, which produced Equation \eqref{eq:simregresults}. We can then plug the observed values of poverty into the equation to compute a predicted level of federal spending, represented by \texttt{fed\_spend\_hat}. For example, for observation 1,961 in our data, actual observed federal spending per capita was \$7.59. However, given that this county's poverty rate was 16.9, our regression model predicts federal spending per capita to be

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{7.95} \SpecialCharTok{+} \FloatTok{0.108}\SpecialCharTok{*}\FloatTok{16.9}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 9.7752
\end{verbatim}

as we can see in the \texttt{fed\_spend\_hat} column ( \(\hat{y}\) ).The right-most column of Table \ref{tab:simregexresid} shows the difference between \emph{predicted} federal spending and \emph{observed} federal spending. Again, this difference is called the \textbf{residual}. Our regression over-estimates federal spending for this county by \$2.18. Thus, the residual for this county is -2.18 because the observed outcome is 2.18 less than the estimated outcome.

The residual is represented mathematically by Equation \eqref{eq:simregresid}

\begin{equation}
e = y - \hat{y}
\label{eq:simregresid}
\end{equation}

where \(e\) is the sample analog of \(\epsilon\). This is simply the equation behind the process of differencing the observed and predicted values of our outcome just described.

\hypertarget{goodness-of-fit}{%
\subsection{Goodness of fit}\label{goodness-of-fit}}

Armed with an understanding of error and its sample analog, the residual, we can now consider goodness-of-fit. We must accept there will be error in our regression, but that does not mean we do not seek to minimize that error as much as possible.

\hypertarget{assessing-fit}{%
\subsubsection*{Assessing fit}\label{assessing-fit}}
\addcontentsline{toc}{subsubsection}{Assessing fit}

Table \ref{tab:simregexfit} provides a standard set of three goodness-of-fit measures often used to assess regression.

\begin{table}

\caption{\label{tab:simregexfit}Goodness-of-fit measures}
\centering
\begin{tabular}[t]{r|r|r}
\hline
r\_squared & adj\_r\_squared & rmse\\
\hline
0.021 & 0.021 & 4.654482\\
\hline
\end{tabular}
\end{table}

The first column titled \texttt{r\_squared} refers to the measure \(R^2\), also known as the \textbf{coefficient of determination} defined in \ref{descriptive-statistics}. The \(R^2\) measures the strength of association between a set of one or more explanatory variables and an outcome variable. Specifically, it quantifies the percent of total variation in the outcome explained by our regression model. In this case, our regression using poverty explains 2.1\% of the total variation in federal spending.

The column titled \texttt{rmse} refers to \textbf{root mean squared error} (RMSE). The RMSE quantifies the typical deviation of the observed data points from the regression line and is particularly useful when predicting a value for our outcome. For example, if after predicting that a county with 30\% poverty will receive 11.25 dollars of federal spending per capita, someone asks us how far off that prediction is likely to be, the RMSE suggests our prediction will tend to be off by plus or minus 4.65 dollars.

Regression involves choices. We choose which variables to use to explain or predict an outcome and how to model their effect on the outcome. This menu of choices will become increasingly evident as we build our regression toolbox. As we make choices, competing regression models emerge from which we must choose the one we prefer to report for decision-making.

The \(R^2\) and RMSE provide us the basis for choosing our preferred model. In general, \textbf{we prefer the model with a \emph{higher} \(R^2\) and/or a lower \emph{RMSE}}. In virtually all cases, these two measures will agree with each other; the model with the higher \(R^2\) will also have the lower RMSE.

\begin{quote}
For a more thorough treatment of fit that is unessential but potentially helpful, check out \ref{goodness-of-fit}.
\end{quote}

\hypertarget{multiple-regression}{%
\section{Multiple regression}\label{multiple-regression}}

Of course, we are not limited to using only one variable to explain or predict an outcome. In fact, it is rather uncommon to use only one variable, but simple linear regression is useful for introducing the method of regression. Now, we can consider more realistic modeling method where we use multiple explanatory variables in our regression, which is aptly named multiple regression.

Equation \eqref{eq:multreg} provides the population model for multiple regression

\begin{equation}
y=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_kx_k+\epsilon
\label{eq:multreg}
\end{equation}

The only difference in this equation compared to Equation \eqref{eq:simreg} is the inclusion of multiple explanatory variables. Each explanatory is numbered and has a corresponding parameter \(\beta\) representing the marginal effect it has on the outcome. In theory, we can add however many explanatory variables we deem worth including, represented by the arbitrary \(k\).

Equation \eqref{eq:multregsample} presents the sample equation for multiple regression.

\begin{equation}
\hat{y}=b_0+b_1x_1+b_2x_2+\cdots +b_kx_k
\label{eq:multregsample}
\end{equation}

Again, nothing is different from before except for more explanatory variables and sample estimates of the parameters.

\hypertarget{using-multiple-regression}{%
\subsection{Using multiple regression}\label{using-multiple-regression}}

Let's return to our example of federal spending per capita in U.S. counties. Previously, we used only the percent of the population in poverty to explain or predict federal spending per capita. Let's add the percent of the population that owns a home and per capita income to our model. Thus, our model can be written as such

\begin{equation}
FedSpend = \beta_0 + \beta_1Poverty + \beta_2HomeOwn + \beta_3Income + \epsilon
\label{eq:multregex}
\end{equation}

which generates the following results

\begin{table}

\caption{\label{tab:multregextab}Multiple regression results}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\hline
intercept & 23.519 & 1.333 & 17.645 & 0.000 & 20.905 & 26.132\\
\hline
poverty & -0.056 & 0.021 & -2.674 & 0.008 & -0.097 & -0.015\\
\hline
homeownership & -0.126 & 0.012 & -10.736 & 0.000 & -0.149 & -0.103\\
\hline
income & 0.000 & 0.000 & -7.723 & 0.000 & 0.000 & 0.000\\
\hline
\end{tabular}
\end{table}

and the following goodness-of-fit measures

\begin{table}

\caption{\label{tab:multregexfit}Fit of multiple regression}
\centering
\begin{tabular}[t]{r|r|r}
\hline
r\_squared & adj\_r\_squared & rmse\\
\hline
0.064 & 0.063 & 4.55216\\
\hline
\end{tabular}
\end{table}

Now we can discuss what is different with multiple regression. First, note that our coefficient or estimate for poverty has changed from 0.108 to 0.105. A small difference to be sure, but that is specific to the example used; sometimes the estimate can change dramatically. Why the change? Because we are \textbf{controlling for other factors}. A slight amount of the marginal effect we reported poverty had on federal spending in our simple regression model was misattributed from the marginal effects of homeownership and/or income on federal spending.

This is a key feature of multiple regression: it estimates the marginal effect of a variable on an outcome, holding all other explanatory variables equal to their respective means. In other words, if we were omnipotent beings who could take each county in our data and set homeownership and income to the mean of homeownership and income according to the observed data, then pull some lever that makes poverty change and nothing else, the estimate for poverty in our multiple regression reports how much each percentage point in poverty changes federal spending. This is how we isolate the effect of one variable on an outcome despite knowing other variables simultaneously affect our outcome.

The interpretation of multiple regression estimates is essentially the same as simple regression. In our example, we can interpret the homeownership estimate like so:

\begin{quote}
On average, our results indicate that a one percentage point increase in the percent of the population that owns a home is associated with a decrease in federal spending per capita of approximately 9 cents, \textbf{holding other factors constant}.
\end{quote}

The part in bold is to point out the small difference between the two interpretations. Here, we are simply reminding a reader that we have controlled for other factors that presumably we have already explained, and our estimate for poverty accounts for those factors by holding them constant. Other common word choices for this part of the interpretation include ``all else equal'' or its Latin translation ``ceteris paribus.''

Again, we can answer any sort of question relevant to our original research question concerning the predicted change or level of federal spending by plugging in the numbers to our regression equation.

\begin{equation}
\hat{FedSpend} = 13.50 + 0.105\times Poverty - 0.093\times HomeOwn + 0Income
\end{equation}

If we wanted to predict the change in federal spending given an 3 percentage point increase in poverty and a decline in home ownership of 4 percentage points, the our answer would be

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.105}\SpecialCharTok{*}\DecValTok{3}\SpecialCharTok{+}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.093}\NormalTok{)}\SpecialCharTok{*}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.687
\end{verbatim}

dollars per capita (on average and all else equal, of course). If we wanted to predict the level of federal spending per capita for a county with 12\% poverty, a 80\% home ownership rate, and \$31,000 income per capita, then we would predict

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{13.50+0.105}\SpecialCharTok{*}\DecValTok{12}\FloatTok{{-}0.093}\SpecialCharTok{*}\DecValTok{80}\SpecialCharTok{+}\DecValTok{0}\SpecialCharTok{*}\DecValTok{31000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.32
\end{verbatim}

dollars per capita.

Not so fast! This example provides a good opportunity to consider another aspect of the units our variables are in. Per capita income is in dollars. This means the estimate for income represents the effect of a \emph{one dollar} change in per capita income on federal spending per capita. That's a very small change that we would expect to have a very small effect on federal spending. This effect is so small that statistical software may round to 0. But what if we changed the units of income to \emph{thousands} of dollars per capita instead of dollars per capita? Then we get the following results.

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

23.519

1.333

17.645

0.000

20.905

26.132

poverty

-0.056

0.021

-2.674

0.008

-0.097

-0.015

homeownership

-0.126

0.012

-10.736

0.000

-0.149

-0.103

income

-0.086

0.011

-7.723

0.000

-0.108

-0.064

Now we see the effect of a \emph{one thousand} dollar change in per capita income on federal spending per capita. Note that the estimates for poverty and homeownership are the same. Therefore, the predicted level of federal spending for our county is actually

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{13.50+0.105}\SpecialCharTok{*}\DecValTok{12}\FloatTok{{-}0.093}\SpecialCharTok{*}\DecValTok{80}\FloatTok{+0.057}\SpecialCharTok{*}\DecValTok{31}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 9.087
\end{verbatim}

\hypertarget{fit-and-adjusted-r-squared}{%
\subsection{Fit and adjusted R squared}\label{fit-and-adjusted-r-squared}}

In addition to doing a better job isolating the marginal effect of one variable on an outcome, including additional explanatory variables can reduce the error in our regression, thus achieve more accurate and/or precise predictions of the outcome.

We can assess this improvement in fit by comparing the results in Table \ref{tab:multregexfit} to those in Table \ref{tab:simregexfit}. We have gone from an RMSE of 4.65 dollars to an RMSE 4.59 dollars. This means our predictions from the multiple regression model tend to be off by 6 cents fewer than the predictions of our simple regression model.

The previous discussion on fit conspicuously skipped over the column titled \texttt{adj\_r\_square} because \textbf{adjusted-\(R^2\)} applies when comparing two or more models with a different number of explanatory variables. One caveat to using \(R^2\) to choose a preferred model is that it mechanically increases as the number of explanatory variables increases whether those additional variables improve the extent to which our regression explains the total variation in the outcome or not. Therefore, it is unfair to compare a model with one explanatory variable to a model with more than one explanatory variable.

The adjusted-\(R^2\) accounts for this unfairness by applying a penalty to each additional explanatory variable. We can fairly compare models with different numbers of explanatory variables using their respective adjusted-\(R^2\). In our example, we have gone from explaining 2.1\% of the total variation in federal spending to explaining 4.7\% of its total variation. Adding home ownership and income has more than doubled the explanatory power of our model of federal spending.

\hypertarget{explanatory-penalty}{%
\subsection{Explanatory penalty}\label{explanatory-penalty}}

Each explanatory variable we add to our regression model imposes a type of penalty on our results. Basically, for each explanatory variable included, we lose an observation in our data (not literally). This will be discussed further in the section on inference, but we need at least 33 observations to make valid inferences about a population based on sample estimates. If we had, say 50 observations in a dataset, and wanted to run a regression with 25 explanatory variables, then it is as though our regression model is based on only 25 observations (50 observations - 25 variables = 25 degrees of freedom). We will obtain results from such a model, but we should not use those results to make inferences.

In case you were wondering why not simply add all the variables we can to a model rather than carefully consider which variables to include and exclude in a model, this penalty is one of the primary reasons. Fewer degrees of freedom jeopardizes our ability to make valid inference. It can also reduce the precision of our predictions. The goal is to maximize the explanatory or predictive power of our regression model at minimal cost (i.e.~excluding superfluous variables). Choosing good regression models is where subject matter expertise plays a crucial role. Experience and knowledge within the context of the research question informs our choices. Statistics is the method by which we apply our expertise to data to make evidence-based decisions.

\begin{quote}
\textbf{To learn how to run regression in R, proceed to Chapter \ref{r-regression}.}
\end{quote}

\hypertarget{kt6}{%
\section{Key terms and concepts}\label{kt6}}

\begin{itemize}
\tightlist
\item
  Line concepts

  \begin{itemize}
  \tightlist
  \item
    y-intercept
  \item
    slope
  \item
    change in y versus value of y
  \end{itemize}
\item
  Regression model components

  \begin{itemize}
  \tightlist
  \item
    outcome/dependent/response variable
  \item
    independent/explanatory variable
  \item
    error term/statistical noise
  \item
    residual
  \item
    population parameter
  \item
    sample coefficients/estimates
  \end{itemize}
\item
  Goodness of fit

  \begin{itemize}
  \tightlist
  \item
    R-squared
  \item
    Adjusted R-squared
  \item
    root mean squared error (RMSE)
  \end{itemize}
\item
  Controlling for other factors in multiple regression
\end{itemize}

\hypertarget{categorical-variables-and-interactions}{%
\chapter{Categorical Variables and Interactions}\label{categorical-variables-and-interactions}}

\begin{quote}
\emph{``For how can one know color in perpetual green, and what good is warmth without the cold to give it sweetness?''}

---John Steinbeck, Travels with Charley
\end{quote}

Chapter \ref{simple-and-multiple-regression} introduced regression models that contain only continuous variables. In this chapter, we build our regression toolbox to include models that contain categorical variables. We will cover three models in particular:

\begin{quote}
\begin{itemize}
\tightlist
\item
  Parallel slopes model: including a categorical explanatory variable
\item
  Interaction model: allowing the slope of the regression line for each level of a categorical variable to differ (i.e.~not parallel)
\item
  Linear probability model: including a two-level (binary) categorical outcome
\end{itemize}
\end{quote}

\hypertarget{lo7}{%
\section{Learning objectives}\label{lo7}}

\begin{itemize}
\tightlist
\item
  Explain why and how to extend simple or multiple regression models to a parallel slopes model
\item
  Interpret results of a parallel slopes model
\item
  Explain why and how to extend regression models to an interaction model
\item
  Interpret results of an interaction model
\item
  Provide advice on choosing between parallel slopes and interaction model
\item
  Explain why and how to extend regression models to a linear probability model
\item
  Interpret results of a linear probability model
\end{itemize}

\hypertarget{parallel-slopes-model}{%
\section{Parallel slopes model}\label{parallel-slopes-model}}

To introduce the inclusion of categorical variables in regression, the simplest type of categorical variable will be used. The simplest categorical variable is commonly referred to as a \textbf{dummy variable}.

\begin{quote}
A dummy variable is a two-level or binary categorical variable. It takes on values of either 1 or 0, where 1 corresponds to ``yes'' or ``true'' and 0 corresponds to ``no'' or ``false''.
\end{quote}

For example, a common way to represent biological sex in data is to use a dummy variable where either male or female is coded as 1 and the other sex is coded as 0 (it remains uncommon to find data that codes gender as non-binary either). The convention is to name such a variable whatever level is coded as 1. For example, a dummy variable coded as 1 for male and 0 for female will often be named ``male'' in a dataset. A variable coded as 1 to denote a person attained a college degree and 0 to denote they did not might be named something like ``college.''

The parallel slopes model using a dummy variable is represented in Equation \eqref{eq:pslope}.

\begin{equation}
y = \beta_0 + \beta_1x + \beta_2d + \epsilon
\label{eq:pslope}
\end{equation}

where \(d\) is simply used to distinguish the variable as a dummy variable whereas \(x\) represents a numerical variable as introduced in Chapter \ref{simple-and-multiple-regression}. The sample regression equation for the parallel slopes model is represented by Equation \eqref{eq:pslopesamp}.

\begin{equation}
\hat{y} = b_0 + b_1x + b_2d
\label{eq:pslopesamp}
\end{equation}

Knowing that \(d\) can equal only 1 or 0, we can plug these values into Equation \eqref{eq:pslopesamp} to understand the logic of the parallel slopes model before considering an example. If \(d=0\), then \(b_2\) drops out of the equation because anything multiplied by 0 equals 0. In that case, our regression equation is,

\begin{equation}
\hat{y} = b_0 + b_1x
\label{eq:pslopesamp0}
\end{equation}

and we can plug in our results to predict changes in or values of \(y\) given changes or values in \(x\) just like in Chapter \ref{simple-and-multiple-regression}. Whatever \(d=0\) represents--females, non-college educated, southern states, etc.--Equation \eqref{eq:pslopesamp0} represents \emph{that group's} regression line.

If \(d=1\), then \(b_2\) stays in the model. Anything multiplied by 1 is equal to itself. That is, \(b_2 \times 1\) simplifies to \(b_2\). Since \(d\) can only equal 1 if not equal to 0, we can drop \(d\) from the equation.

\begin{equation}
\hat{y} = b_0 + b_1x + b_2
\label{eq:pslopesamp1}
\end{equation}

Whatever \(d=1\) represents--males, college educated, northern states, etc.--Equation \eqref{eq:pslopesamp1} represents \emph{that group's} regression line. Note that \(b_2\) is not multiplied by the value of another variable like \(b_1\) is multiplied by some change or value of \(x\). Instead, it is a constant number just like the y-intercept, \(b_0\). In fact, combining \(b_0\) and \(b_2\) gives us a new y-intercept for the group represented by \(d=1\) as shown in Equation \eqref{eq:pslopesamp1alt}.

\begin{equation}
\hat{y} = (b_0 + b_2) + b_1x
\label{eq:pslopesamp1alt}
\end{equation}

The logic of the parallel slopes model is simple. Including a dummy variable \(d\) draws two separate regression lines--one line through the observations for which \(d=0\) and another line through the observations for which \(d=1\).

\begin{quote}
Regression lines for both groups have the same slope because both equations include the same \(b_1x\). The regression line for the \(d=1\) group will be above or below the first regression line by a constant amount equal to \(b_2\), resulting in two regression lines running parallel to each other.
\end{quote}

\hypertarget{using-parallel-slopes}{%
\subsection{Using parallel slopes}\label{using-parallel-slopes}}

Suppose we were interested in whether state laws mandating a jail sentence for drunk driving affects traffic fatalities, presumably by deterring drunk driving. To investigate, we collect the following data, some of which is previewed in Table \ref{tab:trdeath}.

\begin{table}

\caption{\label{tab:trdeath}Sample of state traffic data}
\centering
\begin{tabular}[t]{r|r|r|l|r|r|r|l}
\hline
state & year & mrall & jaild & vmiles & mlda & unrate & region\\
\hline
41 & 1987 & 2.27606 & yes & 8.565328 & 21 & 6.2 & West\\
\hline
22 & 1982 & 2.48916 & yes & 6.137799 & 18 & 10.3 & South\\
\hline
4 & 1985 & 2.80201 & yes & 6.771263 & 21 & 6.5 & West\\
\hline
23 & 1982 & 1.46127 & yes & 6.733286 & 20 & 8.6 & N. East\\
\hline
27 & 1982 & 1.38156 & no & 7.059264 & 19 & 7.8 & Midwest\\
\hline
8 & 1984 & 1.90596 & no & 7.707853 & 21 & 5.6 & West\\
\hline
23 & 1984 & 2.00692 & yes & 8.083908 & 20 & 6.1 & N. East\\
\hline
\end{tabular}
\end{table}

where \texttt{mrall} is number of traffic deaths per 10,000 population, \texttt{jaild} is the dummy variable for whether the state has a mandatory jail sentence for drunk driving, \texttt{vmiles} is the average miles driven per driver in a state, \texttt{mlda} is the minimum legal drinking age at the time, and \texttt{unrate} is the state's unemployment rate. There are 336 observations in this dataset (48 states from 1982 to 1988, making it panel data but here it is used like a pooled cross-sectional).

Suppose we choose to use the following model

\begin{equation}
mrall = \beta_0 + \beta_1vmiles + \beta_2jaild + \epsilon
\label{eq:pslopeexamp}
\end{equation}

Note that Equation \eqref{eq:pslopeexamp} has the exact same structure as Equation \eqref{eq:pslope}. In Equation \eqref{eq:pslopeexamp}, \texttt{vmiles} is the \(x\) variable and \texttt{jaild} is the \(d\) variable. States for which \texttt{jaild\ =\ no} represent the group where \(d=0\) and states for which \texttt{jaild\ =\ yes} represent the group where \(d=1\).

Let us visualize the relationship between these variables using a scatter plot with \texttt{vmiles} on the x axis and using color to differentiate states with and without mandatory jail sentences for drunk driving. Note in Figure \ref{fig:pslopescatter} below there appears to be a positive relationship between the average miles people drive and the rate of traffic fatalities. This makes intuitive sense.

Now, imagine drawing a straight line through the red points that denote states with no mandatory jail for drunk driving and a separate line through the blue dots denoting states with mandatory jail sentencing. Do not force your imaginary lines to be parallel just yet. How do your two lines compare?

Based on the plot points, our blue line should be above our red line, as the group of blue dots appear to be systematically higher than the group of red dots. Whether the slopes between red and blue lines are similar is less obvious. The red points suggest it \emph{may} be the case that our red line should have a steeper slope than our \emph{blue} line, but it is difficult to tell.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/pslopescatter-1} 

}

\caption{Relationship between miles driven and traffic fatalities}\label{fig:pslopescatter}
\end{figure}

\begin{quote}
The exercise we just thought through is critical. Considering whether the slopes of our regression line should or do differ between categorical groups determines whether we should use the parallel slopes model or the interaction model. Which model is best to use is up to us to determine.
\end{quote}

Why not simply add the interaction and if they are the same slope, so be it? Again, because we pay a penalty for adding superfluous explanatory variables. Also, an interaction model is more difficult to interpret and communicate to an audience. Most importantly, we should choose the model that reflects our theory based on our subject matter expertise.

\begin{quote}
Remember, choosing to use a parallel slopes model forces the slopes between groups to be drawn (i.e.~estimated) as if they are parallel whether they actually are or not.
\end{quote}

Let us now run the parallel slopes model, which generates the following results

\begin{table}

\caption{\label{tab:psloperesults}Parallel slopes results}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\hline
intercept & -0.238 & 0.182 & -1.304 & 0.193 & -0.597 & 0.121\\
\hline
vmiles & 0.281 & 0.023 & 12.107 & 0.000 & 0.236 & 0.327\\
\hline
jaildyes & 0.265 & 0.056 & 4.726 & 0.000 & 0.155 & 0.376\\
\hline
\end{tabular}
\end{table}

\begin{quote}
Notice the variable name for the bottom row is \texttt{jaildyes} and the estimate equals 0.265. This is the estimate for when \texttt{jaild\ =\ yes}. There is no estimate for \texttt{jaild\ =\ no} because when \texttt{jaild\ =\ no}, that is the same as \(d=0\) and so the estimate would simply be 0. The \texttt{jaildyes} estimate is how states with a mandatory jail sentence compare to states without one.
\end{quote}

Now we can plug our results into the sample regression equation to answer whatever questions we may have or encounter.

\begin{equation}
\hat{mrall} = -0.238 + 0.281\times vmiles + 0.265 \times jaild
\end{equation}

Compare this equation to Equations \eqref{eq:pslopesamp}-\eqref{eq:pslopesamp1alt}. The \texttt{jaild} variable is the \(d\) variable. It equals either 0 or 1. If a state does not have a mandatory jail sentence, then \texttt{jaild\ =\ 0}, and so we we would have as our regression equation

\[\hat{mrall} = -0.238 + (0.281\times vmiles) + (jaild \times 0)\]

\[\hat{mrall} = -0.238 + (0.281\times vmiles)\]

because, again, 0 multiplied by anything equals 0.

For states with a mandatory jail sentence, \(d=1\), and so we have as our regression equation

\[\hat{mrall} = -0.238 + (0.281 \times vmiles) + (0.265 \times 1)\]

Anything multiplied by 1 is equal to itself, so the above equation simplifies to

\[\hat{mrall} = -0.238 + (0.281 \times vmiles) + 0.265\]

And 0.265 is a constant number just like -0.238. These values can be combined.

\[\hat{mrall} = (-0.238 + 0.265) + (0.281\times vmiles)\]

And we can finally simplify the equation to look like the standard regression equation of \(\hat{y} = b_0 + b_1x\), but remember this is the equation for states with a mandatory jail sentence.

\[\hat{mrall} = 0.027 + (0.281\times vmiles)\]

And as a reminder, the equation for states without a mandatory jail sentence is

\[\hat{mrall} = -0.238 + (0.281\times vmiles)\]

\begin{quote}
The two regression lines have the same slope because we forced them to have the same slope. The only difference between the two regression lines is the y-intercept. Because \(b_2\), which in this example was \texttt{jaildyes}, equals 0.265 as shown in Table \ref{tab:psloperesults}, the intercept for states with a mandatory jail sentence is 0.265 higher than the intercept for states.
\end{quote}

Figure \ref{fig:pslopescatter2} provides the same scatterplot but adds our parallel regression lines. Note that, as expected, the blue line is above the red line. Based on the results, we know the blue line is above the red line by 0.265. We also know that the slope for both lines is 0.281.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/pslopescatter2-1} 

}

\caption{Parallel slopes visualization}\label{fig:pslopescatter2}
\end{figure}

When communicating an interpretation of the results for a parallel slopes model, we can write something like the following:

\begin{quote}
Controlling for whether a state has a mandatory jail sentence for drunk driving, the results indicate that as the average miles driven per driver in a state increases 1 mile, traffic fatalities per 10,000 increase 0.281, on average.
\end{quote}

\begin{quote}
On average, states with mandatory jail sentencing have a higher traffic fatality rate of 0.265 per 10,000, controlling for average miles driven per driver.
\end{quote}

\hypertarget{beyond-dummies}{%
\subsection{Beyond dummies}\label{beyond-dummies}}

What if we want to include a categorical explanatory variable that has more than two levels? Doing so is easy and the logic works exactly the same as before. The only difference is that we use multiple dummy variables to represent the multiple levels of a categorical variable.

Suppose we included an explanatory variable with four levels instead of two. Such a variable can be represented in the regression equation like so:

\begin{equation}
y = \beta_0 + \beta_1x + \beta_2d_1 + \beta_3d_2 + \beta_4d_3 + \epsilon
\label{eq:pslopemultd}
\end{equation}

Just as we used one dummy variable to represent a categorical variable with two levels, we use three dummy variables to represent a categorical variable with four levels. There is always one fewer dummy variables than there are levels of a categorical variable because one level must be used as the reference/base level to which all other levels are compared.

This model represented by Equation \eqref{eq:pslopemultd} draws four regression lines. One line for the group represented when \(d_1=0\), \(d_2=0\), and \(d_3=0\); a second line for the group represented when \(d_1=1\); a third line for the group represented when \(d_2=1\); and a fourth line for the group when \(d_3=1\). All the math demonstrated in the two-level case with Equations \eqref{eq:pslopesamp}-\eqref{eq:pslopesamp1alt} works exactly the same way.

Let us apply this to our example. Suppose we were interested in whether traffic fatalities differ across U.S. regions. From the variables in Table \ref{tab:trdeath}, we might choose to construct the following model.

\begin{equation}
mrall = \beta_0 + \beta_1vmiles + \beta_2region + \epsilon
\label{eq:pslopeexamp2}
\end{equation}

where region is a four-level categorical variable containing South, West, N.East, and Midwest. Note that Equation \eqref{eq:pslopeexamp2} looks different than Equation \label{eq:pslopemultd}. This is because Equation \eqref{eq:pslopeexamp2} leaves the various levels implied within the \texttt{region} variable. We can explicitly state the levels of \texttt{region} in our regression model instead, giving us

\begin{equation}
mrall = \beta_0 + \beta_1vmiles + \beta_2d_{neast} + \beta_3d_{south} + \beta_4d_{west} + \epsilon
\label{eq:pslopeexamp3}
\end{equation}

which now has the exact same structure as Equation \label{eq:pslopemultd}. In this case, the Midwest level is excluded as the reference/base level. This is an arbitrary choice; we could choose to exclude any one of the levels to which all other levels are compared. Statistical software will automatically choose a default level. If the categorical variable is coded using text, then R excludes the level that comes first alphabetically. If coded numerically, one level should be coded as equal to 0 and will be the level that R excludes.

Figure \ref{fig:pslopescatter2} visualizes this model. Note that there are four regression lines, each corresponding to one of the four regions. There are clear differences between the regions. It appears states in the West and South are somehow different than states in the Midwest and Northeast with respect to traffic fatality rates.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/pslopescatter3-1} 

}

\caption{Parallel slopes for 4 groups}\label{fig:pslopescatter3}
\end{figure}

Running this model produces the following results.

\begin{table}

\caption{\label{tab:psloperesults2}Parallel slopes for regions}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\hline
intercept & 0.260 & 0.167 & 1.553 & 0.121 & -0.069 & 0.589\\
\hline
vmiles & 0.188 & 0.021 & 8.895 & 0.000 & 0.147 & 0.230\\
\hline
regionN. East & -0.076 & 0.065 & -1.166 & 0.245 & -0.204 & 0.052\\
\hline
regionSouth & 0.519 & 0.056 & 9.283 & 0.000 & 0.409 & 0.630\\
\hline
regionWest & 0.641 & 0.062 & 10.264 & 0.000 & 0.518 & 0.763\\
\hline
\end{tabular}
\end{table}

Note that Table \ref{tab:psloperesults2} provides estimates for three of the four regions. This is no different from our previous model; \texttt{jaild} has two levels, yes and no, but Table \ref{tab:psloperesults} provides only one estimate for \texttt{jaild=yes}. No matter how many levels in a categorical variable, one of the levels is set such that \(d=0\) and so that level drops out of the equation. Just like with the previous model where the estimate for \texttt{jaild=yes}
indicates how far above or below the regression line is relative to the line for \texttt{jaild=no}, the estimates for whatever levels remain in the equation indicate how far above or below the regression lines are relative to the excluded level.

In Table \ref{tab:psloperesults2}, Midwest is excluded as expected. We can plug our estimates into the sample version of Equation \eqref{eq:pslopeexamp3}.

\[\hat{mrall} = 0.26 + (0.188 \times vmiles) + (-0.076 \times d_{neast}) + (0.519 \times d_{south}) + (0.641 \times d_{west})\]

For states in the Midwest, all three rightmost terms drop from the model because all of the \(d\) variables equal 0, leaving us with

\[\hat{mrall} = 0.26 + (0.188 \times vmiles)\]

For states in the N.East, \(d_{neast}=1\) and the other \(d\) variables equal 0, giving us

\[\hat{mrall} = 0.26 + (0.188 \times vmiles) + (-0.076 \times 1)\]

\[\hat{mrall} = (0.26 - 0.076) + (0.188 \times vmiles)\]

\[\hat{mrall} = 0.184 + (0.188 \times vmiles)\]

This means that states in the N.East have a lower traffic fatality rate than states in the Midwest, on average. How much lower? By the amount of the estimate associated with \$d\_\{neast\}: 0.076. This same process can be applied to the other two regions.

Look back at Figure \ref{fig:pslopescatter2}, noting where the Midwest line is relative to the other regions. Northeast is below Midwest, while South and West are above it.

\begin{quote}
The estimates for the three included regions in Table \ref{tab:psloperesults2} tell us how far the regions are above and below Midwest. Again, all regions have the same slope with respect to average miles driven because we forced them to be the same by using the parallel slopes model.
\end{quote}

When communicating our results, we could write something like the following:

\begin{quote}
On average and controlling for average miles driven per driver, states in the Northeast region experience fewer traffic fatalities then state in the Midwest by approximately 0.08 per 10,000, while states in the South and West experience higher traffic fatality rates than states in the Midwest by 0.52 and 0.64, respectively.
\end{quote}

\hypertarget{interaction-model}{%
\section{Interaction model}\label{interaction-model}}

What if we allowed the two slopes in Figure \ref{fig:pslopescatter2} to differ? If our expertise leads us to theorize the two slopes should differ between states with and without mandatory jail sentencing for drunk driving, and/or if visualizing the data suggests they do, then we can choose to use an interaction model.

\begin{quote}
Allowing the two slopes to differ means we allow the marginal effect of the average miles driven per driver to differ between the two groups of states.
\end{quote}

Figure \ref{fig:interactscatter} visualizes this additional flexibility. Note that the slope of the red line is indeed slightly steeper than the blue line. Thus, this visualization suggests that average miles driven per mile in states without mandatory jail sentences for drunk driving increases the traffic fatality rate by slightly more than it does in states with a mandatory jail sentence.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/interactscatter-1} 

}

\caption{Interaction model visualization}\label{fig:interactscatter}
\end{figure}

This version of an interaction model involves interacting (i.e.~multiplying) a categorical variable with a numerical variable. Equation \eqref{eq:interaction} represents this version of the interaction model.

\begin{equation}
y = \beta_0 + \beta_1x + \beta_2d + \beta_3xd + \epsilon
\label{eq:interaction}
\end{equation}

Equation \eqref{eq:interaction} is similar to Equation \eqref{eq:pslope} for the parallel slopes model. The difference is \(\beta_3xd\). This is the interaction--multiplying two of the explanatory variables in our regression model. Equation \eqref{eq:interactionsamp} provides the sample equation of this interaction model.

\begin{equation}
\hat{y} = b_0 + b_1x + b_2d + b_3xd
\label{eq:interactionsamp}
\end{equation}

Following the same process as with the parallel slopes model, we can rearrange Equation \eqref{eq:interactionsamp} to examine the logic of this interaction model. If \(d=0\), then \(b_2\) and \(b_3x\) drop out of the model because they are multiplied by 0. Thus, we have the same sample regression equation as Equation \eqref{eq:pslopesamp0}.

\begin{equation}
\hat{y} = b_0 + b_1x
\label{eq:interactionsamp0}
\end{equation}

Equation \eqref{eq:interactionsamp0} is the regression line for whatever group is represented when \(d=0\).

If \(d=1\), then \(b_2\) and \(b_3x\) remain in our model. As with the parallel slopes model, \(b_2\) combines with \(b_0\). This shifts the y-intercept for the group for which \(d=1\) either above or below the group for which \(d=0\) by the amount equal to \(b_2\). Because the lines are not parallel, just because a line starts above or below another does not mean it will stay above or below. It depends on the value of the \(b_3x\). The term \(b_3x\) is combined with \(b_1x\). Thus, if \(d=1\), we have the following sample regression equation

\begin{equation}
\hat{y} = b_0 + b_1x + b_2d + b_3xd\\
\hat{y} = b_0 + b_1x + b_2 + b_3x\\
\hat{y} = (b_0 + b_2) + (b_1 + b_3)x
\label{eq:interactionsamp1}
\end{equation}

Equation \eqref{eq:interactionsamp1} is the regression line for whatever group is represented when \(d=1\). This regression line will have an intercept above or below the regression line for which \(d=0\) by the amount \(b_2\), similar to the parallel slopes model. Critically, this regression line will also have a slope greater or lesser than the regression line for which \(d=0\) by the amount \(b_3\).

\hypertarget{using-an-interaction}{%
\subsection{Using an interaction}\label{using-an-interaction}}

Running this interaction model in our example produces the following results

\begin{table}

\caption{\label{tab:interactionmod}Interaction model results}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\hline
intercept & -0.384 & 0.221 & -1.741 & 0.083 & -0.819 & 0.050\\
\hline
vmiles & 0.300 & 0.028 & 10.634 & 0.000 & 0.245 & 0.356\\
\hline
jaildyes & 0.731 & 0.399 & 1.831 & 0.068 & -0.054 & 1.516\\
\hline
vmiles:jaildyes & -0.058 & 0.050 & -1.178 & 0.240 & -0.156 & 0.039\\
\hline
\end{tabular}
\end{table}

Once again, we can plug these values into Equation \eqref{eq:interactionsamp} to obtain the following regression equation

\[\hat{mrall} = -0.384 + (0.3\times vmiles) + (0.731\times jaild) + (-0.058\times vmiles \times jaild)\]

For states without mandatory jail sentencing (jaild = 0), the equation simplifies to

\[\hat{mrall} = -0.384 + 0.3\times vmiles\]

When communicating an interpretation of this equation, we might write something like:

\begin{quote}
For states that do not have a mandatory jail sentence for drunk driving, as the average miles driven per mile increases by 1 mile, traffic fatalities per 10,000 increases by 0.3, on average.
\end{quote}

For states with mandatory jail sentencing (jaild = 1), the equation is

\[\hat{mrall} = -0.384 + (0.3\times vmiles) + (0.731\times jaild) + (-0.058\times vmiles \times jaild)\]

which simplifies to

\[\hat{mrall} = -0.384 + (0.3\times vmiles) + 0.731 + (-0.058\times vmiles)\]

which simplifies further to

\[\hat{mrall} = (-0.384 + 0.731) + (0.3-0.058)\times vmiles\]

and finally to

\[\hat{mrall} = 0.347 + 0.242\times vmiles\]

When communicating an interpretation of this equation, we might write something like:

\begin{quote}
For states that have a mandatory jail sentence for drunk driving, as the average miles driven per mile increases by 1 mile, traffic fatalities per 10,000 increases by 0.242, on average.
\end{quote}

Look back at Figure \ref{fig:interactscatter}. As we suspected the slope of the blue line that represents states with a mandatory jail sentence is less than the slope of the red line representing states without a mandatory jail sentence. How much less? By the amount of the estimate associated with the interaction term, \texttt{vmiles:jaildyes} in Table \ref{tab:interactionmod}: 0.058.

The intercepts of the two lines are also different. If the x-axis in Figure \ref{fig:interactscatter} were extended to 0 and the regression lines were extrapolated to the left until they intersect the y-axis, the blue line would intersect at 0.347, while the red line would intersect at -0.384. Note that the difference between these two intercepts is the estimate associated with the dummy variable \texttt{jaildyes} in Table \ref{tab:interactionmod}: 0.731.

This is a case where the intercept does not have a practical application because \texttt{vmiles} never equals 0, and even if it did, the predicted traffic fatality rate cannot be negative like the regression for states without mandatory sentencing would predict.

\hypertarget{variations}{%
\subsection{Variations}\label{variations}}

Variations on interaction models are beyond the scope of this book, but suffice it to say we can interact any two variables we deem necessary (or more). If you suspect that the effect of a variable on an outcome \emph{depends} on the value of another variable, then an interaction is how to model such a relationship.

\hypertarget{linear-probability-model}{%
\section{Linear probability model}\label{linear-probability-model}}

We have now covered the inclusion of categorical variables on the explanatory side of a regression model. We can also include categorical variables as an outcome. In fact, many interesting question involve outcomes of a categorical nature, particularly binary. For example,

\begin{itemize}
\tightlist
\item
  Did the person graduate from college (yes or no)?
\item
  Did the government default on its bond payments?
\item
  Did the program participant get a job afterward?
\item
  Did the nonprofit receive the grant it applied for?
\end{itemize}

As before, we may want to explain or predict such outcomes based on a set of explanatory variables.

A \textbf{linear probability model} (LPM) is just a special name we give the kind of regression we have already covered but when the outcome is a dummy variable instead of continuous. The key difference between an LPM and what we have already done concerns probability. Regression with a numerical outcome explains or predicts changes or values of the outcome in terms of the outcome's units.

\begin{quote}
The LPM explains or predicts changes or values in the \emph{probability} that the dummy outcome equals 1. If the dummy outcome is coded such that \(d=1\) means the event did occur, then the LPM estimates the change in or value of the probability that the event in question occurs given a change or value for the explanatory variable(s).
\end{quote}

Equation \eqref{eq:lpm} shows the generic population LPM, which is the same as the generic multiple regression population model except for the left-hand side. All this equation is trying to denote is that our estimates on the right-hand side pertain to the \emph{probability} (Pr) that \(y=1\). Equation \eqref{eq:lpmsample} shows the sample LMP equation.

\begin{equation}
Pr(y=1)=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_kx_k+\epsilon
\label{eq:lpm}
\end{equation}

\begin{equation}
\hat{Pr(y=1)}=b_0+b_1x_1+b_2x_2+\cdots +b_kx_k
\label{eq:lpmsample}
\end{equation}

Again, nothing is different with respect to how we use the above equations to answer questions concerning the predicted change or value of the outcome. We simply need to remember that those changes or values will be expressed as probabilities that \(y=1\) and what it means for \(y\) to equal 1 in the context of our particular question.

\hypertarget{using-lpm}{%
\subsection{Using LPM}\label{using-lpm}}

What if instead of modeling traffic fatality rates as an outcome dependent on miles driven and mandatory jail sentencing for drunk driving, we modeled whether a state has mandatory sentencing as an outcome dependent on traffic fatality rates and region? Perhaps states passed such laws because they have high traffic fatality rates. Equation \eqref{eq:lpmex} represents our model in this case.

\begin{equation}
Pr(jaild=1)=\beta_0+\beta_1vmiles+\beta_2region+\epsilon
\label{eq:lpmex}
\end{equation}

Scatter plots for LPMs are not particularly useful for communicating to an audience, but they can provide insight to what it is we are trying to do with an LPM, if it is not yet clear. Figure \ref{fig:lpmscatter} changes the coding of \texttt{jaild} from yes/no to 1/0 and plots it on the y axis against traffic fatality rates on the x axis. Regions are excluded for simplicity.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/lpmscatter-1} 

}

\caption{LPM visualization}\label{fig:lpmscatter}
\end{figure}

Immediately, we should notice Figure \ref{fig:lpmscatter} does not look like the typical scatter plots we have viewed thus far. This is because all observations fall into one of two values for \texttt{jaild}. It is also difficult to tell how our regression line is being drawn through the data.

The points along the x axis are states for which \texttt{jaild\ =\ 0}, meaning they do not impose mandatory jail sentencing for drunk driving. The points at the top are states for which \texttt{jaild\ =\ 1}. Compared to the points at the bottom, note the slight shift to the right the points at the top seem to have made. This shift is what informs the regression line to slope upward. The pattern of these data suggests there is a positive association between traffic fatality rate and passing a mandatory jail sentencing for drunk driving.

The values of \(y\) along the regression line are the predicted probabilities that a state has a mandatory jail law given the corresponding values for traffic fatality rate. For example, states with a rate of 3 appear to have a probability of 0.5 or 50\% of passing a mandatory jail law.

Running this model produces the following results

\begin{table}

\caption{\label{tab:lpmresults}LPM results}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\hline
intercept & -0.066 & 0.102 & -0.645 & 0.520 & -0.267 & 0.135\\
\hline
mrall & 0.117 & 0.054 & 2.171 & 0.031 & 0.011 & 0.222\\
\hline
regionN. East & 0.064 & 0.070 & 0.913 & 0.362 & -0.074 & 0.202\\
\hline
regionSouth & 0.040 & 0.068 & 0.580 & 0.562 & -0.094 & 0.173\\
\hline
regionWest & 0.361 & 0.078 & 4.634 & 0.000 & 0.208 & 0.514\\
\hline
\end{tabular}
\end{table}

Plugging these results into our sample regression equation gives us

\[\hat{Pr(jaild=1)}= -0.07 + (0.12 \times vmiles) + (0.06 \times d_{neast}) + (0.04 \times d_{south}) + (0.36 \times d_{west})\]

Once again, we are back to plug-and-chug. For example, what is the predicted probability that a state in the Midwest with a traffic fatality rate of 2.5 has a mandatory jail sentence for drunk driving?

\[\hat{Pr(jaild=1)}= -0.07 + (0.12 \times 2.5)\]

\[\hat{Pr(jaild=1)}=0.23\]

There is a 23\% likelihood that such a state has such a law.

How would an increase of 2 fatalities per 10,000 affect the probability that a state imposes a mandatory jail law?

\[\Delta \hat{Pr(jaild=1)}=0.12 \times 2\]

\[\Delta \hat{Pr(jaild=1)}=0.24\]

An increase in the traffic fatality rate of 2 is predicted to increase the probability that a state passes a mandatory jail sentence for drunk driving by about 24 \emph{percentage points}.

Furthermore, our results suggest states in the West are substantially more likely to have this law. Compared to the Midwest, the West is 36 percentage points more likely to have a law and about 30 percentage points more likely than states in the South and Northeast.

\hypertarget{fit}{%
\subsection{Fit}\label{fit}}

Because our outcome is a dummy variable, it does not have the same kind of variation we need to assess model fit like before using \(R^2\) or RMSE. Since we are explaining or predicting whether or not an outcome occurs, we can assess the fit of the model based on how well it predicts the observed outcomes.

We could change this threshold, but suppose we decide that if our model predicts the likelihood an outcome at 50\% or greater, then we say that our model predicts the outcome will occur \(y=1\) and so \(y=0\) otherwise. Table @ref(tab: lpmpointstab) shows a few rows applying this logic. Note that each row shows the observed data for each variable in our model, then the predicted probability in the \texttt{jaild\_hat} column, then the rounding of that probability to 0 or 1 in the \texttt{prediction} column. Note the similarities and differences between the observed outcomes in \texttt{jaild} and the predicted outcomes \texttt{prediction}. Sometimes our model predicts correctly, and sometimes it does not.

\begin{table}

\caption{\label{tab:lpmpointstab}Binary predictions from LPM}
\centering
\begin{tabular}[t]{r|r|r|l|r|r}
\hline
ID & jaild & mrall & region & jaild\_hat & prediction\\
\hline
29 & 0 & 2.174 & West & 0.549 & 1\\
\hline
113 & 1 & 1.461 & N. East & 0.169 & 0\\
\hline
254 & 0 & 0.821 & N. East & 0.094 & 0\\
\hline
98 & 1 & 1.936 & Midwest & 0.160 & 0\\
\hline
68 & 0 & 2.575 & West & 0.596 & 1\\
\hline
241 & 1 & 2.080 & West & 0.538 & 1\\
\hline
182 & 0 & 1.825 & N. East & 0.211 & 0\\
\hline
\end{tabular}
\end{table}

Table \ref{tab:confumat} below is referred to as a \textbf{confusion matrix}. It is simply a cross-tabulation of the observed outcomes and the predicted outcomes with the predictions along the top as columns.

\begin{table}

\caption{\label{tab:confumat}Confusion matrix for LPM}
\centering
\begin{tabular}[t]{l|r|r}
\hline
  & 0 & 1\\
\hline
0 & 210 & 31\\
\hline
1 & 56 & 38\\
\hline
\end{tabular}
\end{table}

We can see that there are 248 cases where our model correctly predicts the outcome (210 + 38). There are 87 cases where our model incorrectly predicts the outcome. Specifically, there are 31 cases where our model predicts a state has a law but doesn't and 56 cases where our model predicts a state does not have a law but does. We can also convert these to percentages like the table below. These confusion matrices help us assess and communicate how accurate our model is.

\begin{table}

\caption{\label{tab:unnamed-chunk-33}Confusion matrix for LPM (in proportions)}
\centering
\begin{tabular}[t]{l|r|r}
\hline
  & 0 & 1\\
\hline
0 & 0.79 & 0.45\\
\hline
1 & 0.21 & 0.55\\
\hline
\end{tabular}
\end{table}

\hypertarget{kt7}{%
\section{Key terms and concepts}\label{kt7}}

\begin{itemize}
\tightlist
\item
  Dummy variable
\item
  Parallel slopes
\item
  Interaction
\item
  Difference between parallel slopes and interaction models
\item
  Linear probability model (LPM)
\item
  Confusion matrix
\end{itemize}

\hypertarget{nonlinear-variables}{%
\chapter{Nonlinear Variables}\label{nonlinear-variables}}

\begin{quote}
\emph{``The shortest distance between two points is often unbearable.''}

---Charles Bukowski
\end{quote}

So far, we have repeatedly drawn straight lines through points. But, we know not all relationships are linear. Our income tends to rise and fall with age. Those in charge of the purchasing or production of something should know that average and marginal costs fall and then rise with quantity. Happiness tends to rise sharply with income but then plateaus at around \$70,000 per year. If our goal is to draw the line that fits data best, why draw a straight line through data that is evidently nonlinear?

In this chapter, we will cover two ways to incorporate nonlinear relationships:

\begin{itemize}
\tightlist
\item
  Include a quadratic
\item
  Include a logarithmic transformation
\end{itemize}

\hypertarget{lo8}{%
\section{Learning objectives}\label{lo8}}

\begin{itemize}
\tightlist
\item
  Explain why and how to extend a regression model to include a quadratic relationship
\item
  Interpret the coefficients associated with a quadratic term in a regression model
\item
  Compute the value of the quadratic explanatory variable at which the outcome is at its maximum or minimum
\item
  Explain the difference between percent change and percentage point change
\item
  Explain why to log-transform variables in a regression model
\item
  Interpret results from log-log, log-level, and level-log models
\end{itemize}

\hypertarget{quadratic}{%
\section{Quadratic}\label{quadratic}}

If we theorize or see visual evidence that the association between an explanatory variable and an outcome is such that the outcome initially increases or decreases as the explanatory variable increases, then, at some value of the explanatory variable, the outcome decreases, then we may want to include a quadratic term of that explanatory variable in our regression model. That long-winded statement warrants an immediate visualization provided by Figure \ref{fig:quadscatter} below.

Note that wage appears to initially increase with age, then decreases. The data present a pattern that resembles an inverted U, also known as a concave parabola. Age and wage is a classic example of a quadratic relationship. We should not force ourselves to fit a straight line to these data; we can estimate a better line.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/quadscatter-1} 

}

\caption{Wages by age}\label{fig:quadscatter}
\end{figure}

Equation \eqref{eq:quadratic} presents a generic population regression model with a quadratic term. With respect to the math, the only difference between this and previous models is the choice to square one of the explanatory variables. This is just an example. Any number of explanatory variables can be squared if theory warrants it.

\begin{equation}
y = \beta_0 + \beta_1x_1 + \beta_2x_1^2 + \beta_3x_2 + \cdots + \beta_kx_k + \epsilon
\label{eq:quadratic}
\end{equation}

Thus, the sample equation is as follows

\begin{equation}
\hat{y} = b_0 + b_1x_1 + b_2x_1^2 + b_3x_2 + \cdots + b_kx_k
\label{eq:quadraticsamp}
\end{equation}

Fully understanding the logic of the above equations to answer questions we may encounter as we have done before involves calculus that this book will spare you. In order to report the marginal effect of a variable that has been squared on an outcome in regression, we use Equation \eqref{eq:quadraticsampmarg} below. The result of this equation provides the predicted change in \(y\) given a one-unit change in \(x_1\).

\begin{equation}
b_1 + 2b_2x_1
\label{eq:quadraticsampmarg}
\end{equation}

Note that had we not squared \(x_1\) the predicted change in \(y\) from a one-unit change in \(x_1\) would be \(b_1\), which is exactly the same as in previous models. However, now that we are drawing a curved line, the effect of a one-unit change in \(x\) on \(y\) is not constant; it changes depending on the value of \(x\).

Another important question when a quadratic relationship is involved is at what value of \(x\) is \(y\) maximized or minimized. This can help decision-making, such as how to minimize costs or maximize profit, or maximize the probability of some desirable outcome. In order to report the value of \(x\) at which \(y\) reaches its maximum or minimum, we use Equation \eqref{eq:quadraticsampopt} below. The result of the equation gives us the optimal value of \(x\).

\begin{equation}
x = {\frac{-b_1}{2b_2}}
\label{eq:quadraticsampopt}
\end{equation}

\hypertarget{using-quadratics}{%
\subsection{Using quadratics}\label{using-quadratics}}

Suppose we collect the following data.

\begin{table}

\caption{\label{tab:wagestab}Preview of wages, age, and education}
\centering
\begin{tabular}[t]{r|r|r}
\hline
Wage & Educ & Age\\
\hline
19.13 & 9 & 35\\
\hline
25.67 & 12 & 37\\
\hline
38.77 & 21 & 41\\
\hline
32.37 & 17 & 53\\
\hline
19.34 & 6 & 38\\
\hline
18.76 & 18 & 73\\
\hline
18.11 & 14 & 21\\
\hline
\end{tabular}
\end{table}

Therefore, our regression equation is as follows

\begin{equation}
Wage = \beta_0 + \beta_1Age + \beta_2Age^2 + \beta_3Educ + \epsilon
\label{eq:quadraticex}
\end{equation}

Running this regression generates the following results

\begin{table}

\caption{\label{tab:quadextab}Quadratic model results}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\hline
intercept & -22.722 & 3.023 & -7.517 & 0 & -28.742 & -16.701\\
\hline
Age & 1.350 & 0.134 & 10.077 & 0 & 1.083 & 1.617\\
\hline
I(Age\textasciicircum{}2) & -0.013 & 0.001 & -9.840 & 0 & -0.016 & -0.011\\
\hline
Educ & 1.254 & 0.090 & 13.990 & 0 & 1.075 & 1.432\\
\hline
\end{tabular}
\end{table}

In Table \ref{tab:quadextab}, note that there are two rows for Age--one for the linear or level term and a second for the quadratic term. When the quadratic relationship is an inverted U, or concave, the linear term will be positive and the quadratic will be negative. This corresponds with an initial positive relationship that eventually turns negative once the negative quadratic term overcomes the positive linear term.

Plugging our results from Table \ref{tab:quadextab} into the regression equation, we obtain the following equation

\begin{equation}
\hat{Wage} = -22.7 + 1.4\times Age - 0.01\times Age^2 + 1.3\times Educ
\label{eq:quadraticexsamp}
\end{equation}

We can answer questions regarding the predicted \emph{value} of Wage the same way as before. For example, the predicted wage of an individual who is 40 years old with 16 years of education is

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}}\FloatTok{22.7} \SpecialCharTok{+} \FloatTok{1.4}\SpecialCharTok{*}\DecValTok{40} \SpecialCharTok{{-}} \FloatTok{0.01}\SpecialCharTok{*}\DecValTok{40}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \FloatTok{1.3}\SpecialCharTok{*}\DecValTok{16}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 38.1
\end{verbatim}

dollars per hour.

To predict the \emph{change} in wage given a change in age, we need to know the beginning point for age. For example, if we were asked what is the predicted change is wage for a 24-year old two years later who consequently increases their education from 16 to 18 to get their masters degree, we plug this scenario into Equation \eqref{eq:quadraticsampmarg} like so

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\SpecialCharTok{*}\NormalTok{(}\FloatTok{1.4} \SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\FloatTok{0.01}\SpecialCharTok{*}\DecValTok{24}\NormalTok{) }\SpecialCharTok{+} \FloatTok{1.25}\SpecialCharTok{*}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.34
\end{verbatim}

providing us the answer of a predicted increase of \$4.34.

Controlling for education, at what age do wages tend to reach their maximum? To answer, we plug the results into \eqref{eq:quadraticsampopt} like so

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}}\FloatTok{1.35}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*{-}}\FloatTok{0.013}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 51.92308
\end{verbatim}

According to our results, wages reach their maximum at around 52 years of age.

\hypertarget{log-models}{%
\section{Log models}\label{log-models}}

Once again, we will forego the math of logarithms and instead focus on why we may want to use them in a regression model and how to interpret the results. In short, logarithms are used to express rates of change in a variable (i.e.~percent change) rather than absolute change in a variable (i.e.~unit change).

\hypertarget{logarithmic-scales}{%
\subsection{Logarithmic scales}\label{logarithmic-scales}}

Graphs like Figure \ref{fig:logcovid} below were commonplace during the initial COVID-19 spread. Take a look at the small note at the bottom explaining to readers the purpose of a logarithmic scale. Note how the values on the y axis are evenly dispersed, but each tick mark increases by a factor of 10 (i.e.~the previous value multiplied by 10). The y-axis is in a \textbf{log10} scale. Another common log scale for visualization is a \textbf{log2} scale which increases each interval by a factor of 2.

The use of a log scale allows us to compare states like New York and Wyoming by taking into account large differences in absolute numbers. It would be unfair to compare the absolute number of COVID cases in New York to the absolute number of cases in Wyoming. It would also be unfair to compare the absolute number of new cases each day between the two states. A non-trivial portion of those numbers is a result of the size of the population in each state.

However, it is fair to compare the \emph{rate} of growth between the two states. While it is obviously concerning that New York has over 300,000 deaths, the key feature of this graph is that we can compare the slopes of each state's growth path because population size has been accounted for by the y-axis. Given New York's population and population density, it was likely to have the most cases, but the state also had the fastest growth rate in cases from about day 5 to day 10.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/logs_covid} 

}

\caption{Growth in COVID-19 cases by state}\label{fig:logcovid}
\end{figure}

\hypertarget{percent-v-percentage-point-change}{%
\subsection{Percent v percentage point change}\label{percent-v-percentage-point-change}}

Rate of change typically refers to percent change. The equation for percent change is shown below.

\begin{equation}
PctChange = {\frac{NewValue-OldValue}{OldValue}} \times 100
\label{eq:pctchange}
\end{equation}

For example, if the number of COVID cases increase from 1,000 to 10,000 over the course of a week, the absolute change is 9,000. The percent change is 900\%.

A common cause of confusion is the difference between a percent change and a percentage point change. This occurs when we discuss the change in a variable that is already expressed as a percent. For example, if the U.S. unemployment rate increases from 4\% to 15\% during the pandemic, that's an absolute change of 11 percentage points. The unemployment rate is expressed units of percentage points, so a unit change is a percentage point change. An increase from 4\% to 15\% is also a 275\% percent change.

As we have seen in previous examples of regression, when we include a variable that is not log-transformed, regression estimates the \textbf{unit change} in \(y\) given a \textbf{unit change} in \(x\). If a variable is expressed in units of percentages like unemployment or poverty, then a unit change for those variables is a percentage point change. Including a log-transformed variable in regression estimates percent changes in the variable(s) we transformed.

One reason we may prefer to use percent change is if the variable in question has some underlying impact that differs depending on the initial value from which it changed. This too applies to measures of wealth or income. Suppose we estimate that a policy will, on average, increase peoples' incomes by 12,000 dollars. This average unit change does not quite capture the benefit of the policy. Imagine a society of two people. One person earns 20,000 dollars per year and the other earns 80,000 dollars. That 12,000 likely has a greater positive impact on the low-income individual than it does the high-income individual. Consequently, this can be expressed in percent change. The 12,000 represents a 60\% increase in income for the low-income individual and 15\% for the high-income individual.

\hypertarget{why-logs-in-regression}{%
\subsection{Why logs in regression}\label{why-logs-in-regression}}

To summarize, we may want to use logs in regression if

\begin{itemize}
\tightlist
\item
  it is preferable to express change in percentages rather than units
\item
  a variable we intend to include has a skewed distribution
\item
  we theorize the relationship between two variables follows a logarithmic path
\end{itemize}

Let's consider these last two reasons further. As was mentioned, log scales allow us to compare numbers that are very far apart, as seen in Figure \ref{fig:logcovid}. If the scale for COVID cases were left in constant units, New York and a few other states would be so far above most other states that it would be difficult to fit in a sensible graph. Using logs condensed or pulled those extreme numbers back to a more compact distribution.

As will become clear in the next section on inference, using a sample to make valid conclusions about a population relies heavily on the normal distribution, which was introduced in Chapter \ref{descriptive-statistics}. In a similar sense, we want the variables we use for inference to be approximately normally distributed because extreme values of a skewed distribution can impose undue influence on our results. Log-transformations can transform a skewed distribution to be more normal.

For instance, variables that measure income or wealth tend to be right-skewed. Figure \ref{fig:gapskew} shows the distribution of GDP per capita across most countries in multiple years. Clearly this distribution is not normal and skewed to the right. It is difficult to see because there are so few cases, but some countries have GDP per capita near or more than \$120,000.

Figure \ref{fig:gaplog} shows the distribution if we convert GDP per capita to a log scale (log10 was used but any log scale will achieve the same normalization). Now we have a more normal distribution. This is desirable in statistics.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/gapskew-1} 

}

\caption{Distribution of GDP per capita}\label{fig:gapskew}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/gaplog-1} 

}

\caption{Distribution of log10 GDP per capita}\label{fig:gaplog}
\end{figure}

The third reason for using logs concerns theory, which should always inform the choices we make in statistics. When choosing how to model the relationship between an outcome and an explanatory variable, if past research, experience, visualization of data, or intuition tells us that the outcome changes dramatically at first, then begins to flatten, a logarithmic transformation should be used.

For example, suppose we wish to examine the relationship between national wealth and life expectancy. Intuitively, we expect this relationship to be positive--as wealth increases, life expectancy should increase. Also, life expectancy has some natural ceiling, so it cannot increase indefinitely, and we may expect relatively small increases from low levels of wealth to have much greater impacts on life expectancy then similar increases from high levels of wealth.

Figure \ref{fig:gapskewscatter} provides a scatter plot of GDP per capita and life expectancy in their original units. Note the rapid increase then plateau in life expectancy. A regression line would not fit these data well.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/gapskewscatter-1} 

}

\caption{Relationship between wealth and life expectancy using unit scale}\label{fig:gapskewscatter}
\end{figure}

Using a log transformation in an association between two variables does not change the underlying data or relationship, but it \emph{does} transform the pattern of points to be more linear, thus allowing a linear regression line to do a better job modeling the relationship. Figure \ref{fig:gaplogscatter} uses the same data but GDP per capita has been transformed to log scale. This simple change makes a big difference for the validity of any conclusions we may make regarding the relationship between wealth and life expectancy.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/gaplogscatter-1} 

}

\caption{Relationship between wealth and life expectancy using log scale}\label{fig:gaplogscatter}
\end{figure}

\hypertarget{using-log-models}{%
\subsection{Using log models}\label{using-log-models}}

There are three variations of the log model:

\begin{itemize}
\tightlist
\item
  Level-log: log transforming one or more explanatory variables but not the outcome
\item
  Log-level: log transforming the outcome but not an explanatory variable
\item
  Log-log: log transforming the outcome and an explanatory variable
\end{itemize}

Each model fits slightly different patterns of association best but they share the general pattern of a pronounced initial increase or decrease followed by a plateau. If past research, visuals, or theory does not lead us to choose one model over the other, one option is to compare the goodness-of-fit between the three, choosing the one with the highest \(R^2\) or lowest RMSE.

One last point before presenting each of the models and how to interpret: using the logarithmic transformation uses a special log scale called the \textbf{natural log}, often denoted as \textbf{\(ln\)}, as opposed to, say, \(log_{10}\) or \(log_2\). You do not need to concern yourself with the mathematical properties of the natural log. Just know that the natural log is what is used in regression to transform unit changes to percent changes.

\hypertarget{log-log}{%
\subsubsection*{Log-log}\label{log-log}}
\addcontentsline{toc}{subsubsection}{Log-log}

The log-log model is somewhat special among the three variations because it estimates a commonly used measure in economic or policy analyses--the \textbf{elasticity}. You may have already learned in policy analysis courses that the \textbf{elasticity is the percent change in an outcome given a one percent change in the explanatory variable}.

Equation \eqref{eq:loglog} presents a generic log-log model. Log-log is simply meant to convey that we logged our outcome and logged at least one explanatory variable.

\begin{equation}
ln(y)=\beta_0 + \beta_1ln(x_1) + \beta_2x_2 + \cdots + \beta_kx_k + \epsilon
\label{eq:loglog}
\end{equation}

Thus, the sample equation is

\begin{equation}
\hat{ln(y)}=b_0 + b_1ln(x_1) + b_2x_2 + \cdots + b_kx_k
\label{eq:loglogsamp}
\end{equation}

When we obtain an estimate for \(b_1\) we can plug it into the following template

\begin{quote}
On average, a one percent change in \(x_1\) is associated with a \(b_1\) percent change in \(y\), all else equal.
\end{quote}

Or, if we wanted to report using elasticity language, assuming our audience understands what we are talking about:

\begin{quote}
According to the results, the \(x_1\) elasiticy of \(y\) is \(b_1\).
\end{quote}

\hypertarget{level-log}{%
\subsubsection*{Level-log}\label{level-log}}
\addcontentsline{toc}{subsubsection}{Level-log}

Equation \eqref{eq:levlog} presents a generic level-log model. Level-log is simply meant to convey that we logged at least one explanatory variable.

\begin{equation}
y=\beta_0 + \beta_1ln(x_1) + \beta_2x_2 + \cdots + \beta_kx_k + \epsilon
\label{eq:levlog}
\end{equation}

Thus, the sample equation is

\begin{equation}
\hat{y}=b_0 + b_1ln(x_1) + b_2x_2 + \cdots + b_kx_k
\label{eq:levlogsamp}
\end{equation}

When we obtain an estimate for \(b_1\) we can plug it into the following template

\begin{quote}
On average, a one percent change in \(x_1\) is associated with a \(\frac{b_1}{100}\) unit change in \(y\), all else equal.
\end{quote}

Or, if dividing our estimate by 100 results in too small of a number to report, we can say the following

\begin{quote}
On average, a doubling of \(x_1\) is associated with a \(b_1\) unit change in \(y\), all else equal.
\end{quote}

because a doubling is equal to a 100 percent increase. Multiplying \(\frac{b_1}{100}\) by 100 cancels out the 100 in the denominator, leaving us with just \(b_1\).

\hypertarget{log-level}{%
\subsubsection*{Log-level}\label{log-level}}
\addcontentsline{toc}{subsubsection}{Log-level}

Equation \eqref{eq:loglev} presents a generic log-level model. Log-level is simply meant to convey that we logged our outcome.

\begin{equation}
ln(y)=\beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_kx_k + \epsilon
\label{eq:loglev}
\end{equation}

Thus, the sample equation is

\begin{equation}
\hat{ln(y)}=b_0 + b_1x_1 + b_2x_2 + \cdots + b_kx_k
\label{eq:loglevsamp}
\end{equation}

When we obtain an estimate for \(b_1\) we can plug it into the following template

\begin{quote}
On average, a one unit change in \(x_1\) is associated with a \(b_1 \times 100\) percent change in \(y\), all else equal.
\end{quote}

\hypertarget{example}{%
\subsubsection*{Example}\label{example}}
\addcontentsline{toc}{subsubsection}{Example}

Let's continue our investigation of national life expectancy using the various log models. Suppose we are interested in using the following base model for the three varieties of log models. Continent is included because perhaps we think it will capture some geographical, social, and/or cultural differences that impact life expectancy.

\begin{equation}
LifeExp=\beta_0 + \beta_1GDPpercap + \beta_2Continent + \epsilon
\label{eq:logex}
\end{equation}

The following tables present results for each of the three log models.

\begin{table}

\caption{\label{tab:loglogtab}Log-log results}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\hline
intercept & 3.062 & 0.026 & 117.692 & 0 & 3.011 & 3.113\\
\hline
log(gdpPercap) & 0.112 & 0.004 & 31.843 & 0 & 0.105 & 0.119\\
\hline
continentAmericas & 0.133 & 0.011 & 12.519 & 0 & 0.112 & 0.154\\
\hline
continentAsia & 0.110 & 0.009 & 12.037 & 0 & 0.092 & 0.128\\
\hline
continentEurope & 0.166 & 0.012 & 14.357 & 0 & 0.143 & 0.189\\
\hline
continentOceania & 0.152 & 0.029 & 5.187 & 0 & 0.095 & 0.210\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:levlogtab}Level-log results}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\hline
intercept & 2.317 & 1.359 & 1.704 & 0.088 & -0.349 & 4.983\\
\hline
log(gdpPercap) & 6.422 & 0.183 & 35.003 & 0.000 & 6.062 & 6.782\\
\hline
continentAmericas & 7.015 & 0.554 & 12.652 & 0.000 & 5.927 & 8.102\\
\hline
continentAsia & 5.912 & 0.477 & 12.400 & 0.000 & 4.977 & 6.847\\
\hline
continentEurope & 9.577 & 0.604 & 15.855 & 0.000 & 8.392 & 10.762\\
\hline
continentOceania & 9.213 & 1.536 & 5.999 & 0.000 & 6.201 & 12.226\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:loglevtab}Log-level results}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\hline
intercept & 3.856 & 0.006 & 601.881 & 0 & 3.843 & 3.869\\
\hline
gdpPercap & 0.000 & 0.000 & 16.374 & 0 & 0.000 & 0.000\\
\hline
continentAmericas & 0.250 & 0.011 & 22.054 & 0 & 0.228 & 0.272\\
\hline
continentAsia & 0.160 & 0.010 & 15.322 & 0 & 0.140 & 0.181\\
\hline
continentEurope & 0.311 & 0.012 & 26.383 & 0 & 0.288 & 0.334\\
\hline
continentOceania & 0.316 & 0.034 & 9.381 & 0 & 0.250 & 0.382\\
\hline
\end{tabular}
\end{table}

Our log-log results indicate that a one percent increase in GDP per capita is associated with a 0.11 percent increase in life expectancy, on average and controlling for continent.

Our level-log results indicate that a one percent increase in GDP per capita is associated with an increase in life expectancy of 0.06 years.

Our log-level results indicate that a one dollar increase in GDP per capita is associated with a indiscernible percent increase in life expectancy. Changing GDP per capita from dollars to something like thousands of dollars would probably give an estimate that doesn't round to 0.

The continent estimates can be interpreted in similar fashion, remembering that with categorical variables, the estimate of each level of the variable is relative to the base comparison excluded from the equation. In this example, Africa is the base comparison. Let's focus on Asia for interpretation.

Our log-log results indicate that life expectancy in Asia is 11\% greater than life expectancy in Africa. Since a dummy variable can only change from 0 to 1, this is equivalent to a 100 percent change. Therefore, we must multiply our estimate by 100.

Our level-log results indicate that life expectancy in Asia is 5.9 years greater than life expectancy in Africa. Lastly, our log-level results indicate that life expectancy in Asia is 16\% greater than life expectancy in Africa.

\begin{quote}
\textbf{To learn how to include nonlinear variables in regression using R, proceed to Chapter \ref{r-nonlinear-regression}.}
\end{quote}

\hypertarget{kt8}{%
\section{Key terms and concepts}\label{kt8}}

\begin{itemize}
\tightlist
\item
  Marginal effect
\item
  Logarithmic scale
\item
  Percent change
\item
  Percentage point change
\item
  Natural log
\item
  Elasticity
\end{itemize}

\hypertarget{causation-and-bias}{%
\chapter{Causation and Bias}\label{causation-and-bias}}

\begin{quote}
\emph{``All models are wrong but some are useful.''}

---George Box
\end{quote}

Our regression toolbox has grown considerably over the previous three chapters. We can now set out to answer a multitude of research questions that require us to accommodate different types of variables that may share linear or nonlinear relationships. Nevertheless, a model is a simplified version of our complex world. In this sense, all models are wrong. The goal is to make modeling choices that prevent our model from being \emph{so} wrong that they are useless for decision-making.

\hypertarget{lo9}{%
\section{Learning objectives}\label{lo9}}

\begin{itemize}
\tightlist
\item
  Explain the difference between using regression to predict an outcome versus explain an outcome and the consequences each has on model choices
\item
  Explain internal and external validity
\item
  Explain the criteria to validly claim a causal relationship
\item
  Use a directed acyclical graph (DAG) to represent a regression model
\item
  Identify confounders and colliders in a DAG
\item
  Identify the number of backdoor paths in a DAG and each are open or closed
\item
  Given a DAG, identify the variables one would need to control for to close any backdoor paths
\item
  Determine whether a regression model plausibly eliminates omitted variable bias
\item
  Predict the direction of omitted variable bias
\end{itemize}

First, let's consider the two possible goals of regression:

\begin{itemize}
\tightlist
\item
  \textbf{Explain} the change in an outcome due to a change in a set of explanatory variables
\item
  \textbf{Predict} the value of an outcome given values for a set of explanatory variables
\end{itemize}

The usefulness of a model with the sole goal of prediction is how well it predicts the outcome. That may sound obvious and cyclical but it is an important point. Which variables we choose to include and their linear or nonlinear relationships is a secondary concern of prediction, if at all. If we don't care \emph{how} variables affect the outcome and only care about predicting the outcome with the greatest accuracy and precision possible, then we can throw together whatever model we want to achieve that without much concern for what the model actually means.

Models with the goal of prediction are common within the field of forecasting, which will be introduced in Chapter \ref{forecasting}. Sometimes, we care about good prediction and \emph{how} some explanatory variables impact the outcome. Then we are back in the realm of explanation where we have to take special care about which variables are included and excluded from our model as well as how they relate with each other.

The focus of this chapter is explanation using regression. Many scenarios within program evaluation or policy analysis involve explaining whether and to what extent one variable impacts another we care about changing. Ultimately, our concern is causality. It is one thing to conclude two variables are associated with each other; it is an entirely different thing to conclude that a change in one variable \emph{causes} the other to change. If we propose to spend millions of dollars on a program to help people, or decide to cut a program that does not, we should be as certain as about this causal claim as statistics allows us to be.

Sufficient understanding of causality or causal inference warrants its own course. It does not involve regression models that much different from what you have learned so far or will learn by the end of this book, but it does involve a broader and deeper understanding of research design and knowing how to identify threats to internal and external validity. Let us revisit the figure of credible analysis first shown in Chapter \ref{measurement-and-missing}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/credible} 

}

\caption{Components of credible analysis}\label{fig:credfigrepeat}
\end{figure}

Chapter \ref{measurement-and-missing} covered measurement validity and measurement reliability. Now, let us define internal validity and external reliability.

\begin{itemize}
\tightlist
\item
  \textbf{Internal validity:} the credibility of the theoretical assumptions applied to the causal connection established between the explanatory variable(s) and its (their) effect on the outcome.
\item
  \textbf{External validity:} can results of the analysis be applied beyond the subjects included or context involved?
\end{itemize}

In other words, is there reason to believe our results are critically mistaken and are those affected or the context in which they were affected so unique or limited that we can not generalize to other potential targets or contexts?

\hypertarget{causality}{%
\section{Causality}\label{causality}}

Three conditions must be met to credibly claim a causal relationship. We will address each in turn.

\begin{itemize}
\tightlist
\item
  The explanatory variable is correlated with the outcome
\item
  The change in the explanatory variable occurred prior to the change in the outcome
\item
  No alternative explanation exists to which the change in the outcome could be attributed instead of the explanatory variable
\end{itemize}

Correlation between the explanatory and outcome variables is perhaps the most straightforward condition to satisfy. We have not yet covered inference and how to identify statistically significant results. For now, suffice it to say that if we run a regression and the estimate for our explanatory variable is statistically significant, then we have established correlation between the explanatory and outcome variables that is unlikely to be random.

The second condition is succinctly referred to as \textbf{temporal precedence}. In order for something to be a cause, it must occur prior to its alleged effect. Otherwise, perhaps it is our supposed outcome that is having an effect on the cause, similar to what was considered in Chapter \ref{categorical-variables-and-interactions} with the mandatory jail for drunk driving. This is also known as \textbf{reverse causality}.

Reverse causality could still be argued even when it seems clear the cause occurred prior to the effect. For example, the mere availability of a scholarship may cause students to reach higher levels of academic achievement. If we were to claim receipt of the scholarship caused a rise in the likelihood of completing college, which obviously occurred prior to graduation, this may not be accurate. Perhaps by virtue of motivating oneself to perform better in school would result in a higher likelihood of graduation whether the scholarship was received or not. Then again, the student would not have been as motivated if not for the scholarship. Perhaps the \emph{availability} of the scholarship would meet temporal precedence more convincingly.

As may be evident by now, something as seemingly simple as before-and-after can become complex if the causal pathway is considered carefully. Credible causal claims require subject matter expertise as much as quantitative skills. For temporal precedence, do your best to ensure your explanatory variable was measured or occurred prior to when your outcome was measured or occurred.

The remainder of this chapter concerns the third and most difficult condition to satisfy: no plausible rival or alternative explanation for our causal claim. Since we do not have the luxury of delving deep into causal modeling, what can MPA students learn that will serve them well when they need to consider whether the possibility of alternative explanations has been reasonably eliminated? My answer to this is the \textbf{directed acyclical graph}.

\hypertarget{directed-acyclical-graphs}{%
\section{Directed acyclical graphs}\label{directed-acyclical-graphs}}

Directed acyclical graphs (DAGs) are visual representations of causal pathways. Constructing a DAG involves theory, existing research, or theory. A DAG requires us to state our assumptions clearly, thus allowing us and others to evaluate the internal validity of our model.

Figure \ref{fig:dagbasic} below shows a basic DAG. This model involves three variables, X, Y, and Z. Y denotes the outcome and X denotes the variable of primary interest for which we intend to estimate a causal effect. Z denotes any other variables in the model. The arrow from X to Y indicates our claim that X causes changes in Y. X also causes Z to change, and Z causes Y to change. In this example, the claim is that X has a direct effect on Y and an indirect (mediated) effect on Y through Z.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/dag_basic} 

}

\caption{A basic DAG}\label{fig:dagbasic}
\end{figure}

Any DAG follows a few rules and conventions:

\begin{itemize}
\tightlist
\item
  Only one-directional arrows (directed)
\item
  No arrow from Y back to X (acyclical)
\item
  Solid arrows used for relationships between observable variables or variables specifically observed in our data
\item
  Dashed arrows used for relationships between unobservable variables (e.g.~ability, attitudes, propensities for certain behaviors) or variables unobserved in our data
\end{itemize}

Underlying every regression with the goal to explain a causal relationship is a DAG. Consider the following regression model

\begin{equation}
BMI = \beta_0 + \beta_1Exercise + \epsilon
\label{eq:dagreg}
\end{equation}

Figure \ref{fig:dagreg} below shows the DAG that corresponds with Equation \eqref{eq:dagreg}. The central claim is that exercise causes BMI to change. Therefore, there is a solid arrow from exercise to BMI. Also, note that because \(\epsilon\), by definition, represents all other unobserved factors that affect BMI, there is a dashed line from \(\epsilon\) to BMI. Lastly, this DAG makes the assumption that no variables contained in \(\epsilon\) affects exercise, nor does exercise affect any variables contained in \(\epsilon\)

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/dag_reg} 

}

\caption{DAG representation of a regression model}\label{fig:dagreg}
\end{figure}

With this basic set-up, we can begin to learn how to evaluate a DAG in order to determine if our regression model credibly eliminates alternative explanations of our causal claim.

\hypertarget{evaluationg-dags}{%
\subsection{Evaluationg DAGs}\label{evaluationg-dags}}

We will review two aspects of evaluating DAGs for causal claims:

\begin{itemize}
\tightlist
\item
  Identifying backdoor paths
\item
  Adjusting regression models based on the presence of confounding or colliding variables
\end{itemize}

A backdoor path is any indirect path from X to Y no matter the direction of the arrows that connect it. For example, Figure \ref{fig:dagbasic} has one backdoor path: X --\textgreater{} Z --\textgreater{} Y. Figure \ref{fig:dagreg} has no backdoor paths between exercise and BMI.

\hypertarget{confounders}{%
\subsubsection*{Confounders}\label{confounders}}
\addcontentsline{toc}{subsubsection}{Confounders}

Identifying backdoor paths allow us to then identify whether confounding or colliding variables are present in our model. Figure \ref{fig:dagconf} below shows a variation on the simple DAG with one backdoor path. The backdoor path still runs from X to Z to Y, but now the direction of the arrows connecting this path are different. Specifically, the backdoor path is X \textless-- Z --\textgreater{} Y.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/dag_confound} 

}

\caption{Example of confounder variable}\label{fig:dagconf}
\end{figure}

The variable Z in this case is a \textbf{confounder}. Any variable in a backdoor path with arrows directed away from it toward X and Y is a confounder. This representation means Z affects X and Y. If as Z changes, X and Y change, then we may incorrectly attribute the effect of Z on Y to the effect of X on Y because it appears to us that as X changes, Y changes. And it does, but it is really Z that is causing changes.

This example of a confounder is sometimes referred to as spurious correlation. For example, ice cream sales and crime are spuriously correlated due to both increasing because of temperature. It would be mistake to claim an increase in ice cream sales causes an increase in crime.

\hypertarget{colliders}{%
\subsubsection*{Colliders}\label{colliders}}
\addcontentsline{toc}{subsubsection}{Colliders}

Figure \ref{fig:dagcoll} below shows another variation of the simple DAG with one backdoor path between X and Y that goes through Z. The direction of the arrows are such that this backdoor path can be written as X --\textgreater{} Z \textless-- Y. In this case, Z is a \textbf{collider}. Any variable on which the arrows connecting X and Y converge is a collider.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/dag_collide} 

}

\caption{Example of collider variable}\label{fig:dagcoll}
\end{figure}

Z is just some variable that is affected by both X and Y. Changes in Z do not cause changes in X or Y. Therefore, if we estimate how much Y changes in response to a change in X, Z has nothing to do with that causal estimate.

\hypertarget{backdoor-criterion}{%
\subsection{Backdoor criterion}\label{backdoor-criterion}}

For any theoretical model we intend to estimate via regression, we can now identify backdoor paths and whether there are confounding or colliding variables along those backdoor paths. How do we use this information to eliminate plausible alternative explanations and confidently claim a one-unit change in X causes Y to change by \(\beta\)? We need to satisfy the \textbf{backdoor criterion}.

The backdoor criterion is satisfied if all backdoor paths between X and Y are closed. We know if a backdoor path is open or closed depending on the presence of confounding or colliding variables.

\begin{itemize}
\tightlist
\item
  A confounder variable opens a backdoor path
\item
  A collider variable closes a backdoor path
\end{itemize}

If a backdoor path is open, we can close it by controlling for the confounder variable or any other variable along the backdoor path between X and Y. For instance, if we were to identify a backdoor path of the following form

X \textless-- Z --\textgreater{} A --\textgreater{} Y

where A is some fourth variable in our model, then controlling for Z or A will close this backdoor path. If a set of control variables in our regression model closes all backdoor paths, then we have satisfied the backdoor criterion and we can consider our estimate of X on Y to be a causal estimate.

Consider the simple backdoor path again where Z is a \textbf{collider} variable.

X --\textgreater{} Z \textless-- Y

This backdoor path is already closed. We don't need to control for anything in our model because of this backdoor path. In fact, \textbf{controlling for a collider opens a backdoor path}. Therefore, we should not control for Z in our model, as doing so opens a backdoor that was already closed.

This is a valuable insight provided by the use of DAGs. If we already have the data, it costs us virtually no time or effort to include a variable in our model we think may affect our outcome Y, and it can be quite tempting to throw variables into a regression model for not much more reason than we have it in our data. However, a DAG helps us consider all of the relationships between the variables in our model. If an explanatory variable is a collider, then including it may threaten our ability to make causal claims. Sometimes, deliberately excluding a variable from a regression model is the right choice and DAGs give us a fairly simple way to make and explain that choice.

Consider another backdoor path where Z is a collider and A is a confounder

X --\textgreater{} Z \textless-- A --\textgreater{} Y

A opens this backdoor path but Z blocks A's confounding. Controlling for Z would open this backdoor path. We can control for A in our regression without reopening the backdoor path, but it is not necessary.

\hypertarget{dags-and-regression}{%
\subsection{DAGs and regression}\label{dags-and-regression}}

Let us relate this new information to making choices about regression models. Referring back to Equation \eqref{eq:dagreg} and Figure \ref{fig:dagreg}, recall that \(\epsilon\) represents all other variables we do not observe or cannot include in our regression model but affect our outcome, BMI. Based on the DAG for this regression model, there are no backdoor paths. Therefore, whatever estimate we get for \(\beta_1\) is causal estimate.

However, Figure \ref{fig:dagreg} is probably incorrect. There are likely variables contained in \(\epsilon\) that affect exercise. If that is the case, then our DAG should be drawn like Figure \ref{fig:dagregovb} below. Now we have a backdoor path of the form

Exercise \textless-- \(\epsilon\) --\textgreater{} BMI

where \(\epsilon\) is a confounder. Therefore, this backdoor path is open, and because we do not observe the variables in \(\epsilon\), we do not currently have the means to close it.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/dag_regovb} 

}

\caption{Counfounding error term}\label{fig:dagregovb}
\end{figure}

The issue illustrated by Figure \ref{fig:dagregovb} is commonly referred to as \textbf{omitted variable bias} (OVB) and it is the bane of analysts' attempts to estimate causal relationships. To have omitted variable bias means we have failed to satisfy the third criterion of causality. There is a variable out there we have not controlled for which causes our explanatory variable of interest and our outcome to change. Therefore, we cannot trust our estimate of the effect of our explanatory variable on our outcome because it may be due to the omitted variable. In other words, an omitted variable is biasing our estimate, \(b_1\) to be systematically above or below the population parameter \(\beta_1\).

Our next task then is to identify the set of variables that would eliminate the arrow from \(\epsilon\) to exercise. If we can credibly break the link between \(\epsilon\) and our explanatory variables, then we can credibly claim there is no omitted variable bias in our model.

For the sake of this example, suppose healthy eating is the key omitted variable. Healthy nutrition gives us the energy to exercise and obviously affects our BMI. Suppose we collect a variable that measures the extent to which a person's diet is healthy. Now, we have a new DAG, as depicted in Figure \ref{fig:dagregnut}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/dag_regnut} 

}

\caption{Eliminating OVB}\label{fig:dagregnut}
\end{figure}

To close the backdoor path in this DAG, we need to control for nutrition in our regression model. Equation \eqref{eq:dagreg2} shows our new regression model. If our theory informing our new DAG is correct, then our estimates of \(\beta_1\) and \(\beta_2\) are unbiased.

\begin{equation}
BMI = \beta_0 + \beta_1Exercise + \beta_2Nutrition + \epsilon
\label{eq:dagreg2}
\end{equation}

The DAG in Figure \ref{fig:dagregnut} may still be fail to be sufficiently convincing to those who have expertise in public health or related fields. In that case, we would continue the process of identifying and controlling for variables until we credibly break the link between \(\epsilon\) and all explanatory variables in our model.

Isolating causal effects is hard, and careers can be made by successfully doing so. Regardless of whether an unbiased causal estimate can be obtained for a particular question, knowing whether or not threats exist and what could be done about it is valuable, especially for managers or consumers of statistical analyses who have expertise concerning the potential causal pathways involved.

\hypertarget{direction-of-ovb}{%
\section{Direction of OVB}\label{direction-of-ovb}}

Fortunately, we can salvage estimates that suffer from omitted variable bias to make causal conclusions in some cases. Again, doing so requires us to have knowledge about the variables involved and their causal pathways.

We will cover this more thoroughly in the following chapters, but a statistically significant result means we can confidently conclude the association between X and Y is not equal to zero. In other words, our regression results provide an estimate so much less or greater than zero that it would be highly unlikely to see these results if the true association were equal to zero.

That is the issue with OVB: it causes our estimate to be lower or higher than what it should be. Therefore, OVB may lead us to conclude statistically significant results when otherwise there would not be statistically significant results in the absence of OVB; our estimate would not be sufficiently far from zero to confidently conclude a relationship.

However, if we can predict the direction of the OVB--whether it is pushing our estimate below or above what it should be--then we may be able to salvage our results. For instance, suppose we obtain a statistically significant estimate of 10 that we suspect is biased due to an omitted variable. If we can credibly claim the OVB causes our estimate to be lower than what it should be, then we still have useful results; our result would be even greater than 10 if not for OVB. Similarly, if our estimate were -10 and we suspect the OVB causes our estimate to be greater than what is should be, then we would have an even lower estimate in the absence of OVB.

The moral of this section is that when you or someone identifies a variable that may cause OVB, all is not lost. If OVB works against the estimate's value relative to zero, then it actually lowers the likelihood of significant results that you still obtained. However, if OVB works with the estimate's value relative to zero, then it increases the likelihood of finding the significant results you found when there may not be a significant relationship.

How can we postulate the direction of OVB? Figure \ref{fig:dagovbdirect} below shows a simple confounding scenario where our estimate of the effect of X on Y is biased due to an omitted variable Z. If X and Y move in the same direction because of a change in Z, then OVB is positive. If X and Y move in opposite direction because of a change if Z, then OVB is negative.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/dag_ovbdirect} 

}

\caption{Predicting direction of OVB}\label{fig:dagovbdirect}
\end{figure}

Just because you cannot claim a causal relationship does not mean you do not have useful results. It is worth reiterating the value of prediction. Regardless of whether we have a model that eliminates OVB, if it accurately predicts an outcome we would like to preempt, then it could be useful. Sure, we would like to know the underlying causes of an outcome, but the ability to accurately predict the likelihood of an outcome still allows policy or programs to intervene.

If I have a model that is biased but accurately predicts students who will drop out of college, I will use that model to provide assistance to those likely to drop out. In addition to making a difference by helping my target population, perhaps I will gain insights into the underlying causes I have failed to include in my model.

\hypertarget{kt9}{%
\section{Key terms and concepts}\label{kt9}}

\begin{itemize}
\tightlist
\item
  Internal validity
\item
  External validity
\item
  Establishing correlation
\item
  Temporal precedence
\item
  Reverse causality
\item
  DAG

  \begin{itemize}
  \tightlist
  \item
    confounder
  \item
    collider
  \item
    backdoor path
  \item
    backdoor criterion
  \end{itemize}
\item
  Omitted variable bias
\end{itemize}

\hypertarget{part-inference}{%
\part{Inference}\label{part-inference}}

\hypertarget{sampling}{%
\chapter{Sampling}\label{sampling}}

\begin{quote}
\emph{``This is what I'm learning, at 82 years old: the main thing is to be in love with the search for truth.''}

---Maya Angelou
\end{quote}

We now turn our attention to inference, which involves taking a sample from a population to make conclusions about the population with some degree of certainty. At its foundation, inference is about the search for truth. Specifically, when we calculate some estimate using a sample of data, we use inference to say whether that estimate is a good guess of the unobserved population parameter. Rather than focus on sampling techniques (e.g.~random, clustered, stratified, convenience), this chapter focuses on the theory that allows us to use samples for inference as well as the potential limitations of doing so.

\hypertarget{lo10}{%
\section{Learning objectives}\label{lo10}}

\begin{itemize}
\tightlist
\item
  Explain the 68-95-99 rule and apply it given the mean and standard deviation of a normal distribution
\item
  Explain how a sampling distribution is constructed
\item
  Explain why the Central Limit Theorem is needed to conduct inferential statistics and how it allows us to do so
\item
  Given an estimate and standard error, construct 95 and 99 percent confidence intervals
\item
  Interpret confidence intervals
\item
  Explain the effect of random or biased sampling on the accuracy and precision of a sample estimate and sampling distribution
\item
  Explain the effect of sample size on the accuracy and precision of a sample estimate and sampling distribution
\end{itemize}

\hypertarget{normal-distribution}{%
\section{Normal distribution}\label{normal-distribution}}

When we calculate an estimate, the estimate is highly unlikely to be exactly equal to the population parameter it is intended to represent. But, given a sample and an estimate, we can calculate the range in which the parameter falls with a certain degree of confidence. If that range does not include zero, then we have reason to believe the parameter is positive or negative. A non-zero parameter may be cause for action. Or, depending on the situation, a parameter of zero may be cause for action. We are able to calculate a confidence interval because of the normal distribution.

Inference relies on the normal distribution introduced in Chapter \ref{descriptive-statistics}. The normal distribution has a unique and useful quality such that wherever the mean of a variable's distribution lies, if the distribution of the variable is normal, then 68\% of the values lie within one standard deviation above and below that mean, 95\% of the values lie within two standard deviations above and below, and 99\% lie within three standard deviation. This quality of the normal distribution is sometimes called the \textbf{68-95-99 rule}, which is shown in Figure \ref{fig:normdist} below. The Greek symbol \(\mu\) (mû) denotes the mean and the symbol \(\sigma\) (sigma) denotes the standard deviation.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/normdist} 

}

\caption{68-95-99 rule of normal distribution}\label{fig:normdist}
\end{figure}

Suppose we take a sample of 397 professor salaries in order to estimate the average salary of all professors at the university. From our sample, we calculate a mean of 113706 dollars and a standard deviation of 30289 dollars. Figure \ref{fig:profsaldist} below shows the distribution of this sample of salaries.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/profsaldist-1} 

}

\caption{Distribution of professor salaries}\label{fig:profsaldist}
\end{figure}

Note that the distribution of salaries is roughly normal-looking, though it is skewed somewhat to the right. If the distribution of 397 salaries were perfectly normal, then 68\% of those salaries fall between 83417 and 143995, 95\% percent of the 397 salaries fall within 53128 and 174284, and 99\% of the 397 salaries fall within 22839 and 204573. However, since we can observe the entire distribution, we can calculate the range in which 95\% of values fall or any percentage of values. The exact 95\% range for Figure \ref{fig:profsaldist} is 70,761 to 181,511.

These ranges are merely descriptions of our sample just like the measures used in Chapter \ref{descriptive-statistics}. We have not yet made any inference about the salaries of all professors at the university.

Again, it is highly unlikely that the average salary of all professors at the university equals 113706 dollars exactly. Our estimate is a guess of something we do not directly observe. Naturally, we want to know a range in which we can be reasonably confident the average salary of all professors falls. This range is called a \textbf{confidence interval}. However, unlike the distribution of 397 salaries, we only have one estimate from our one sample of salaries. How can we calculate a confidence interval of something we do not observe? To construct a confidence interval, \textbf{we \emph{assume} the \emph{sampling} distribution of the mean of salaries is normal}.

\hypertarget{sampling-distribution}{%
\section{Sampling Distribution}\label{sampling-distribution}}

Distributions were covered in Chapter \ref{descriptive-statistics}. A variable is comprised of multiple values that can be plotted along a number line or axis to form a distribution. This distribution can be described in terms of its center via the mean and its spread via the standard deviation.

A sampling distribution is simply a distribution comprised of multiple estimates, each taken from a separate sample, instead of multiple values, each taken from a separate unit of analysis. Imagine if the distribution in Figure \ref{fig:profsaldist} were made of 397 averages from 397 samples of salaries. If we have no reason to suspect our estimates are systematically above or below the population mean (i.e.~unbiased), then we have a distribution of guesses for the population mean, the center of which should approach the population mean given enough sample estimates. Assuming this sampling distribution is normally distributed, then we can construct an interval in which 95\% of the estimates fall as a plausible range in which the unobserved population mean falls.

This tends to be a large theoretical leap for many to make. To reiterate, we have only one sample, not 397. How do we construct a 95\% confidence interval off of a sampling distribution we do not have the data to observe? We do so using theory and assumptions. Most importantly, we use the \textbf{Central Limit Theorem}.

\hypertarget{central-limit-theorem}{%
\section{Central Limit Theorem}\label{central-limit-theorem}}

The Central Limit Theorem may seem like magic more than anything else in statistics, though it is scientifically sound. Given a sufficient sample size, the Central Limit Theorem allows us to assume sampling distributions are normally distributed even though we do not have data to observe the sampling distribution. Without it, we could not construct confidence intervals. Thus, we could not make inferences about a population.

Seeing the Central Limit Theorem work is believing, especially when circumstances are set that would seem to work against it. To do this, let us revisit the distribution of a six-sided die discussed in Chapter \ref{descriptive-statistics}. With each of the six values having an equal probability of occurring, we know each value has about a 17\% chance of occurring. If we were to roll the die some number of times divisible by 6, then all values should occur the same number of times, resulting in a distribution like that depicted in Figure \ref{fig:dieuniform}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/dieuniform-1} 

}

\caption{Probability distribution of a six-sided die}\label{fig:dieuniform}
\end{figure}

The distribution in Figure \ref{fig:dieuniform} is decidedly not normal. Yet, the Central Limit Theorem states that if we took numerous samples from this distribution, each of sufficient sample size, then the sampling distribution will be normal. We typically do not know the distribution of a variable like we do with a six-sided die, thus we do not know where an important measure like the mean is in that distribution. However, if any estimate regarding any variable--no matter the distribution of the variable--is normally distributed, then we do not need to know the distribution of the variable. This is the power and importance of the Central Limit Theorem.

To see how the Central Limit Theorem works, we need a population we can observe but would not be able to in usual circumstances. Suppose we had a population comprised of 10,000 observations. Each observation is the result of rolling a six-sided die. This is obviously a play example for the sake of instruction, but one could imagine the six values of the die to be something more interesting and important, such as levels of education. Table \ref{tab:diepop} below shows a preview of our simulated population.

\begin{table}

\caption{\label{tab:diepop}Preview of simulated population from uniform distribution}
\centering
\begin{tabular}[t]{r|r}
\hline
ID & education\\
\hline
1 & 2\\
\hline
2 & 6\\
\hline
3 & 4\\
\hline
9998 & 4\\
\hline
9999 & 6\\
\hline
10000 & 4\\
\hline
\end{tabular}
\end{table}

Since we have the entire population, we can calculate the population mean, which would typically be a population parameter we cannot calculate. The mean ``education level'' of this population is 3.518. This is almost exactly equal to the mean we should expect from many rolls of a six-sided die. The distribution of the population's education is shown in Figure \ref{fig:diepophist}. The solid red line represents the population mean.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/diepophist-1} 

}

\caption{Distribution of simulated population}\label{fig:diepophist}
\end{figure}

Suppose we were to draw one random sample of 20 from this population. The distribution of this sample is shown in Figure \ref{fig:diesamp201}. The mean of this sample is 3.65, represented by the purple dashed line. The red solid line represents the population mean of 3.518.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/diesamp201-1} 

}

\caption{Distribution of sample of 20 from simulated population}\label{fig:diesamp201}
\end{figure}

The sample mean of 3.65 may or may not be a good guess of the population mean of 3.518; such judgments depend on the context of the research question or the decision needing made. Suppose we take a second random sample of 20 from the population, as shown in Figure \ref{fig:diesamp202}. The mean of this second sample is 3.95.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/diesamp202-1} 

}

\caption{Distribution of a second sample of 20 from simulated population}\label{fig:diesamp202}
\end{figure}

Note that the distribution of the two samples are different and neither are uniform like the population distribution. This is the manifestation of randomness. Each sample gives us a different estimate of the population mean, both of which are too high. Estimates of other samples would fall below the population mean. With enough samples and sample estimates of the mean, we can construct a sampling distribution.

Suppose we take 100 samples of 20 from the population, estimating the mean of each sample to construct a \emph{sampling} distribution of the mean. This sampling distribution is shown in Figure \ref{fig:diesampdist20100}. Note that with only 100 samples of only 20 observations each, the sampling distribution roughly resembles the normal distribution. Also, the mean, or center, of the sampling distribution represented by the yellow line is very close to the population mean.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/diesampdist20100-1} 

}

\caption{Sampling distribution of 100 sample means from samples of size 20}\label{fig:diesampdist20100}
\end{figure}

The centering of the sampling distribution at the population mean is due to having an unbiased estimate from random sampling. If our estimate is unbiased, then we expect it to equal the population parameter, \emph{on average}. This applies to any estimate and its potential bias, including the estimates in regression. If our regression model is unbiased, then our estimate comes from a sampling distribution that centers at the population parameter.

Now let's take 10,000 samples with a size of 33 observations each, calculating the mean of each sample. The sampling distribution of the 10,000 sample means is shown in Figure \ref{fig:diesampdist33} below. Note that it looks very similar to the normal distribution with a mean equal to 3.157. From a variable that is not normally distributed, we obtain a sampling distribution that is normal. No matter the distribution of the variable, the Central Limit Theorem assures us its sampling distribution will be normal, provided we have a large enough sample.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/diesampdist33-1} 

}

\caption{Sampling distribution of 10,000 sample means from samples of size 33}\label{fig:diesampdist33}
\end{figure}

In reality, we cannot draw 10,000 samples from the population. If we could, and calculated the mean of the sampling distribution, then we could be \emph{extremely} confident that we have estimated the population parameter with virtually perfect accuracy. Alas, we have only one sample and no sampling distribution to observe. Though we can safely assume our one estimate comes from a normal sampling distribution that, if there is no bias, is centered at the population mean, we do not know where \emph{our} sample falls within that distribution. Our one sample could be one with an estimate in or near the left or right tails of the sampling distribution in Figure \ref{fig:diesampdist33}. Therefore, we need a range of plausible values for the population parameter. For this range we need to measure the spread of the sampling distribution in terms of its standard deviation.

\hypertarget{confidence-intervals}{%
\section{Confidence intervals}\label{confidence-intervals}}

The standard error is used to construct confidence intervals. The standard error is essentially the same as the standard deviation. It is the name given to the standard deviation of a sampling distribution instead of a variable's distribution. Now that we can safely assume our sample estimate comes from a normal sampling distribution, and given that we need to account for the randomness of any one sample, we can calculate the range of values within which 95\% or 99\% of the estimates in the sampling distribution falls. In short, we have returned to applying the 68-95-99 rule, only this time we use it to construct a confidence interval.

The 95\% confidence interval for any estimate is 2 standard errors (1.96, technically) below and above the estimate. The 99\% confidence interval is about 3 standard errors below and above the estimate. Referring back to the sampling distribution in Figure \ref{fig:diesampdist33}, the standard deviation is equal to 0.26. Suppose we drew one sample that gave us an estimate of 4. Suppose the standard error of that estimate happened to equal 0.26. In that case, the 95\% confidence interval is approximately 3.48 to 4.52. Since we know the population parameter equals 3.518, we know our confidence interval captures it, which is what we hope to be the case but cannot confirm in typical circumstances.

How do we calculate the standard error to construct the 95\% confidence interval or any confidence interval without observing the sampling distribution? Again, theory and assumptions. Throughout this running example, we have been trying to estimate the mean of the population. The standard error (SE) of a sample mean is calculated using the following equation

\begin{equation}
SE = \frac{s}{\sqrt{n}}
\label{eq:semean}
\end{equation}

where \(s\) is the standard deviation of the variable in our sample data and \(n\) is the number of observations in our sample.

Equation \eqref{eq:semean} highlights one of the reasons sample size is a point of interest in analysis. In addition to needing at least 33 observations for the Central Limit Theorem to work reliably, sample size affects the precision of our confidence interval. As \(n\) increases, the denominator in Equation \eqref{eq:semean} increases. Given a standard deviation, greater denominator results in a smaller SE than a lesser denominator. That is, as our sample size increases, the range of our confidence interval decreases.

To demonstrate the effect of sample size on precision, suppose we drew 10,000 samples of size 1,000 instead of 33, as was done for the sampling distribution in Figure \ref{fig:diesampdist33}. Figure \ref{fig:diesampdist1000} depicts the sampling distribution of this hypothetical scenario. Not that the distribution is virtually identical to normal. Of most importance is the spread of the sampling distribution. The standard deviation of the sampling distribution (or standard error) in Figure \ref{fig:diesampdist33} is 0.26. The standard error of the sampling distribution in Figure \ref{fig:diesampdist1000} is equal to 0.05.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/diesampdist1000-1} 

}

\caption{Sampling distribution of 10,000 sample means from samples of size 100}\label{fig:diesampdist1000}
\end{figure}

From a sample of size 1,000 it is much less likely that we would obtain a sample estimate as far from the parameter of 3.518 as 4. Moreover, whatever our estimate, we can construct a confidence interval with the same level of confidence that will be much smaller. A more precise confidence interval may allow for more confident decision-making.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Though in reality we obtain only one estimate from one sample, we assume that estimate is one value of a normally distributed sampling distribution that is centered at the population parameter we intended to estimate. Our one sample estimate is highly unlikely to equal the population parameter because of randomness. Therefore, in addition to our specific estimate of the parameter, we construct a range of plausible values that captures that population parameter.

To walk through the full process of estimation using one sample in this simulated data example, one of the 10,000 samples of size 1,000 (sample 379) had a mean education level of 3.552. The standard deviation of education in this sample was 1.53. Given 1,000 observations, then the standard error is equal to

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{1.53}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.04838285
\end{verbatim}

Therefore, assuming a normal sampling distribution and absence of bias, the 95\% confidence interval for our estimate of the population mean of education is

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{3.552}\SpecialCharTok{{-}}\NormalTok{(}\FloatTok{1.96}\SpecialCharTok{*}\FloatTok{0.048}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.45792
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{3.552}\SpecialCharTok{+}\NormalTok{(}\FloatTok{1.96}\SpecialCharTok{*}\FloatTok{0.048}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.64608
\end{verbatim}

The 95\% confidence interval of this particular sample captures the population parameter of 3.518.

A common interpretation of, say, a 95\% confidence interval is that our population parameter has a 95\% probability of falling within our confidence interval. This is incorrect. A confidence interval either contains the population parameter or it does not. There is no 95\% probability to speak of; only 0\% or 100\%. What a confidence interval conveys is that if we were to draw numerous samples from this population rather than just the one, then we would expect 95\% of the confidence intervals constructed from all of our samples to capture the population parameter and 5\% of the confidence intervals to fail. Our sample could be one of those 5\% of samples for which the confidence intervals fail to capture the population parameter.

One out of 20 samples are expected to fail to capture the population parameter that it was intended to estimate. This is why many have concerns regarding a crisis of replication in science. If only one study is published, and replications of a study are difficult to have published, then we do not know if the one that was published is the anomaly or not.

\hypertarget{kt10}{%
\section{Key terms and concepts}\label{kt10}}

\begin{itemize}
\tightlist
\item
  Normal distribution
\item
  68-95-99 rule
\item
  Sampling distribution
\item
  Sample estimate
\item
  Central Limit Theorem
\item
  Confidence intervals
\item
  Standard error
\item
  Accuracy of sampling distribution and estimate
\item
  Precision of sampling distribution and estimate
\end{itemize}

\hypertarget{hypothesis-testing}{%
\chapter{Hypothesis Testing}\label{hypothesis-testing}}

\hypertarget{lo11}{%
\section{Learning objectives}\label{lo11}}

\begin{itemize}
\tightlist
\item
  Identify or construct the null and alternative hypotheses of a research question
\item
  Interpret a p-value and apply it to determine the outcome of a hypothesis test
\item
  Distinguish between types I and II error in a research scenario
\item
  Explain when a chi-square test or t-test is appropriate
\end{itemize}

\hypertarget{hypothesis-testing-1}{%
\section{Hypothesis testing}\label{hypothesis-testing-1}}

At its foundation, hypothesis testing is a simple procedural process. It does involve some application of statistical theory, the most fascinating and misunderstood aspect of which is the p-value. This section covers how to set up and use a hypothesis test to determine if an estimate is statistically significant.

\hypertarget{null-and-alternative-hypos}{%
\subsection{Null and alternative hypos}\label{null-and-alternative-hypos}}

Setting up a hypothesis test first involves establishing two mutually exclusive, competing statements:

\begin{itemize}
\tightlist
\item
  Null hypothesis: the condition I intend to test for is not true, not the case
\item
  Alternative hypothesis: the condition I intend to test for is true, is the case
\end{itemize}

The condition is based on our research question that warranted the analysis in the first place. For example, is the average age of MPA students less than the average age of all graduate students? Are females more likely to enroll in an MPA program than males? Does obtaining an MPA increase earnings?

Based on our question, we make a hypothesis \emph{before} computing an estimate that would answer the question. For example, the average age of MPA students is less than the average age of all graduate students. Females are more likely to enroll in an MPA program than males. The effect of attaining an MPA increases earnings. These examples are alternative hypotheses; the affirmative of the condition we set out to test. The null hypothesis is the negative of the condition. For example, average age of MPA students is equal to graduate students. The likelihood of enrolling in an MPA program are equal between males and females. An MPA degree does not increase earnings.

Note that the examples of alternative hypotheses were all directional. They used words like less/more than or increase/decrease. An alternative hypothesis need not be directional even though we may expect one direction over the other. For example, our alternative hypotheses could be that average age differs between MPA students and other grad students, the likelihood of enrolling differs between males and females, and attaining an MPA affects income.

I encourage students to stick with non-directional hypotheses. In additional to simplifying the analysis, doing so reduces the likelihood that we report a statistically significant result when in fact there is not one (i.e.~false positive). A strong case can be and is made that the statistical analyses allow too high of a likelihood for false positives. Claiming a direction means we have theoretically ruled out half of the possible results of our analysis before we even conduct the test, thereby increasing the likelihood we get the results we expected. If we are able to do this, one might wonder why conduct the test in the first place.

Translating the above into more mathematical concepts, most research questions involve differences: the difference in mean age between MPA students and other grad students, the difference in the proportions of female and male MPA graduates, the difference in the slopes of regression lines drawn through data points for income and education level or degree type.

Thus, the null hypothesis states that any of these differences equals zero. The alternative hypothesis states that any of these differences does not equal zero. The null hypothesis is denoted by \(H_0\) and the alternative hypothesis is denoted by \(H_A\).

\begin{itemize}
\tightlist
\item
  \(H_0: \mu_{MPA}-\mu_{grad} = 0\) or \(H_A: \mu_{MPA}-\mu_{grad} \neq 0\)
\item
  \(H_0: \rho_{female}-\rho_{male} = 0\) or \(H_A: \rho_{female}-\rho_{male} \neq 0\)
\item
  \(H_0: \beta_{MPA} = 0\) or \(H_A: \beta_{MPA} \neq 0\)
\end{itemize}

Note the use of population parameters above. Again, inference computes a sample estimate to make inferences about a population. Therefore, our hypotheses include the unobserved population parameter. Our estimate and confidence interval represents our best guess of that population parameter. The purpose of the hypothesis test is to establish a threshold at which we are sufficiently confident in our results to make a conclusion concerning our hypotheses \emph{prior} to viewing the results.

\hypertarget{conclusion-and-error-type}{%
\subsection{Conclusion and error type}\label{conclusion-and-error-type}}

There are two possible outcomes of a hypothesis test. We either

\begin{itemize}
\tightlist
\item
  reject the null hypothesis, or
\item
  fail to reject the null hypothesis.
\end{itemize}

We never accept the null hypothesis or reject the alternative hypothesis. This may seem like semantics, but it actually has important implications for our conclusions.

Suppose our results do not meet the threshold to reject the hypothesis, thus leading us to fail to reject the null hypothesis. This does not mean the null hypothesis is true. Our null hypothesis states whatever parameter of interest in our study equals zero. We do not observe the population parameter. Therefore, we cannot say that our null hypothesis is true. Instead, we say that we do not have sufficient evidence to reject the null. It may be true or false, but we cannot determine which based on our particular estimate from our particular sample.

Conversely, if we reject the null, then our conclusion is that the null hypothesis is false. We can rule out with reasonable confidence that the population parameter does not equal zero because doing so does not require us to claim a particular value for the population parameter; just that it is not equal to zero.

A popular example of hypothesis testing is a jury decision in a court case. A defendant is accused of committing a crime. In truth, the defendant is either innocent or guilty of that crime. Ideally, the defendant is presumed innocent until proven guilty according to a jury of their peers. Therefore, the null hypothesis is that the defendant is innocent and the alternative hypothesis is that the defendant is guilty. The jury decides either that the defendant is guilty or not guilty. If guilty, then the jury has rejected the null hypothesis of innocence. If not guilty, then the jury has failed to reject the null hypothesis. Note that the jury does not decide the defendant is innocent, which would be equivalent to accepting the null or rejecting the alternative.

Despite our best efforts, there always remains some chance that we have reached the wrong conclusion with our hypothesis test. We can make one of two possible errors:

\begin{itemize}
\tightlist
\item
  Type I error or false positive: rejecting the null when the null is actually true
\item
  Type II error or false negative: failing to reject the null when the null is actually false
\end{itemize}

For instance, Type I error is finding an innocent defendant guilty, a healthy patient sick, or an ineffective program effective. Type II error is finding a guilty defendant not guilty, no evidence of illness in a sick patient, or no evidence of efficacy in an effective program. Again, the null hypothesis involves the population parameter, so we do not \emph{know} if it is actually true or not. The null must be true or false, and the outcome of our hypothesis test claims whether the null does not appear to be false or is false. Therefore, there is a probability that we have reached the incorrect conclusion.

It is impossible to eliminate the chance of Types I and II errors, though it is possible to increase or decrease their likelihoods. However, the two share an inverse relationship; as we reduce the chance of one type of error, we increase the chance of the other type of error. Depending on the context of our research question, we may be more or less concerned about Type II error, but the focus of a hypothesis test is placed on Type I error, which serves as the threshold for our decision.

\hypertarget{decision-rule}{%
\subsection{Decision rule}\label{decision-rule}}

Before testing our hypothesis, we ask ourselves the following question: ``What is the maximum probability of Type I error that I or others should be willing to tolerate?'' Actually, this question has been answered for us in most disciplines. The common threshold for this tolerance is 5\% or 1\% probability of rejecting the null hypothesis when the null is actually true. Social sciences typically use 5\%.

With our threshold set, we can now test our hypothesis. We calculate the sample estimate and the standard error of our estimate, which are used to calculate the confidence interval. The confidence level of our confidence interval depends on our chosen threshold for Type I error. If our threshold for Type I error is 5\%, then we calculate the 95\% confidence interval. If our threshold is 1\%, we use a 99\% confidence interval.

Our confidence interval is our best guess of the plausible range of values for the population parameter. We have decided to tolerate the chance that our confidence interval is one of the five out of 100 confidence intervals--or 1 out of 100--expected to fail to capture the parameter. Our null hypothesis states that the parameter equals zero. Therefore, if our confidence interval does not contain zero, we reject the null hypothesis. If our confidence interval does contain zero, then we have failed to reject the null hypothesis because the parameter \emph{might} equal zero with a higher probability than we decided to tolerate.

In most cases, we do not need more information than the estimate, standard error, and confidence interval to make a decision regarding our hypothesis test. However, an analysis provides us an additional piece of information that allows us to arrive at the same conclusion but from a different perspective called the \textbf{p-value}.

\begin{quote}
The p-value tells us the probability of obtaining the estimate we did, or an estimate further away from the null hypothesis, if the null hypothesis were actually true.
\end{quote}

The p-value provides us a concise decision rule. The tolerance threshold we set is often referred to as the significance level and denoted by the Greek letter \(\alpha\) (alpha).

\begin{itemize}
\tightlist
\item
  If \(p<\alpha\), reject the null hypothesis.
\item
  If \(p\geq \alpha\), fail to reject the null hypothesis.
\end{itemize}

Again, a typical value for \(\alpha\) is 5\%, or 0.05. Having chosen a 5\% significance level, if our results generate a p-value of 0.04, for instance, then the likelihood of obtaining our result in a world where the null is true is 4\%. Therefore, we reject the null hypothesis. If our p-value were equal to 0.06, then the likelihood of obtaining our result in a world where the null is true is 6\%. This exceeds our maximum tolerance, thus we fail to reject the null hypothesis.

\hypertarget{chi-square-test}{%
\section{Chi-square test}\label{chi-square-test}}

The Chi-square (\(\chi^2\)) test is a common choice for introducing the application of hypothesis testing. Chi-square is used to test whether two \emph{nominal} variables are associated to a statistically significant extent. A nominal variable, such as race, sex, or political party affiliation, has two or more of levels. If one wanted to test if, given the level for a unit of analysis in one nominal variable (e.g.~male), there is a higher likelihood for a particular level in another nominal variable to occur (e.g.~Republican), a Chi-square test is an appropriate choice.

For an example, consider a poll targeted to the general U.S. public asking if workers who have illegally entered the U.S. should be 1) allowed to keep their jobs and apply for citizenship, 2) allowed to keep their jobs as temporary guest workers but not allowed to apply for citizenship, and 3) lose their jobs and have to leave the country. The poll also asked for political party affiliation. A total of 890 responses were collected, generating the following results.

\begin{table}

\caption{\label{tab:pollparty}Response by political party}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & Republican & Democrat & Independent\\
\hline
Apply for citizenship & 57 & 101 & 120\\
\hline
Guest worker & 121 & 28 & 113\\
\hline
Leave the country & 179 & 45 & 126\\
\hline
\end{tabular}
\end{table}

We see in \ref{tab:pollparty} that 179 out of 357 Republicans (50\%) responded that illegal immigrants should be forced to leave the country, while 101 out of 174 Democrats (58\%) responded that illegal immigrants should be allowed to apply for citizenship. Is there a statistically significant pattern in responses conditional on political party affiliation, or are these differences due to random noise?

The null hypothesis for this question is that there is no association between the opinion on illegal immigration and political party affiliation. That is to say, if we chose any of the three political party levels in our data, the probability of an individual providing one of the three opinions is equal the other opinions, or

\(H_0: P_{leave} = P_{guest} = P_{citizen}\)

The alternative hypothesis is that there is an association between opinion on illegal immigration and political party affiliation, or

\(H_A\): at least one \(P\) is not equal to the others

Next, suppose we chose to use the customary 5\% statistical significance level, or \(\alpha=0.05\). Now we are ready to test our hypothesis using a Chi-square test. Doing so generates the following results.

\begin{verbatim}
	Pearson's Chi-squared test

data:  immigration_poll
X-squared = 100.95, df = 4, p-value < 0.00000000000000022
\end{verbatim}

This is one case where there are no confidence intervals to compute since our variables are not numerical. Instead, we rely on the p-value. Our p-value is less than 0.00000000000000022. More concisely, \(p<0.05\). Therefore, we reject the null hypothesis that there is no association between the three illegal immigration opinions in the survey and political party affiliation. Furthermore, the probability for us to get the values in Table \ref{tab:pollparty} or more extreme in a world where the null hypothesis is actually true is equal to an infinitesimal percent, not to suggest that such a small p-value is required to make inferences.

Our results allow us to make inferences such as Republicans are more likely to believe illegal immigrants should lose their jobs and have to leave the country, while Democrats are more likely to believe they should be allowed to keep their jobs and apply for citizenship. Not a particularly surprising inference, but perhaps that is because many such inferences in the past have been made using similar techniques and reported many times.

\hypertarget{t-test}{%
\section{T-test}\label{t-test}}

The t-test is another common introductory application of hypothesis testing. A t-test is used to test the association between a nominal variable with two levels and a numerical variable. It is frequently used in simple program evaluations with a pre/post or treatment/control design. Both involve a nominal variable with two levels. If one want to test if a numerical outcome is different between the two levels, then a t-test is an appropriation choice.

There are two varieties of the t-test. To test if an average of a numerical outcome is different between two groups, such as a treatment and control group, then we use an \textbf{independent t-test}. To test if an average numerical outcome is different before and after a treatment for the \emph{same} units of analysis, we use a \textbf{dependent t-test}. The difference between the two t-tests concerns how we approximate the sampling distributions and confidence intervals, but their use for hypothesis testing is essentially the same.

Suppose we work for a nonprofit that provides job training workshops and want to evaluate their effectiveness. One way to go about such a task is to compare the earnings of participants (i.e.~treatment group) to the earnings of non-participants (i.e.~control group). We have earnings data for 185 participants and 128 non-participants, some of which is previewed in the table below.

\begin{table}

\caption{\label{tab:jobtraindata}Preview of job training data}
\centering
\begin{tabular}[t]{r|r}
\hline
treatment & earnings\\
\hline
1 & 66493.964\\
\hline
1 & 0.000\\
\hline
0 & 46651.829\\
\hline
1 & 10070.227\\
\hline
1 & 0.000\\
\hline
0 & 1211.736\\
\hline
\end{tabular}
\end{table}

Before constructing the null and alternative hypotheses, a note about the population in this example. In program evaluations or generally any analysis aimed toward testing whether some event caused an effect on an outcome of interest, there are two populations. There is the entire population of units of analysis (e.g.~all people, all nations, all dogs) and the subset of that population for whom/which the program, policy, or intervention is intended.

The choice of population leads to two slightly different questions. If the entire population is our research population, then our intent is to estimate the average effect of the program on a randomly chosen unit from the entire population. This is referred to as the \textbf{average treatment effect} (ATE). If the subset that the program targets is our research population, then our intent is to report the average effect of a randomly chosen targeted unit. This is referred to as the \textbf{average treatment on the treated} (ATT). The detailed differences between the two are beyond the scope of this book and more appropriate for a class in program evaluation or causal inference, but the existence of this difference is worth being aware of.

Given that job training programs are not intended for all people, the presumption is that we want to estimate the average effect on those the program targets. Of course, we could choose as our population only those who participated in the program. In that case, we need not bother with hypothesis tests, as we would be calculating the population parameters directly from the observed data. However, we would not be able to generalize the results.

With the population in mind, our null hypothesis is that the average earnings of participants is equal to the average earnings of non-participants, or

\(H_0: \mu_{treated} = \mu_{untreated}\)

Our alternative hypothesis is that the average earnings of participants is not equal to the average earnings of non-participants, or

\(H_A: \mu_{treated} \neq \mu_{untreated}\)

Choosing a significance level of 5\%, we are ready to test our hypothesis.

A simple computation of the average earnings between the two groups provides the following information.

\begin{table}

\caption{\label{tab:jobtrainsum}Comparison of means between treated and untreated}
\centering
\begin{tabular}[t]{r|r}
\hline
treatment & Average Earnings\\
\hline
0 & 21645.10\\
\hline
1 & 26031.49\\
\hline
\end{tabular}
\end{table}

Participants have a higher average earnings than non-participants, but is this difference statistically significant? For that, we should use an \emph{independent} t-test because participants and non-participants are two different groups. Running the t-test provides the following results

\begin{verbatim}
	Welch Two Sample t-test

data:  earnings by treatment
t = -1.1921, df = 275.58, p-value = 0.2342
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -11629.708   2856.939
sample estimates:
mean in group 0 mean in group 1 
       21645.10        26031.49 
\end{verbatim}

Our p-value is greater than our significance level, \(0.23>0.05\). Therefore, we fail to reject the null hypothesis and conclude that there is not statistically significant evidence that participants earn more than non-participants, on average. Note that our 95\% confidence interval ranges between -11,630 and 2,856. It includes zero, thus we cannot claim the difference between the means is not equal to zero with reasonable confidence.

\begin{quote}
\textbf{To learn how to conduct chi-square and t-tests in R, proceed to Chapter \ref{r-evaluations}.}
\end{quote}

\hypertarget{kt11}{%
\section{Key terms and concepts}\label{kt11}}

\begin{itemize}
\tightlist
\item
  Margin of error
\item
  Survey weight
\item
  Null and alternative hypotheses
\item
  Rejecting the null
\item
  Failing to reject the null
\item
  Types I and II error
\item
  Confidence level
\item
  Significance level
\item
  P-value
\item
  Chi-square test
\item
  T-test
\end{itemize}

\hypertarget{significance}{%
\chapter{Significance}\label{significance}}

\begin{quote}
\emph{``One out of every four people is suffering from some form of mental illness. Check three friends. If they're OK, then it's you.''}

---Rita Mae Brown
\end{quote}

We can now apply our knowledge of inference to fully understand all of our regression results and extend our results to other questions. First, this chapter explains each column in a regression table as well as how to test additional hypotheses that standard regression results do not answer by default. Then, while inference is used to identify statistical significance, that does not necessarily mean our results are \emph{practically} significant. This chapter ends with how to determine the latter.

\hypertarget{lo12}{%
\section{Learning objectives}\label{lo12}}

\begin{itemize}
\tightlist
\item
  Explain and interpret the standard components in a table of regression results
\item
  Construct the null and alternative hypotheses of a variable in a regression model
\item
  Determine the outcome of the hypothesis test based on the regression results
\item
  Explain the consequence of choosing a significance level for a hypothesis test
\item
  Distinguish between statistical and practical significance
\item
  Determine whether results are practically significant
\end{itemize}

\hypertarget{regression-table}{%
\section{Regression table}\label{regression-table}}

Chapters \ref{simple-and-multiple-regression}, \ref{categorical-variables-and-interactions}, and \ref{nonlinear-variables} presented numerous regression tables. These tables included the standard set of results that statistical programs provide by default. Below is one of the tables from Chapter \ref{categorical-variables-and-interactions} for a regression to explain traffic fatalities as a function of miles driven and U.S. region.

\begin{table}

\caption{\label{tab:extable}Parallel slopes for regions}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\hline
intercept & 0.260 & 0.167 & 1.553 & 0.121 & -0.069 & 0.589\\
\hline
vmiles & 0.188 & 0.021 & 8.895 & 0.000 & 0.147 & 0.230\\
\hline
regionN. East & -0.076 & 0.065 & -1.166 & 0.245 & -0.204 & 0.052\\
\hline
regionSouth & 0.519 & 0.056 & 9.283 & 0.000 & 0.409 & 0.630\\
\hline
regionWest & 0.641 & 0.062 & 10.264 & 0.000 & 0.518 & 0.763\\
\hline
\end{tabular}
\end{table}

We have covered how to interpret the \texttt{estimate} column at length. This value is the sample estimate of our unobserved population parameter. Provided our model is unbiased, and because of the Central Limit Theorem, we assume that the value of our estimate was drawn from a sampling distribution that is approximately normal and a mean equal to the population parameter. We assume our estimate is the mean of that sampling distribution. For example, in Table \ref{tab:extable}, it is assumed the estimate for \texttt{vmiles} of 0.188 represents the mean of an unobserved sampling distribution comprised of numerous estimates for \texttt{vmiles} that would be obtained if we repeated the regression using numerous samples.

The \texttt{std-error} column is the standard error of the regression estimate (aka coefficient) in the same row. This value is an approximation of the standard deviation of the sampling distribution from which the estimate was drawn. For example, the standard error for \texttt{vmiles} of 0.021 represents the standard deviation of its sampling distribution.

We know the true population parameter is highly unlikely to exactly equal our estimate but is expected to fall somewhere within our sampling distribution. With our estimate assumed to be the center of the normal sampling distribution and the standard error its standard deviation, we can apply the 68-95-99 rule to construct a range of values that represent a percentage of the estimates within the sampling distribution. The common choices are 95\% and 99\%, with 95\% being the default in statistical programs.

The \texttt{lower\_ci} and \texttt{upper\_ci} columns provide the 95\% confidence interval. This range represents our best guess of the plausible values for the unobserved population parameter. The population parameter either falls within our confidence interval or it does not. There is \emph{not} a 95\% probability that the parameter falls within our confidence interval. Rather, it is one range that if we were to repeat the analysis many times using different samples to construct many confidence intervals, we expect 95\% of those ranges to successfully capture the population parameter. Therefore, the population parameter is no more likely to equal our estimate as it is to equal any value within our confidence interval.

The values for the confidence interval are obtained by subtracting and adding 1.96 standard errors from the estimate. In Table \ref{tab:extable}, the 95\% confidence interval for \texttt{vmiles} is 0.147 (\(0.188-2\times 0.021\)) to 0.230 (\(0.188+2\times 0.021\)). If we were to repeat this regression 20 or 100 times, we would expect 19 or 95 of the resulting confidence intervals to capture the population parameter for \texttt{vmiles}. Understanding this chosen rate of success/failure, 0.147-0.230 is our best guess of the range of plausible values for the true response in traffic fatalities to the average number of miles driven. It is just as likely that this true population parameter equals 0.147 or 0.230 as it is to equal 0.188.

The \texttt{statistic} and \texttt{p\_value} columns concern hypothesis testing. As a reminder, the regression model that produced Table \ref{tab:extable} was

\begin{equation}
mrall = \beta_0 + \beta_1vmiles + \beta_2region + \epsilon
\label{eq:exmodel}
\end{equation}

where \texttt{mrall} is the number of traffic fatalities in a state per 10,000 population.

If the purpose of our regression is to \emph{explain} traffic fatalities, then the inclusion of \texttt{vmiles} and \texttt{region} implies a research question along the lines of ``Do distances driven and region in a state affect traffic fatalities?'' Therefore, our regression model sets out to test whether \texttt{vmiles} and \texttt{region} has a statistically significant effect on \texttt{mrall}.

The standard null hypothesis for a regression model is no effect, or

\(H_0: \beta=0\)

and the alternative hypothesis is

\(H_A: \beta \neq 0\)

for each of the explanatory variables we include in the model. As we now know, our results will lead us to either reject the null hypothesis or fail to reject the null hypothesis for each explanatory variable.

If the null hypothesis were actually true, \(\beta=0\), for any of our explanatory variables, then its sampling distribution \emph{should} be centered at 0, not centered at the value of our estimate. For example, if \(\beta_1=0\) for \texttt{vmiles}, then its sampling distribution should have a mean of 0, not 0.188. This alternative distribution if the null were true is referred to as the \textbf{null distribution}. Just like the sampling distribution, we assume the null distribution is approximately normal.

The \texttt{statistic} and \texttt{p\_value} columns answer the following question: ``If the null for my explanatory variable were true, thus my estimate having a null distribution with a mean equal to zero and a standard deviation equal to the standard error of my estimate, how likely is it that I got the estimate I got?''

Specifically, the \texttt{statistic} column equals how many standard deviations or standard errors our estimate is away from the center of the null distribution, zero. The value is equal to the \texttt{estimate} divided by the \texttt{std\_error}. For example, the estimate for \texttt{vmiles} is 8.895 standard errors away from 0 (\(\frac {0.188}{0.021}\)). Assuming the null distribution is normal, how likely is it for us to get this estimate or one further away from 0 if the null were true? This is what the \texttt{p\_value} provides us. If only 5\% of the values in a normal distribution lie 3 standard deviations from the center, then an extremely small percentage of values must lie almost 9 standard deviations from the center. This is why the p-value for \texttt{vmiles} rounds to zero.

Supposing we chose a 5\% significance level prior to running the regression, our p-value for \texttt{vmiles} is statistically significant, meaning we reject the null hypothesis that \(\beta_1=0\). In other words, there is statistically significant evidence that the average number of miles driven by each driver in a state is associated (perhaps causes) with an increase in the state's traffic fatality rate.

Since \texttt{region} is a nominal variable with four categories, it results in three estimates as if each level was a separate dummy variable equal to 1 if a state is in that region and 0 otherwise. The three regions in Table \ref{tab:extable} are compared to the excluded region, Midwest. The null hypothesis is that there is no difference between the Midwest and another region of focus. Let us first focus on states in the West. On average, the traffic fatality rate for states in the West is 0.64 higher than states in the Midwest. The p-value is below 0.05, thus we can reject the null hypothesis and report this result as statistically significant.

By contrast, the traffic fatality rate for states in the Northeast is 0.08 less than states in the Midwest. However, the p-value is greater than 0.05. Therefore, we fail to reject the null hypothesis, meaning we cannot conclude with reasonable confidence that \(\beta_{Northeast} \neq 0\).

Note that every estimate for which the p-value is less than 0.05, its confidence interval does not contain zero. These two parts of the table will always agree because they answer the same question from slightly different angles. If we choose a 95\% confidence interval as our best guess of the plausible ranges for the population parameter, and that interval does not contain 0, then we must have obtained an estimate that, if the null were true, is so far away from 0 that the likelihood of getting it is less than 5\%. Had we chosen a 99\% confidence interval, or 1\% significance level, then for any interval that does not contain 0 the corresponding p-value is less than 0.01.

\hypertarget{other-hypotheses}{%
\section{Other hypotheses}\label{other-hypotheses}}

By default, the standard regression table addresses hypothesis tests of the form \(H_0: \beta=0\) and \(H_A: \beta \neq 0\). If our null hypothesis is \(\beta\) equals something other than 0, then we need to be careful because the \texttt{statistic} and \texttt{p\_value} columns do not apply. However, the confidence interval can still be used. If the confidence interval does not contain the value used for our null hypothesis, then we can conclude with our chosen level of confidence that the population parameter does not equal that value.

Comparing different levels within a categorical variable is slightly more complicated. In Table \ref{tab:extable}, we can conclude that states in the South and West are different than states in the Midwest, and we cannot conclude states in the Northeast are different than states in the Midwest. But, what if we wanted to compare states in the South to states in the West, or Northeast to South? Our regression and our results were not set-up to make such comparisons.

A quick and dirty way to make various comparisons across levels is to examine their confidence intervals. If the confidence intervals do not overlap or do not come close to overlapping, then we can be reasonably certain the population parameters for the two levels are not equal to each other. For example, the \texttt{upper\_ci} for Northeast is 0.052 and the \texttt{lower\_ci} for South is 0.4. The two intervals are separated by multiple standard errors. Therefore, it is probably safe to conclude they are different. By contrast, the confidence intervals for South and West overlap substantially. Therefore, it is not safe to conclude that South and West are different. Alternatively, we can tell our statistical software to exclude a specific level, thereby allowing us to test our hypothesis without the guesswork.

Finally, every regression result includes a global hypothesis test of the form

\(H_0: \beta_0 = \beta_1 = \cdots = \beta_k = 0\)

\(H_A\): at least one \(\beta \neq 0\).

Keep in mind that all of our conclusions are probabilistic. There is always a chance of Type I and Type II error. Since the hypothesis test assumes a sampling distribution for each explanatory variable, each explanatory variable we add is like taking an additional sample from some underlying population relevant to our regression. At 95\% confidence, we \emph{expect} 1 out of every 20 intervals to fail at capturing the parameter, such as not including zero when the parameter is truly zero. The global hypothesis test above is a way of testing whether we got a significant result due to the random chance that 1 out of every 20 explanatory variables can be expected to be significant even if the null were true. This global hypothesis test is commonly referred to as an \textbf{F-test}. Its use and interpretation is covered in the R Chapter.

\hypertarget{practical-significance}{%
\section{Practical significance}\label{practical-significance}}

It is easy to lose sight of the forest for the trees when focusing on statistical significance. Just because we find a statistically significant relationship does not mean that relationship is practically significant or economically meaningful. Also, obtaining insignificant results does not necessarily mean you have results that are not important or worth reporting. Such distinctions between statistical and practical significance require an analyst or manager to have a broader sense of the underlying data and the context of the results.

After obtaining our results, asking ourselves three questions can help determine if our results are practically significant:

\begin{itemize}
\tightlist
\item
  What is the typical change in the explanatory variable associated with the statistically significant estimate?
\item
  Is the predicted change in the outcome due to a typical change in the explanatory variable negligible or meaningful?
\item
  If the explanatory variable is statistically significant, is its confidence interval so close to zero that using the upper or lower bound instead of the midpoint estimate would make the predicted change in the outcome negligible? If the explanatory variable is statistically insignificant, is its confidence interval so closely around zero that the entire range of plausible values of the parameter would lead to a negligible change in the outcome?
\end{itemize}

The estimate in our regression table conveys the predicted change in the outcome given a 1-unit or percent change in the explanatory variable. Referring back to Table \ref{tab:extable}, as the average number of miles driven per driver increases by one mile, traffic fatality rate increases 0.188 per 10,000. Is a one-mile increase a realistic change in the average distances driven per driver? Is one mile representative of its typical variation?

How do we get a sense of what is a typical change in the explanatory variable? The standard deviation of a variable tells us the average deviation from the variable's mean. For example, the standard deviation of \texttt{vmiles} is 1.1, so a typical change in \texttt{vmiles} is quite close to one unit. Based on the estimate for \texttt{vmiles}, a 1.1 unit change is predicted to change the traffic fatality rate by 0.21 (\(0.188 \times 1.1\)).

Next, is a predicted change of 0.21 in the traffic fatality rate negligible or meaningful? Again, we can use descriptive measures to answer this question. The mean traffic fatality rate is 2.0 and its standard deviation is 0.6. Thus, the predicted change in \texttt{mrall} from a typical change in \texttt{vmiles} is about 10\% of the mean and about one-third a standard deviation. Given the typical variation in traffic fatality rate is 0.6, is a change of 0.2 negligible or meaningful? This is where professional judgment and context plays a role, as there is no universal rule to determine what is a meaningful effect. Since the context is something as consequential as fatalities, perhaps any change is practically significant.

Lastly, since the population parameter is just as likely to equal any value in the confidence interval as it is the estimate, we should check if the lower or upper bound of the confidence interval changes our answer regarding practical significance. Since the result for \texttt{vmiles} is positive, we should focus on how close the lower bound is to zero. The lower bound for \texttt{vmiles} equals 0.147. Repeating the calculations above using the lower bound indicates that a typical change of 1.1 in \texttt{vmiles} predicts a change in \texttt{mrall} of 0.16, which is about 8\% of the mean fatality rate and one-fourth its standard deviation. Does this represent a negligible or meaningful change? Again, professional judgment and context is required.

Students of statistics are taught to focus so much on the estimate and statistical significance that they understandably get the impression that insignificance implies the results are useless. This is not necessarily the case. Once again, the confidence interval is helpful to determine whether statistically insignificant results are still practically significant.

Suppose the p-value for \texttt{vmiles} was equal to or greater than 0.05, thus leading us to fail to reject the null hypothesis. This would also mean that our 95\% confidence interval contains 0. Whether the results are still useful depends on the precision of the confidence interval around 0 relative to what we consider a meaningful change in the fatality rate given a typical change in \texttt{vmiles}. For instance, if the confidence interval ranged between -10 and 10, then our best guess for the effect ranges between substantially negative to positive or possibly no effect. This sort of imprecision is useless. However, what if the confidence interval was -0.01 to 0.01? Then, assuming a change of 0.01 in the fatality rate is negligible, we could conclude the effect of \texttt{vmiles} is negligible with a reasonable level of confidence despite failing to reject the null hypothesis.

\hypertarget{kt12}{%
\section{Key terms and concepts}\label{kt12}}

\begin{itemize}
\tightlist
\item
  Regression results

  \begin{itemize}
  \tightlist
  \item
    estimate
  \item
    standard error
  \item
    statistic or t-statistic
  \item
    p-value
  \item
    lower and upper confidence intervals
  \end{itemize}
\item
  Null distribution
\item
  Practical significance
\end{itemize}

\hypertarget{regression-diagnostics}{%
\chapter{Regression Diagnostics}\label{regression-diagnostics}}

\begin{quote}
\emph{``The hardest assumption to challenge is the one you don't even know you are making.''}

---Douglas Adams
\end{quote}

As previous chapters explained, the regression model we choose to use for explaining or predicting an outcome and the inferences we make involve several assumptions based on sound statistical theory. However, this is not to suggest that those assumptions cannot be violated. Bad choices regarding the inclusion or exclusion of explanatory variables, small sample size, and statistical oddities in our data can cause necessary assumptions to break down. If so, we may make or accept invalid conclusions.

Recall the credible analysis figure depicted below. Whether one's role is a producer or consumer of a quantitative analysis, expertise on the subject in question can make significant contributions to every level of Figure \ref{fig:credfigrepeat2}. Understanding how variables are measured helps us evaluate measurement validity and reliability. Understanding the causal pathways between variables helps us evaluate internal and external validity. Understanding inference, probabilities of error, and the context of the results can help us make valid statistical conclusions like whether we have statistical and or practical significance. Analysts and managers alike can involve themselves in this process and work together to ensure an analysis is as credible as possible.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/credible} 

}

\caption{Components of credible analysis}\label{fig:credfigrepeat2}
\end{figure}

This chapter covers some remaining assumptions and diagnostics that a credible quantitative analysis should include. While running diagnostics may primarily fall within the role of an analyst, those managing an analysis can ask good questions or identify potential issues if they at least know what else can go wrong.

\hypertarget{lo13}{%
\section{Learning objectives}\label{lo13}}

\begin{itemize}
\tightlist
\item
  Determine whether and which classical regression assumptions may be violated based on a residual versus fitted plot (RVFP)
\item
  Explain why and when multicollinearity may be a problem and propose potential solutions
\item
  Distinguish between outlier, high-leverage, and high-influence observations in regression
\item
  Identify influential observations using a residual vs.~fitted plot (RVLP)
\end{itemize}

\hypertarget{classical-assumptions}{%
\section{Classical assumptions}\label{classical-assumptions}}

The estimates we obtain from regression are the best linear unbiased estimates possible \emph{if} certain assumptions hold. If they do not, then our estimates could be biased or they could render our hypothesis tests invalid, creating a higher chance of Types I and II error than we chose that our significance level establishes. Fortunately, these assumptions can be remembered with an apt acronym: LINE.

For the assumptions of regression to hold, the relationship between the outcome and explanatory variables must be \textbf{Linear} (or modeled correctly as nonlinear), the observations must be \textbf{Independent} of each other, the data points must be \textbf{Normally} distributed around the regression line, and the data points should have \textbf{Equal} variation around the regression line. A key tool used to evaluate these assumptions is a \textbf{Residual vs.~Fitted Plot} (RVFP). An RVFP is a simple transformation of the regression line plot. Figure \ref{fig:genericreg} below shows a generic regression line fit to data with the outcome and predicted outcome on the y axis. An RVFP rotates the predicted outcome to the x axis, resulting in a horizontal line. This allows the distance between the observed and the fitted outcome to be vertical. Thus, the residuals of the regression are plotted on the y axis. Figure \ref{fig:genericrvfp} shows a generic RVFP.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/genericreg} 

}

\caption{Generic regression line through data}\label{fig:genericreg}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/genericrvfp} 

}

\caption{Generic residual vs. fitted plot}\label{fig:genericrvfp}
\end{figure}

Note that the residuals in the RVFP above appear to be randomly positioned; there is no discernible pattern in the scatter plot. No pattern in the RVFP is a visual indication that the classic regression assumptions are not violated.

Certain patterns in the RVFP signal violations of certain assumptions. For example, Figure \ref{fig:rvfplinear} below shows a clear case that the linear assumptions is violated due to age and wage sharing a quadratic relationship.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/rvfp_linear} 

}

\caption{Fitting linear model to quadratic data}\label{fig:rvfplinear}
\end{figure}

The RVFP can also be used to check whether residuals are normally distributed around the regression line and whether the residuals have equal variance. Figures \ref{fig:rvfpnormal} and \ref{fig:rvfpequal} below show examples where each is clearly violated.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/rvfp_normal} 

}

\caption{Violation of normally distributed risiduals}\label{fig:rvfpnormal}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/rvfp_equal} 

}

\caption{Violation of equal variation}\label{fig:rvfpequal}
\end{figure}

There is not a direct visual check for the assumption that observations are independent of each other. However, signs that normality or linearity have been violated could be due to violation of independence.

Independent observations is a very strong assumption. It states that the units in our data share absolutely no relationship with each other; the information pertaining to one unit has absolutely no bearing on the information gathered for another. Consider all the scenarios in which this assumption is likely violated: individuals in the same household or community, governments in the same state/province, states/provinces in the same country. Random sampling ensures independence, but random sampling is often unfeasible or not applicable to a research question.

Other than controlling for the variable(s) by which observations are related covered in Chapter \ref{causation-and-bias}, there are statistical methods to account for dependence across observations, but they are beyond the scope of this book. A competent analyst should know at least a few such methods. As with any of the above assumptions, a manager who is knowledgeable in statistics knows to ask questions regarding independence.

\hypertarget{multicollinearity}{%
\section{Multicollinearity}\label{multicollinearity}}

Multicollinearity involves whether two or more explanatory variables in our regression are strongly correlated. If the correlation between two or more explanatory variables is strong enough, it can result in Type II error (i.e.~false negative) for one or more of the variables sharing the strong correlation.

Recall that multiple regression isolates the effect of one variable on the outcome by holding all other explanatory variables constant at their mean. This requires variables to vary while holding others constant. If the values of two variables move in near perfect tandem, then regression will find it difficult to isolate the effect of one while another is held constant.

It is as if regression creates a traffic intersection with each variable having its own lane and stoplight. To investigate the isolated effect of one variable, regression turns the stoplight for that variable green and sets the stoplights for the other variables to red, letting them idle at their mean. But suppose two variables have decided not to move unless the other is allowed to move. Thus, when one gets the green light to go, it does not move, and regression estimates an effect that is less likely to be statistically significant than should be the case.

Calculating the correlation coefficient covered in Chapter \ref{descriptive-statistics} can give us a sense of whether multicollinearity may be an issue. As a general rule of thumb, if two variables have a correlation coefficient greater than 0.8 or less than -0.8, then multicollinearity could be a problem. Once a regression is run, if one or more variables that you thought should reject the null fail to do so, this could be due to multicollinearity with another explanatory variable in the model.

The solution to multicollinearity is somewhat subjective. If one variable is integral to the original purpose of your analysis, then consider dropping the other variable causing the problem. However, dropping a variable from your model should not be done lightly. The inclusion of a variable implies a theoretical claim that it affects the outcome. By dropping that variable because it is correlated with another explanatory variable, you may be introducing omitted variable bias because the dropped variable may be a confounder as discussed in Chapter \ref{causation-and-bias}. Instead, you could combine the collinear variables into a single index variable, which were discussed in Chapter \ref{data}. For instance, if the collinear variables are numerical, you calculate the average between them as a more holistic measure of the construct they both represent and include that in your regression model instead.

\hypertarget{influential-data}{%
\section{Influential Data}\label{influential-data}}

Regression is an extension of correlation, which is fundamentally based on the mean. As is any measure based on the mean, regression estimates are sensitive to extreme values in our data. Depending on our sample size, one or a few extreme values can substantially impact our regression estimates. We should be aware of influential observations and consider whether our conclusions or recommendations should differ depending on whether influential observations are included.

One must be more specific when communicating extreme values in regression, as there are three varieties:

\begin{itemize}
\tightlist
\item
  Regression Outlier: an observation with a extreme residual
\item
  High-leverage observation: an observation with an extreme value with respect to a particular variable; an outlier in the distribution of the explanatory variable
\item
  High-influence: a regression outlier with high leverage
\end{itemize}

Figure \ref{fig:influential} below provides a visual example of an influential observation in regression. Note the plot point in the far upper-right corner in the left panel. This plot point has an extreme positive residual and it imposes high positive leverage because it is positioned far from the center of the poverty distribution. As a result, this plot point pulls the slope of the regression line upward. The right panel visualizes the same regression with the influential observation removed. The regression line is noticeably flatter and fits the data better.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/influential} 

}

\caption{Regression with and without a high-influence observation}\label{fig:influential}
\end{figure}

Figure \ref{fig:influential} provides an obvious case. The primary question is how do we decide an observation is high-influence? As is the case when identifying outliers of a single distribution, there is no definitive rule for identifying high-influence observations in regression. Furthermore, whether to exclude a high-influence observation is subjective and depends on the context. Either way, influential observations should be noted in a report.

A key tool used to investigate possible high-influence observations is a residual vs.~leverage plot (RVLP). This is similar to an RVFP in that it is a simple transformation of the standard regression scatter plot that allows us to identify outliers, high-leverage, and high-influence observations more effectively. Figure \ref{fig:rvlp} below shows an RVLP for the regression of poverty and murder rates.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/rvlp} 

}

\caption{Residual vs. leverage plot}\label{fig:rvlp}
\end{figure}

The software used to produce this RVLP also adds something called Cook's distance to the plot, denoted by the red dashed line. Cook's distance is a measure commonly used to identify influential observations. One rule of thumb is that any observation with a Cook's distance greater than 1 should be investigated. Here, we see that observation 51 in our data has a Cook's distance greater than 1.

\begin{quote}
\textbf{To learn how to run regression diagnostics in R, proceed to Chapter \ref{r-regression-diagnostics}.}
\end{quote}

\hypertarget{kt13}{%
\section{Key terms and concepts}\label{kt13}}

\begin{itemize}
\tightlist
\item
  Violations of regression assumptions
\item
  Multicollinearity
\item
  Regression outlier
\item
  High-leverage observation
\item
  High-influence observation
\item
  Excluding observations from a regression model
\end{itemize}

\hypertarget{part-advanced-topics}{%
\part{Advanced Topics}\label{part-advanced-topics}}

\hypertarget{forecasting}{%
\chapter{Forecasting}\label{forecasting}}

\begin{quote}
\emph{``Forecasting is the art of saying what will happen, and then explaining why it didn't!''}

---Anonymous; Balaji Rajagopalan
\end{quote}

Previous chapters primarily used cross-sectional data to demonstrate various applications. Those applications fundamentally apply to time series and panel data as well. However, time series and panel data contain additional information, opening a vast array of additional methods that go far beyond the scope of this book.

This and the next chapter offer narrow coverage of two common, yet potentially advanced data applications in public administration: forecasting with time series data and fixed effects analysis with panel data. The intent is to provide the readers a few skills to conduct or understand basic analyses in each scenario.

\hypertarget{what-is-forecasting}{%
\section{What is forecasting}\label{what-is-forecasting}}

Recall in Chapter \ref{data} that time series measures one or more characteristics pertaining to the same subject over time. Therefore, the unit of analysis is the unit of time over which those characteristics are measured.

\begin{table}

\caption{\label{tab:timeseriesrep}Time series example}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
country & continent & year & lifeExp & pop & gdpPercap\\
\hline
United States & Americas & 1987 & 75.020 & 242803533 & 29884.35\\
\hline
United States & Americas & 1992 & 76.090 & 256894189 & 32003.93\\
\hline
United States & Americas & 1997 & 76.810 & 272911760 & 35767.43\\
\hline
United States & Americas & 2002 & 77.310 & 287675526 & 39097.10\\
\hline
United States & Americas & 2007 & 78.242 & 301139947 & 42951.65\\
\hline
\end{tabular}
\end{table}

Forecasting involves making out-of-sample predictions for a measure within a time series. Throughout the chapters on regression, we made out-of-sample predictions each time we computed the predicted value of the outcome in our regression, \(\hat{y}\), for a scenario not observed in our sample. Forecasting is no different in this regard. It is specific to predictions with time series data. Since the unit of analysis in time series data is a unit of time, an out-of-sample prediction involves a time period unobserved in our sample (i.e.~the future).

Analyses can seek to predict, to explain, or both. Keep in mind that forecasting is typically focused on prediction rather than explanation. Would it be helpful to know why an outcome is the value that it is in most cases? Certainly, but good decisions can be made by knowing what to expect regardless of why. Moreover, the benefits of modeling a valid explanatory model may not exceed the costs of delaying accurate predictions.

If the focus is solely prediction, then we do not need to concern ourselves with internal validity or omitted variable bias. Frankly, we do not care if our model makes theoretical sense as long as its predictions are accurate. While this frees us from many constraints, it makes goodness-of-fit even more important. Therefore, the primary focus of this chapter is how to identify a good forecast model and how to choose the best model among multiple good models.

Lastly, keep in mind that a forecasting also relies on confidence intervals. Whereas explanatory regression places focus on the confidence intervals around the estimated effect of an explanatory variable on an outcome, forecasts focus on the confidence intervals around the predicted value of the outcome. These confidence intervals convey the range of values that our forecast model expects the future outcome to fall within some percentage of simulated futures.

\hypertarget{patterns}{%
\section{Patterns}\label{patterns}}

We rely on patterns to make good forecasts. A time series that exhibits no patterns offers no information for predicting the future. Time series can exhibit the following three types of patterns:

\begin{itemize}
\tightlist
\item
  Trend: a long-term increase or decrease
\item
  Seasonal: a repeated pattern according to a calendar interval usually shorter than a year
\item
  Cyclic: irregular increases or decreases over unfixed periods of multiple years
\end{itemize}

With a time series of U.S. GDP in Figure \ref{fig:usgdp}, we can see two of the aforementioned patterns. First, there is an obvious upward trend. Secondly, there appear to be irregularly spaced plateaus or dips, most of which represent economic recessions. Recessions exhibit a cyclical pattern. Phenomena related to weather or holidays, such as energy production, consumption, and travel, are likely to exhibit seasonal patterns like the sales data shown in Figure \ref{fig:sales} below.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/usgdp-1} 

}

\caption{U.S. GDP 1975-2019}\label{fig:usgdp}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/sales-1} 

}

\caption{Sales data}\label{fig:sales}
\end{figure}

\hypertarget{autocorrelation}{%
\subsection{Autocorrelation}\label{autocorrelation}}

Again, it is useful for forecasts if a time series exhibits a pattern. Another way to think of a pattern is that past values provide some information for predicting future values.

Whereas correlation measures the linear association between two variables, autocorrelation measures the linear association between an outcome and past values of that outcome. We can use an autocorrelation plot to examine if past values appear to predict future values.

Figure \ref{fig:acfgdp} below is an autocorrelation plot of U.S. GDP. For all measurements along the time series of GDP, the autocorrelation plot quantifies the correlation between a chosen ``current'' GDP and past measurements of GDP called lags. Figure \ref{fig:acfgdp} goes as far as 22 lagged measures. The blue dashed line denotes the threshold at which the correlations are statistically significant at the 95\% confidence level.

We can see that the first lag of GDP is almost perfectly correlated with current GDP. In other words, last quarter's GDP is a very strong predictor of current GDP. The strength of the correlation decreases over time but remains statistically significant. This gradual decrease in autocorrelation is indicative of time series with a trend pattern.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/acfgdp-1} 

}

\caption{Autocorrelation of U.S. GDP}\label{fig:acfgdp}
\end{figure}

Figure \ref{fig:acfgdp} below shows the autocorrelation from the quarterly sales time series that exhibited a seasonal pattern. The autocorrelation plot suggests that each even-numbered lag is correlated with the current sales measure, switching between negative and positive each time. This peak and valley pattern is common in seasonal data.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/acfsales-1} 

}

\caption{Autocorrelation of sales}\label{fig:acfsales}
\end{figure}

In each of the examples above, we can use information from the past to predict the future. A time series that shows no autocorrelation is called \textbf{white noise}. White noise provides us no significant information about predicting the future. Figures \ref{fig:wn} and \ref{fig:acfwn} below provide an example of white noise. Note there is no discernible pattern in the time series plot and no autocorrelations are statistically significant.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/wn-1} 

}

\caption{White noise time series}\label{fig:wn}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/acfwn-1} 

}

\caption{Autocorrelation of white noise}\label{fig:acfwn}
\end{figure}

\hypertarget{forecasting-basics}{%
\section{Forecasting basics}\label{forecasting-basics}}

Forecasts use past observed data to predict future unobserved data. If time series exhibits a pattern such that autocorrelation is present, we can use the past to improve predictions of the future.

\hypertarget{evaluation}{%
\subsection{Evaluation}\label{evaluation}}

The central goal of a forecast is to provide the most accurate prediction. How can we evaluate the accuracy of our predictions if the future events have not occurred? As was the case in previous chapters on regression, a forecast essentially draws a line through data. We can get a sense of how accurate our forecast model is by comparing its predictions to observed values. That is, we can use the residuals of a forecast model to evaluate its goodness-of-fit. A better fitting model is expected to generate more accurate predictions, on average.

\hypertarget{residuals}{%
\subsubsection*{Residuals}\label{residuals}}
\addcontentsline{toc}{subsubsection}{Residuals}

Figure \ref{fig:lagtime} shows a forecast model denoted by the red line that simply uses the previous GDP measure to predict current GDP, compared to observed GDP denoted by the blue line. Recall how strongly lagged GDP was correlated with current GDP. This results in a forecast that appears to fit the trend fairly well. Nevertheless, there is error for almost every year, and since GDP in this time window exhibits a consistent upward trend, using last year's GDP causes a consistent underestimation.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/lagtime-1} 

}

\caption{Comparing observed to predicted}\label{fig:lagtime}
\end{figure}

Figure \ref{fig:checkresidual} below plots the residuals between observed and predicted GDP--the vertical distance between blue and red lines--in the top panel. The bottom-left panel is a autocorrelation plot for the residuals--computing the correlation between current residuals and lagged residuals--and the bottom-right panel shows the histogram of the residuals.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/checkresidual-1} 

}

\caption{Residual diagnostics}\label{fig:checkresidual}
\end{figure}

Figure \ref{fig:checkresidual} provides a lot of useful information related to the central goal of forecasting. In order for us to conclude we have a good forecast, two goals must be met:

\begin{itemize}
\tightlist
\item
  The time series of residuals should be white noise, and
\item
  the residuals should have a mean approximately equal to zero.
\end{itemize}

It is difficult to tell from the top panel of Figure \ref{fig:checkresidual} whether these goals are met. However, notice that the residuals are almost always positive, which we would expect since we know our forecast almost always underestimates GDP. Therefore, the mean is certainly greater than zero, as can be seen in the histogram.

The autocorrelation plot of the residuals suggests that residuals lagged up to six time periods is significantly correlated with current residuals. This is further evidence that the time series of our residuals is not white noise.

A good forecast extracts as much information from past data as possible to predict the future. If it fails to do so, then lagged residuals will be correlated with current residuals. Therefore, our simple forecast for GDP has not extracted all the information from the past that could inform future predictions, resulting in a sub-par forecast.

\hypertarget{root-mean-squared-error}{%
\subsubsection*{Root Mean Squared Error}\label{root-mean-squared-error}}
\addcontentsline{toc}{subsubsection}{Root Mean Squared Error}

Multiple models could achieve residuals that are white noise and have a mean equal to zero. We can further evaluate forecast models by comparing their root mean squared errors (RMSE). Recall from Chapter \ref{simple-and-multiple-regression} that the RMSE quantifies the typical deviation of the observed data points from the regression line and is analogous to the standard deviation or standard error measures. In fact, the 95\% confidence interval around a forecast is based on two RMSEs above and below the point forecast, just as two standard errors are used to construct a 95\% confidence interval around a point estimate in regression.

Table \ref{tab:frmse} shows a set of standard goodness-of-fit measures for our simple forecast of GDP. We will only concern ourselves with RMSE. According to the results, the point forecast of our model is off by plus-or-minus 137 billion dollars, on average. If we developed a model with a smaller RMSE, we would prefer it to this model, provided its residuals behave no worse.

\begin{table}

\caption{\label{tab:frmse}Forecast goodness-of-fit measures}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r}
\hline
  & ME & RMSE & MAE & MPE & MAPE & MASE & ACF1\\
\hline
Training set & 110.1394 & 137.2821 & 118.1553 & 1.421887 & 1.475215 & 0.2562912 & 0.4933519\\
\hline
\end{tabular}
\end{table}

\hypertarget{models}{%
\subsection{Models}\label{models}}

There are four basic forecasting models:

\begin{itemize}
\tightlist
\item
  Mean: future outcomes predicted to equal the average of the outcome over the entire time series
\item
  Naive: future outcomes predicted to equal the last observed outcome
\item
  Drift: draws a straight line connecting the first and last observed outcome and extrapolates it into the future
\item
  Seasonal naive: same as naive but predicts each future season to equal its last observed season
\end{itemize}

Figures \ref{fig:formean}, \ref{fig:fornaive}, and \ref{fig:fordrift} below demonstrate the mean, naive, and drift forecast models applied to U.S. GDP, respectively. It should be obvious that using the mean is a poor choice and will be for any time series with a strong trend pattern. Under normal circumstances absent of an impending economic shutdown, we would likely conclude that the drift model provides a more accurate forecast than the naive model.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/formean-1} 

}

\caption{Mean Forecast}\label{fig:formean}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/fornaive-1} 

}

\caption{Naive Forecast}\label{fig:fornaive}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/fordrift-1} 

}

\caption{Drift Forecast}\label{fig:fordrift}
\end{figure}

According to the drift model, predicted GDP for the next ten time periods is shown in Table \ref{tab:gdpdrift}. Again, this is not a sophisticated model, and some may be alarmed by making predictions based on simply connecting the first and last observations, then extending the line into the future. It is important to keep in mind that the utility of a forecast is not the exact point forecasts in Table \ref{tab:gdpdrift}. In fact, it would be misleading to report GDP in Q2 of 2020 is predicted to be 21.65 trillion dollars. The utility of a forecast is the corresponding confidence interval. If this is our best model, then we can report that GDP in Q2 of 2020 is predicted to be between 21.48 and 21.81 trillion dollars with 95\% confidence.

\begin{table}

\caption{\label{tab:gdpdrift}Forecast values}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
  & Point Forecast & Lo 80 & Hi 80 & Lo 95 & Hi 95\\
\hline
2020 Q2 & 21645.05 & 21539.73 & 21750.36 & 21483.98 & 21806.11\\
\hline
2020 Q3 & 21755.19 & 21605.84 & 21904.53 & 21526.78 & 21983.59\\
\hline
2020 Q4 & 21865.33 & 21681.91 & 22048.74 & 21584.82 & 22145.83\\
\hline
2021 Q1 & 21975.46 & 21763.10 & 22187.83 & 21650.68 & 22300.25\\
\hline
2021 Q2 & 22085.60 & 21847.53 & 22323.68 & 21721.50 & 22449.71\\
\hline
2021 Q3 & 22195.74 & 21934.24 & 22457.25 & 21795.81 & 22595.68\\
\hline
2021 Q4 & 22305.88 & 22022.67 & 22589.10 & 21872.74 & 22739.02\\
\hline
2022 Q1 & 22416.02 & 22112.44 & 22719.60 & 21951.74 & 22880.30\\
\hline
2022 Q2 & 22526.16 & 22203.31 & 22849.01 & 22032.41 & 23019.91\\
\hline
2022 Q3 & 22636.30 & 22295.09 & 22977.51 & 22114.46 & 23158.14\\
\hline
\end{tabular}
\end{table}

Perhaps more sophisticated methods would provide a better forecast model. If so, then the model will fit observed data better, resulting in more precise confidence intervals. Greater precision could indeed be valuable depending on the context, as many decisions can be aided by considering best- and worst-case scenarios. Nevertheless, as long as our model achieves residuals that look like white noise with a mean approximately equal to zero, we can be fairly confident that our model is not wildly inaccurate though it may be less precise than an alternative model.

Let us check the residuals for our drift model. As can be seen in Figure \ref{fig:gdpdriftresid}, the mean of the residuals is approximately zero, but it appears that there is still information in past measures not extracted by our simple drift model. These results suggest we should try to improve our model.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/gdpdriftresid-1} 

}

\caption{GDP drift residuals}\label{fig:gdpdriftresid}
\end{figure}

The figures below compare mean, naive, and seasonal naive models using the seasonal sales data from earlier. Because this time series does not exhibit a clear trend, the mean model is not as obviously bad as it was with GDP, though it is highly imprecise. The same applies to the naive model. If we care about predicting specific seasons (i.e.~quarters), then clearly the seasonal naive model is the preferred choice.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/fcompare2-1} 

}

\caption{Comparison of forecast models to seasonal data}\label{fig:fcompare2-1}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/fcompare2-2} 

}

\caption{Comparison of forecast models to seasonal data}\label{fig:fcompare2-2}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/fcompare2-3} 

}

\caption{Comparison of forecast models to seasonal data}\label{fig:fcompare2-3}
\end{figure}

Let us check the residuals of the seasonal naive model. The residuals have a mean of zero, and with the exception of one significantly correlated residual for lag 4, it appears we have mostly white noise. This model may be sufficient in many cases. The fact that sales from a year ago still provide information for current sales suggests there may be an annual trend component to this time series that our seasonal naive model does not extract. Therefore, a better model is achievable.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/salesresid-1} 

}

\caption{Residual check}\label{fig:salesresid}
\end{figure}

\hypertarget{recap}{%
\section{Recap}\label{recap}}

We have only scratched the surface of forecasting. The corresponding R Chapter covers how to implement the models and plots above as well as incorporating explanatory variables into a forecast model.

Here are the key takeaways from this chapter:

\begin{itemize}
\tightlist
\item
  Prediction does not care about the theory of a model.
\item
  Patterns in time series contain information that can be used to predict the future.
\item
  A good forecast model extracts all useful information from the past to predict the future. If this is achieved, the residuals from our forecast will look like white noise and have a mean equal to zero.
\item
  The best model among competing good models is the model with the smallest RMSE.
\end{itemize}

\hypertarget{panel-analysis}{%
\chapter{Panel Analysis}\label{panel-analysis}}

\begin{quote}
\emph{``The more things change, the more they are the same.''}

---Jean-Baptiste Alphonse Karr
\end{quote}

\hypertarget{panel-data}{%
\section{Panel data}\label{panel-data}}

Recall from Chapter \ref{data} that panel data measures the \emph{same} units over multiple time periods. Table \ref{tab:panelrep} below provides an example of panel data. Panels for geographic or political areas such as counties, school or voting districts, states, and countries are common and easy to obtain. Due to privacy protections and challenges of following people over time, panels of individual people are somewhat more difficult to obtain but are quite common.

\begin{table}

\caption{\label{tab:panelrep}Panel example}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
country & continent & year & lifeExp & pop & gdpPercap\\
\hline
Argentina & Americas & 1997 & 73.275 & 36203463 & 10967.282\\
\hline
Argentina & Americas & 2002 & 74.340 & 38331121 & 8797.641\\
\hline
Argentina & Americas & 2007 & 75.320 & 40301927 & 12779.380\\
\hline
Bolivia & Americas & 1997 & 62.050 & 7693188 & 3326.143\\
\hline
Bolivia & Americas & 2002 & 63.883 & 8445134 & 3413.263\\
\hline
Bolivia & Americas & 2007 & 65.554 & 9119152 & 3822.137\\
\hline
\end{tabular}
\end{table}

Cross-sectional data is like having a single picture for each unit among multiple units. We use variation across those units with respect to some variable (e.g.~income, unemployment rates) to explain or predict outcomes of interest. Time series is like having a video of one subject, following that subject over multiple time periods. We use variation over time to explain or predict outcomes of interest for that subject. Panel data is like having videos for multiple subjects. Therefore, we have variation across units \emph{and} over time to use for explaining or predicting outcomes of interest.

\hypertarget{fixed-effects}{%
\section{Fixed effects}\label{fixed-effects}}

The additional information contained within panel data affords us a wide array of new analytic techniques that go far beyond the scope of this book. There is one technique or model, however, that is probably the most common and very easy to use: the fixed effects model.

Recall the standard multiple regression model shown below. This model is slightly different from what you have seen before because it uses \emph{indexing} to indicate that we have panel data. This indexing is done with the \emph{i} and \emph{t} subscripts, which represent subject and time, respectively. It is simply used to convey that we have multiple subjects \emph{i} over multiple time periods \emph{t}.

\begin{equation}
y_{it}=\beta_0+\beta_1x_{1it}+\beta_2x_{2it}+\cdots+\beta_kx_{kit}+\epsilon_{it}
\label{eq:multregpan}
\end{equation}

A fixed effects model is a slightly modified version of Equation \eqref{eq:multregpan} that represents an important conceptual leap. Recall that the \(\epsilon_{it}\) term represents all the factors that are associated with or affect the outcome \(y_{it}\) that we cannot include in our model for various reasons. This error term is inevitable and not a problem as long as there are no factors that also affect any of our explanatory variables. Otherwise, we have omitted variable bias in our model and may not be able to use our results.

In most cases, someone can probably think of an omitted variable that is related to one or more of the explanatory variables. In other words, it is really difficult to convincingly guard against claims of omitted variable bias. However, having panel data allows us to guard against an important source of potential OVB by using a fixed effects model. Using a fixed effects model allows us to control for all of the omitted factors that do not change over time.

The fixed effects model is represented in Equation \eqref{eq:femod} below. Note the new term (the Greek letter alpha) immediately to the right of the equal sign has replaced the usual y-intercept, \(\beta_0\), term. Also, note the index for this new term only includes \emph{i}. Because our data contains a time series for each subject \emph{i}, we can model a unique y-intercept for each subject. The unique y-intercepts represent all of the stuff that makes the subjects inherently different from each other and do not change over time, or at do not meaningfully change over the time span of our data.

\begin{equation}
y_{it}=\alpha_{i}+\beta_1x_{1it}+\beta_2x_{2it}+\cdots+\beta_kx_{kit}+\epsilon_{it}
\label{eq:femod}
\end{equation}

The fixed effect model is essentially identical to controlling for a categorical variable like we saw in Chapter \ref{categorical-variables-and-interactions}. Recall the graph below where we controlled for the region each state is in when modeling the relationship between miles driven and traffic fatalities. We controlled for region not because we thought being in the West literally causes drivers to have more fatal accidents, but rather because regions might capture unobserved geographic or infrastructure characteristics that affect traffic fatalities.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/pslopescatter3rep-1} 

}

\caption{Parallel slopes for 4 groups}\label{fig:pslopescatter3rep}
\end{figure}

The fixed effect model takes this idea a little further, controlling for the unobserved characteristics of each unit--the state in this case--rather than some aggregated level like region. This produces a separate regression line for each unit, as seen in Figure \ref{fig:feviz}. For unobserved reasons, states seem to have inherent differences with respect to miles driven and fatalities. Note how flat the common regression slope is for all of the states compared to the slope of the regression without fixed effects in Figure \ref{fig:olsviz}. Ignoring the inherent differences between states leads to the conclusion that distance driven has a larger effect on the fatality rate than what the data actually suggests once we control for those differences. This is the primary reason for using a fixed effects model.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/feviz-1} 

}

\caption{Visualizing fixed effects}\label{fig:feviz}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/olsviz-1} 

}

\caption{Ignoring fixed effects}\label{fig:olsviz}
\end{figure}

There is one trade-off of using fixed effects that casual users should be aware of: using a fixed effect absorbs all constant variables. Variables that tend not to change over time such as race, sex, geography, membership to some higher-level unit (e.g.~employee within an agency or union) all collapse into the fixed effect. This means that we will not obtain an estimate for these variables in a fixed effects model because they are also fixed. If we really care about getting an estimate from a time-invariant variable, then we cannot use a fixed effects model.

For example, recall the regression results we obtained in Chapter \ref{categorical-variables-and-interactions} for the following parallel slopes model.

\begin{equation}
mrall = \beta_0 + \beta_1vmiles + \beta_2region + \epsilon
\label{eq:pslopeexamp2rep}
\end{equation}

\begin{table}

\caption{\label{tab:psloperesults2rep}Parallel slopes for regions}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\hline
intercept & 0.260 & 0.167 & 1.553 & 0.121 & -0.069 & 0.589\\
\hline
vmiles & 0.188 & 0.021 & 8.895 & 0.000 & 0.147 & 0.230\\
\hline
regionN. East & -0.076 & 0.065 & -1.166 & 0.245 & -0.204 & 0.052\\
\hline
regionSouth & 0.519 & 0.056 & 9.283 & 0.000 & 0.409 & 0.630\\
\hline
regionWest & 0.641 & 0.062 & 10.264 & 0.000 & 0.518 & 0.763\\
\hline
\end{tabular}
\end{table}

We got estimates for three of the regions, providing us average differences in the traffic fatality rate relative to the fourth excluded region.

Running a fixed effects model for Equation \eqref{eq:pslopeexamp2rep} generates the following results. Note the absence of a single intercept because there is a separate intercept for each state that typically is not included in a table. More importantly, there is no estimate for the regions because a state's region is absorbed by the state's fixed effect.

We are left with an estimate for the only variable in our model that differs across time within each state: \texttt{vmiles}. The estimate is statistically significant at the standard 5\% significance level and is substantially less than the estimate in the model ignoring fixed effects. Here, as the average miles driven per driver \emph{within} a state increases by 1 mile, the fatality rate increases by 0.057 deaths, all else equal.

\begin{table}

\caption{\label{tab:unnamed-chunk-62}Fixed effects results}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
term & estimate & std.error & statistic & p.value\\
\hline
vmiles & 0.057 & 0.019 & 2.956 & 0.003\\
\hline
\end{tabular}
\end{table}

\hypertarget{part-r-chapters}{%
\part{R Chapters}\label{part-r-chapters}}

\hypertarget{r-chapter-introduction}{%
\chapter{R Chapter Introduction}\label{r-chapter-introduction}}

This section contains what are referred to as R Chapters, each of which corresponds to a chapter in the previous section. Chapters in the previous section focus on concepts that are applicable regardless of statistical software. R Chapters present those concepts in ways to practically apply them via a short series of exercises using R.

Each R Chapter begins with a list of learning objectives followed by a what you need to set up in terms of packages and data to complete the chapter. Each chapter then guides you through a few exercises that require you to operate R. Periodically, they will ask you to interpret your results and/or connect what you have done to the concept it was meant to help you understand. By the end of each chapter, you will have at least one document to save and upload to eLC. That document will contain code and answers to questions.

Once you upload your R Chapter work to eLC, a document will become available that contains my answers to those same exercises. This is meant to provide almost immediate feedback. You should compare your work to my own, making note of any differences and attempting to make sense of them. Keep in mind that my answers are not necessarily definitive. R Chapters will be incorporated into class discussion when possible, but feel free to ask specific questions about each R Chapter during class.

\hypertarget{what-is-r-and-rstudio}{%
\section{What is R and RStudio}\label{what-is-r-and-rstudio}}

R is a programming language for statistical computing. RStudio is a user interface for R. These two programs are analogous to a smart phone. Your phone has base code you never interact with directly but is what allows your phone to work. Similarly, you should never have to launch and interact with R on your computer. Instead, you interact with this code, doing all the cool things it allows you to do through what you see on the screen. RStudio is like the screen of your phone.

\hypertarget{installing-r-and-rstudio}{%
\section{Installing R and RStudio}\label{installing-r-and-rstudio}}

First, download and install R \href{https://cloud.r-project.org/}{here}.

\begin{itemize}
\tightlist
\item
  \textbf{Windows user:} click on ``Download R for Windows'', then click on ``base'', then click on ``Download R \#.\#.\# for Windows.''
\item
  \textbf{MacOS user:}, click on ``Download R for (Mac) OS X.'' What you click on next depends on what version of macOS you are using. Under ``Latest release,'' you will see a link such as ``R-\#.\#.\#.pkg'' with a description to the right that indicates which versions of macOS it is compatible with, such as macOS 10.13 (High Sierra) and higher. If you are using an older version of macOS, scroll down to the header ``Binaries for legacy OS X systems'' where you can find the link that will work with your version. If you do not know which version of macOS you are using, click on the apple symbol in the top-left of your screen, then click on ``About This Mac.'' The resulting window will display your version of macOS.
\end{itemize}

Second, download and install RStudio \href{https://www.rstudio.com/products/rstudio/download/}{here}.

\begin{itemize}
\tightlist
\item
  Click on the download link beneath the ``RStudio Desktop'' version that is ``FREE.''
\item
  The website should automatically provide a link under step 2 to download the version of RStudio recommended for your computer.
\end{itemize}

\hypertarget{rstudio-orientation}{%
\section{RStudio orientation}\label{rstudio-orientation}}

\begin{learncheck}
\textbf{Exercise 1} Launch RStudio
\end{learncheck}

Upon launch, you should see three sections regerred to as panes:

\begin{center}\includegraphics[width=\textwidth]{images/rstudio_sshot} \end{center}

\begin{itemize}
\tightlist
\item
  Console pane (left) is where you can tell R what to do. It also displays the results of commands and any messages, warnings, and errors. You will rarely need to use the console except when installing a package. \textbf{Only install a package via the console, never as code you would save and (re)run.}
\item
  Environment pane (top right) displays all the data in your current R session. A session is the time between launching and closing R.
\item
  Files pane (bottom right) allows you to navigate your files, displays plots, provides a list of installed packages, allows you to search for help, and displays file exports.
\end{itemize}

You will usually see a fourth pane in the upper-left, with the console in the bottom-left, while working in RStudio -- the source editor pane.

\begin{learncheck}
\textbf{Exercise 2} Two options: 1) See that plus sign (+) icon with a
piece of paper in the very top left of RStudio? Click on that and you
will see a list of options. In this course, we will always use R
Markdown documents. Select R Markdown. 2) In the RStudio menu bar at the
top of your screen, go to
\texttt{File\ -\textgreater{}\ New\ File\ -\textgreater{}\ R\ Markdown}.
Either way, a dialog box will appear. You do not need to do anything
other than click \texttt{OK}. A new pane should appear with some default
content in it. \textbf{This is the pane where you will tell R what to do
99\% of the time because it allows you to write code that you can save.}
\end{learncheck}

\hypertarget{r-markdown}{%
\subsection{R Markdown}\label{r-markdown}}

An R Markdown document allows you to fluidly combine prose that you would write for a report and R code that produces the tables and graphs you wish to incorporate. It can do much more than this, as outlined by this brief \href{https://rmarkdown.rstudio.com/lesson-1.html}{video}. In this course, we will use R Markdown for R Labs and Problem Sets in addition to these R Chapters.

An R Markdown file consists of two parts:
- YAML Header: at the top contained within three dashes, \texttt{-\/-\/-}. The YAML sets the global options for the document and how it exports; typically used to adjust the formatting of the export document
- Body: where you write prose and code

Be sure to read the default content, as it is informative.

\begin{learncheck}
\textbf{Exercise 3} In the YAML, change the title to ``R Chapter 1''.
\end{learncheck}

\begin{learncheck}
\textbf{Exercise 4} Click on the \texttt{Knit} button at the top of the
source editor pane (looks like a ball of yarn with a needle stuck in it.
A drop down menu will appear. Click on HTML. RStudio will then prompt
you to save your document. Save your R Markdown file using your last
name (\texttt{lastname\_rch16}) wherever on your computer you prefer. R
will start to process your document and a new window will appear that
contains your export document. Feel free to test knitting to Word if
your computer has Word installed. PDF will not yet work. RStudio saves
your document every time you knit.
\end{learncheck}

\hypertarget{r-packages}{%
\subsection{R Packages}\label{r-packages}}

Many tasks in R require you to install R packages that augment its functionality. Extending the smartphone analogy from before, your phone comes with base programs (e.g.~calendar, weather), but others develop third-party applications to augment the functionality of your phone. This is also the case with R, which has an active community that develops useful third-party apps called packages.

\hypertarget{install-packages}{%
\subsubsection*{Install Packages}\label{install-packages}}
\addcontentsline{toc}{subsubsection}{Install Packages}

Just like your phone, you have to first install a R package to use it. In your phone's case, you go to your app store and download an application that will then show up on your screen.

Installing packages via RStudio is essentially the same. To install R packages, we type the following generic code \textbf{into the console pane} (bottom-left).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"name\_of\_package"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Where \texttt{"name\_of\_package"} is replaced with the actual name of the R package we want to install. This code accesses R's ``app store'' and downloads it to your computer. You only need to install a R package once. The package is saved on your computer where R can find it.

\begin{quote}
Remember to include quotes around the name of the R package when installing it.
\end{quote}

\begin{learncheck}
\textbf{Exercise 5} We will almost always need to use a package called
\texttt{tidyverse}. In your \textbf{console pane} (bottom-left) of
RStudio, type \texttt{install.packages("tidyverse")}, then click
\texttt{Enter} on your keyboard. This will begin the installation.
Monitor the console pane while \texttt{tidyverse} installs. RStudio may
ask you some Yes/No questions during the process. Answer all questions
in the affirmative by typing \texttt{Yes} then clicking \texttt{Enter}.
\end{learncheck}

\hypertarget{load-packages}{%
\subsubsection*{Load Packages}\label{load-packages}}
\addcontentsline{toc}{subsubsection}{Load Packages}

When an application is installed on your phone, you still have to launch it (i.e.~click on it) to use it. The same goes for using a R package. Each time you launch RStudio, you need to load the package(s) that contain the functions you plan to use. Closing RStudio is like shutting off your phone. When you open your phone, an application isn't running in the background unless you previously launched it.

Therefore, you should load needed R packages each time you open RStudio. And because you want your code to work each time you or someone else tries to run it, you should include this in your saved code. This means you should load R packages in the source editor pane (top-left) whether it be an R script of R markdown document. The following generic code is used to load a package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(name\_of\_package)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
You need not use quotes around the name of the R package when loading it. Quotes are only needed to install, not load.
\end{quote}

\begin{learncheck}
\textbf{Exercise 6} Near the top of your document, you should see a code
chunk named \texttt{setup} containing the code
\texttt{knitr::opts\_chunk\$set(echo\ =\ TRUE)}. Start a new line
\emph{inside} of this code chunk. Type \texttt{library(tidyverse)} on
the new script. Next, run this code chunk either by clicking on the
little sideways arrow at the right of the code chunk, or by using the
keyboard shortcut \texttt{Cmd+Enter} on Mac or \texttt{Ctrl+Enter} or
\texttt{Window+Enter} on PC. This shortcut only executes the line of
code on which your cursor (the blinking vertical line) is. The console
pane (bottom-left) will provide information about the loading.
\end{learncheck}

\begin{quote}
Remember, you must load a package before using any functions included in that package or else you will receive the following error message, \texttt{Error:\ could\ not\ find\ function}. I will tell you what packages are needed to complete all assignments, but remember that you need to install and load the package to use it.
\end{quote}

\hypertarget{save-and-upload}{%
\section{Save and Upload}\label{save-and-upload}}

Save your document once more either by knitting again or like any other software -- clicking on the floppy disk icon in the menu at the top or using the menu bar \texttt{File\ -\textgreater{}\ Save\ As}. Then upload your .Rmd document to eLC. Once you upload, answers will become available on the Welcome module of eLC.

\hypertarget{additional-resources}{%
\section{Additional Resources}\label{additional-resources}}

There are many resources that provide orientations to R. Below are a two that I consider particularly helpful and accessible.

\begin{itemize}
\tightlist
\item
  Chapters 1-3 of \href{https://rbasics.netlify.app/index.html}{Getting Used to R, RStudio, and R Markdown}
\item
  \href{https://rladiessydney.org/courses/ryouwithme/01-basicbasics-0/}{BasicBasics of RYouWithMe} by R-Ladies Sydney
\end{itemize}

\hypertarget{r-data}{%
\chapter{R Data}\label{r-data}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Examine datasets to determine their dimensions, unit of analysis, and structure
\item
  Examine variables to determine their type
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Start a R Markdown document. Keep the YAML and delete the default content in the body.
\item
  Load the following packages. This requires you to start a code chunk \texttt{Cmd+Option+I} or \texttt{Ctrl+Option+I} on PC. Or use the Insert menu in the top right of the source editor pane.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(carData)  }\CommentTok{\#if this fails, you need to install}
\end{Highlighting}
\end{Shaded}

Before you begin, go to the \texttt{Packages} tab in the bottom-right pane. Find \texttt{carData} in the list and click on the name. This should take you to the \texttt{Help} tab, which will contain the documentation for \texttt{carData}. This page serves as a directory to all of the datasets that come loaded with the package. We will be examining some of these datasets. If you want or need to learn more about a particular dataset, you can click on its name in this list.

\hypertarget{viewing-datasets}{%
\section{Viewing datasets}\label{viewing-datasets}}

Perhaps one of the first obstacles to using R is that you do not constantly stare at a spreadsheet, creating somewhat of a disconnect between what you do to data and seeing it done.

You can examine a dataset in spreadsheet form using the following command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{View}\NormalTok{(dataset)}
\end{Highlighting}
\end{Shaded}

where you replace \texttt{dataset} with the actual name of the dataset.

\begin{learncheck}
\textbf{Exercise 1:} In your document, start a new code chunk to use the
\texttt{View} command on the \texttt{Arrests} dataset that should be
available in your current R session.
\end{learncheck}

A new tab should have appeared in the top-left pane containing the spreadsheet of \texttt{Arrests}. This dataset contains information on arrests for possession of small amounts of marijuana in the city of Toronto.

\begin{learncheck}
\textbf{Exercise 2} Based on what you see in the spreadsheet of
\texttt{Arrests}, what is the structure of this dataset? What is the
unit of analysis? Remember to write your answer outside of a code chunk.
\end{learncheck}

Let's examine a new dataset called \texttt{Florida} which contains county voting results for the 2000 presidential election.

\begin{learncheck}
\textbf{Exercise 3} What is the structure and unit of analysis of the
\texttt{Florida} dataset?
\end{learncheck}

\begin{learncheck}
\textbf{Exercise 4} Do the same thing one more time with the
\texttt{USPop} dataset. What is its structure and unit of analysis?
\end{learncheck}

\hypertarget{warning-about-view}{%
\subsection{Warning about View}\label{warning-about-view}}

\begin{quote}
\texttt{View} is useful but \textbf{should not} be included in a document that you plan to export. This is because R will attempt to print the entire dataset to the export document. This is almost always a mistake.
\end{quote}

To avoid making this mistake, I suggest you not use the \texttt{View} function as code, but rather use a point-and-click alternative.

\begin{learncheck}
\textbf{Exercise 5} In an existing or new code chunk, run the following
code, which saves \texttt{Arrests} to your environment pane in the
top-right. Then click on \texttt{arrests}. This should do the same thing
as running \texttt{View}. If you find yourself needing to see the
spreadsheet, use this point-and-click method.
\end{learncheck}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arrests }\OtherTok{\textless{}{-}}\NormalTok{ Arrests}
\end{Highlighting}
\end{Shaded}

\hypertarget{glimpse-data}{%
\section{Glimpse Data}\label{glimpse-data}}

If your dataset is moderately large, \texttt{View} is an inefficient way to get a sense of your data. The \texttt{glimpse} function generates a compact printout providing key information about a dataset. The general syntax is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(dataset)}
\end{Highlighting}
\end{Shaded}

where we replace \texttt{dataset} with the name of the \texttt{dataset}.

\begin{learncheck}
\textbf{Exercise 6} In an existing or new code chunk, use
\texttt{glimpse} on \texttt{Arrests}.
\end{learncheck}

Notice that the results show you the \textbf{dimensions} of the dataset--the number of rows (observation) and columns (variables). Next, it provides a vertical list of variables, with several of their values listed horizontally. This allows a very wide dataset with many variables to export to documents more easily.

\begin{learncheck}
\textbf{Exercise 7} Having now examined \texttt{Arrests} using
\texttt{View} and \texttt{glimpse}, what type are the following
variables based on the taxonomy used in Chapter \ref{data}.

\begin{itemize}
\tightlist
\item
  year
\item
  age
\item
  sex
\end{itemize}
\end{learncheck}

\hypertarget{variable-types-in-r}{%
\section{Variable Types in R}\label{variable-types-in-r}}

The column immediately to the right of the variable name in the \texttt{glimpse} printout is also informative. It tells you how each variable is stored in R. A variable can be stored in several ways:

\begin{itemize}
\tightlist
\item
  \textbf{Integers:} commonly used for discrete variables
\item
  \textbf{Doubles/numerics:} commonly used for continuous variabls but can also store discrete variables
\item
  \textbf{Logicals:} commonly used for categorical variables that are binary (i.e.~1 or 0). In R, logicals are assigned \texttt{TRUE}, if equal to 1, or \texttt{FALSE}, if equal to 0.
\item
  \textbf{Factors:} commonly used for categorical variables. Factors can store categorical variables with any number of levels. Therefore, a binary variable can be stored as a factor instead of a logical if you want the variable to be assigned different values like ``Yes'' or ``No.''
\item
  \textbf{Characters:} commonly used for strings of text that don't fit the other storage types well, such as open-ended responses in a survey. However, any variable can be stored as a character. A numerical variable can be stored as a character and R will not recognize its values as numbers.
\end{itemize}

\begin{learncheck}
\textbf{Exercise 8} Are the variables in exercise 7 stored in a way that
makes sense given your answers?
\end{learncheck}

Variables will not always be stored in R the way they should. Sometimes we have to tell R how to store a variable based on our own understanding of their type. This skill will be covered later.

\hypertarget{save-and-upload}{%
\section{Save and Upload}\label{save-and-upload}}

Change the title in the YAML to ``R Chapter 2''. Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download.

\hypertarget{r-missing-data}{%
\chapter{R Missing Data}\label{r-missing-data}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Determine if a dataset has missing values
\item
  Determine which variables in a dataset have missing values and how many values are missing
\item
  Run functions on variables that have missing values
\item
  Replace all missing values with a non-missing value, such as 0, if doing so is advisable
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\item
  Start a R Markdown file, keeping the YAML and deleting the default content
\item
  Change the YAML to the following:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}{-}{-}}
\NormalTok{title}\SpecialCharTok{:} \StringTok{\textquotesingle{}R Chapter 3\textquotesingle{}}
\NormalTok{author}\SpecialCharTok{:} \StringTok{\textquotesingle{}Your name\textquotesingle{}}
\NormalTok{output}\SpecialCharTok{:}\NormalTok{ html\_document}
\SpecialCharTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(carData)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data}{%
\section{Data}\label{data}}

We will use the \texttt{SLID} data from the \texttt{carData} package to learn how to deal with missing data. Per its documentation,

\begin{quote}
``The SLID data frame has 7425 rows and 5 columns. The data are from the 1994 wave of the Canadian Survey of Labour and Income Dynamics, for the province of Ontario. There are missing data, particularly for wages.''
\end{quote}

As is always the case when we begin working with new data, we want to get a sense of what it contains.

\begin{learncheck}
\textbf{Exercise 1:} Use \texttt{glimpse} to examine \texttt{SLID}.
\end{learncheck}

This is a moderately large dataset with 7,425 observations. Obviously, it would be terribly inefficient to look for missing values manually by scrolling through a spreadsheet of this size. We can already see from the glimpse results that wages has missing values given the \texttt{NA}s.

\hypertarget{checking-for-missing-data}{%
\section{Checking for missing data}\label{checking-for-missing-data}}

If it is not immediately obvious that a dataset contains missing values, we can tell R to check if an entire dataset has \emph{any} missing data using the following function

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anyNA}\NormalTok{(dataset)}
\end{Highlighting}
\end{Shaded}

where we replace \texttt{dataset} with the name of the dataset. If the dataset has at least one missing value, then \texttt{anyNA} will return \texttt{TRUE}.

\begin{learncheck}
\textbf{Exercise 2:} Use \texttt{anyNA} to confirm \texttt{SLID} has
missing values.
\end{learncheck}

The \texttt{anyNA} hasn't told us anything we didn't already know given the obvious \texttt{NA}s present in wages. Next, we may want to know which variables have missing values.

To determine which variables have missing values, we want to run \texttt{anyNA} repeatedly for each variable in our dataset. To run any function repeatedly on each row or column of a dataset, we can use the following function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apply}\NormalTok{(dataset, }\DecValTok{1}\NormalTok{ (}\ControlFlowTok{for}\NormalTok{ rows, or) }\DecValTok{2}\NormalTok{ (}\ControlFlowTok{for}\NormalTok{ columns), }\ControlFlowTok{function}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

where we replace \texttt{dataset} with the name of our dataset, include either \texttt{1} or \texttt{2}, and replace \texttt{function} with the name of the function we want to repeat.

\begin{learncheck}
\textbf{Exercise 3:} Use \texttt{apply} to run the \texttt{anyNA}
function repeatedly on each column.
\end{learncheck}

Your results should tell you that wages, education, and language contain missing values.

\hypertarget{counting-missing-values}{%
\section{Counting missing values}\label{counting-missing-values}}

Once we know a variable has missing values, we typically want to know how many values are missing or what percentage of total observations are missing for that variable.

The \texttt{is.na} function tests every value of a variable for whether it is missing. If a value is NA, \texttt{is.na} returns \texttt{TRUE}. To illustrate, the below code assigns a series of ten values to \texttt{v}, five of which are missing. This \texttt{v} object is no different from a variable in a dataset. Then, using the \texttt{is.na} function on \texttt{v} will return a list of TRUE/FALSE values accordingly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{v }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{5}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{11}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}
\FunctionTok{is.na}\NormalTok{(v)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1]  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE
\end{verbatim}

Recall in Chapter \ref{r-data} that the logical value of \texttt{TRUE} equals 1 in R, while \texttt{FALSE} equals 0. This means we can do math on TRUE/FALSE values just like we would if they were coded as 1/0.

If \texttt{is.na} gives us \texttt{TRUE} for every \texttt{NA}, then adding all the \texttt{TRUE}s will give us the total count of missing values.

To sum all the values of any variable, we can use the \texttt{sum} function

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(v))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5
\end{verbatim}

The result tells us 5 of the 10 values in \texttt{v} are missing. We can easily determine that 50\% of the data for \texttt{v} is missing. But what if we have some denominator that is not as easy as 10? We can quickly to determine the percent of missing values by taking the average of \texttt{TRUE}s and \texttt{FALSE}s from the \texttt{is.na} function because the average sums the values of the variable and divides by the number of values.

We take the average of the \texttt{is.na} function using the \texttt{mean} function. Since each TRUE value is equal to 1, adding up all the TRUEs will equal 5, which is then divided by the total number of values, 10, giving us 0.5 or 50\%.

\begin{quote}
Whenever we have a dummy 1/0 variable, the average of that variable is the percentage of observations equal to 1. In this case, 1 represents missing, but it could represent anything.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(v))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5
\end{verbatim}

As expected, we get 0.5 or 50\%. Building from this example, we can quantify the total and percent of missing values for wages like so

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(SLID}\SpecialCharTok{$}\NormalTok{wages))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3278
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(SLID}\SpecialCharTok{$}\NormalTok{wages))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.4414815
\end{verbatim}

Wages is missing 3,278 observations, or about 44\% of all observations.

\begin{learncheck}
\textbf{Exercise 4:} Use the \texttt{sum} and \texttt{mean} function on
\texttt{is.na} to determine the count and percent of missing values for
the \texttt{education} and \texttt{language} variables.
\end{learncheck}

If we had, say, 10 variables with missing values, the process above would be rather tedious. Like before, we can tell R to repeatedly quantify missing values for each variable using a slightly different function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sapply}\NormalTok{(SLID, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    wages education       age       sex  language 
     3278       249         0         0       121 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sapply}\NormalTok{(SLID, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{mean}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     wages  education        age        sex   language 
0.44148148 0.03353535 0.00000000 0.00000000 0.01629630 
\end{verbatim}

\hypertarget{bypassing-missing-values}{%
\section{Bypassing missing values}\label{bypassing-missing-values}}

Many functions that execute some kind of computation (e.g.~sum, average) do not work if you execute them on variables that contain missing values. This is deliberate so users are notified of missing values.

For instance, below I try to calculate average years of education.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(SLID}\SpecialCharTok{$}\NormalTok{education)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] NA
\end{verbatim}

In order to have functions bypass missing values, we have to include the \texttt{na.rm=TRUE} option that tells R to skip \texttt{NA}s.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(SLID}\SpecialCharTok{$}\NormalTok{education, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 12.49608
\end{verbatim}

Since education is missing only 3\% of its values, this is probably a good approximation of what the average would be if there were no missing values.

\begin{learncheck}
\textbf{Exercise 5:} Compute the average for \texttt{wages}.
\end{learncheck}

It is unclear what to do with average wages since almost half of its values are missing. At the very least, we can report something like, ``Only 56\% of respondents reported a wage. Of those who reported a wage, the average equals \$15.55.''

\hypertarget{drop-all-missing-cases}{%
\section{Drop all missing cases}\label{drop-all-missing-cases}}

First, we should always be careful when dropping data, as it could change our analysis and mislead a reader. Always ask yourself why a variable might be missing values and whether it matters to the conclusions you plan to make from the values that are not missing. If you do choose to remove observations that have missing values, always be transparent by stating how many observations from the total were removed due to missing data.

Suppose we want to remove all observations from SLID with a missing value for any variable. That is, we want to purge SLID of all missing values, perhaps so we don't have to keep including \texttt{na.rm=TRUE} in all of our functions.

To remove all missing values, we can use the \texttt{na.omit} function like so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_nomissing }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(dataset)}
\end{Highlighting}
\end{Shaded}

where we create a new dataset indicating we've replaced the missing values (we don't want to overwrite the original data). Inside the \texttt{na.omit} function, we include the name of the \texttt{dataset}.

\begin{learncheck}
\textbf{Exercise 6:} Create a new dataset \texttt{SLID\_nomissing} that
removes all missing values. Then calculate the average education on this
new dataset \emph{without} including \texttt{na.rm=TRUE}. Is the average
education the same as in SLID?
\end{learncheck}

\hypertarget{save-and-upload}{%
\section{Save and Upload}\label{save-and-upload}}

Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download.

\hypertarget{r-description}{%
\chapter{R Description}\label{r-description}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Calculate descriptive statistics individually
\item
  Automate a professional-quality table of descriptive statistics
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Start a R Markdown document
\item
  Change the YAML to the following:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}{-}{-}}
\NormalTok{title}\SpecialCharTok{:} \StringTok{\textquotesingle{}R Chapter 4\textquotesingle{}}
\NormalTok{author}\SpecialCharTok{:} \StringTok{\textquotesingle{}Your name\textquotesingle{}}
\NormalTok{output}\SpecialCharTok{:} 
\NormalTok{  html\_document}\SpecialCharTok{:}
\NormalTok{    theme}\SpecialCharTok{:}\NormalTok{ spacelab}
\NormalTok{    df\_print}\SpecialCharTok{:}\NormalTok{ paged}
\SpecialCharTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(arsenal)}
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{library}\NormalTok{(carData)}
\end{Highlighting}
\end{Shaded}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Summary statistics tables are ubiquitous in reports and studies. Usually a project involves numerous variables that would require too many visualizations, though we should still consider visualizations for the most important variables. A standard table of summary stats provides readers a reference for key measures pertaining to all our variables in a fairly compact form.

In this chapter, we set out to summarize variables within the \texttt{States} dataset of the \texttt{carData} package.

\begin{learncheck}
\textbf{Exercise 1:} Use the \texttt{glimpse} function to examine the
\texttt{States} dataset.
\end{learncheck}

\texttt{States} is a cross-section of the 50 states and D.C. containing education and related statistics. Be sure to skim the documentation for \texttt{States} to understand each variable. You can do that by clicking on the \texttt{carData} package under the Packages tab then clicking on the \texttt{States} link.

\hypertarget{individual-stats}{%
\section{Individual Stats}\label{individual-stats}}

Before producing a table of descriptive statistics, it is helpful to review how one would tell R to compute each statistic covered in Chapter \ref{descriptive-statistics} individually.

\begin{quote}
Because R can hold many datasets/objects at once, we need to tell it which dataset/object to apply a given function. We have had to do this many times already. Similarly, if we want R to apply a function to a \emph{specific variable} within a dataset, we need to tell which variable in which dataset. This is done using the \texttt{\$} operator.
\end{quote}

Below are all of the useful descriptive measures of center and spread applied to the \texttt{pay} (i.e.~average teacher's salary in 1,000s) variable in the \texttt{States} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(States}\SpecialCharTok{$}\NormalTok{pay)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 30.94118
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{(States}\SpecialCharTok{$}\NormalTok{pay)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 30
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(States}\SpecialCharTok{$}\NormalTok{pay)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5.308151
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{IQR}\NormalTok{(States}\SpecialCharTok{$}\NormalTok{pay)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{range}\NormalTok{(States}\SpecialCharTok{$}\NormalTok{pay)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 22 43
\end{verbatim}

\begin{learncheck}
\textbf{Exercise 2:} Calculate the average and standard deviation for
state spending on public education in \$1000s per student.
\end{learncheck}

\hypertarget{summary-table}{%
\section{Summary Table}\label{summary-table}}

Summary tables come in many styles, so there is no way to cover everything. In most cases, a summary table includes the following descriptive measures depending on the type of variable:

\begin{itemize}
\tightlist
\item
  Numerical variables

  \begin{itemize}
  \tightlist
  \item
    Mean
  \item
    Standard deviation
  \item
    Minimum
  \item
    Maximum
  \end{itemize}
\item
  Categorical variables

  \begin{itemize}
  \tightlist
  \item
    Counts for each level, and/or
  \item
    Percentages for each level
  \end{itemize}
\end{itemize}

If a variable is skewed, then it may be wise to replace the mean and standard deviation with the median and IQR. We will learn how to do this.

\hypertarget{background-table}{%
\subsection{Background Table}\label{background-table}}

Sometimes we do not want to print a fancy table. Rather, we may want to quickly see a full set of descriptive statistics for ourselves that our reader will never see. This can be done using the \texttt{summary} function on our dataset like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(gapminder)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        country        continent        year         lifeExp     
 Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  
 Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  
 Algeria    :  12   Asia    :396   Median :1980   Median :60.71  
 Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  
 Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  
 Australia  :  12                  Max.   :2007   Max.   :82.60  
 (Other)    :1632                                                
      pop               gdpPercap       
 Min.   :     60011   Min.   :   241.2  
 1st Qu.:   2793664   1st Qu.:  1202.1  
 Median :   7023596   Median :  3531.8  
 Mean   :  29601212   Mean   :  7215.3  
 3rd Qu.:  19585222   3rd Qu.:  9325.5  
 Max.   :1318683096   Max.   :113523.1  
                                        
\end{verbatim}

We would almost certainly want to suppress this code and output (i.e.~\texttt{include=FALSE} code chunk option) if preparing a report for an external audience.

\begin{learncheck}
\textbf{Exercise 3:} Generate a background table of descriptive
statistics for all of the variables in \texttt{States}. Suppress the
code and output.
\end{learncheck}

\hypertarget{using-arsenal}{%
\subsection{Using Arsenal}\label{using-arsenal}}

Due to the many styles of summary tables, there are numerous R packages designed to produce summary tables. The best R package in terms of quickly getting the information to a nicely formatted table of which I am aware is Arsenal. Therefore, we will learn how to use Arsenal. I will demonstrate Arsenal using the \texttt{gapminder} dataset. Then, I will ask you to replicate those demonstrations using the \texttt{States} data.

Producing a summary table with Arsenal involves at least two, probably three, steps.

\begin{itemize}
\tightlist
\item
  Create a new object containing the summary statistics we want to include in a table
\item
  Relabel the variables to something appropriate for our audience
\item
  Generating the summary table based on the new object we just created
\end{itemize}

Here is an example following the steps above using \texttt{gapminder} data without altering any of Aresenal's default options that we will want to know how to alter in some cases.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.gapminder }\OtherTok{\textless{}{-}} \FunctionTok{tableby}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ continent }\SpecialCharTok{+}\NormalTok{ gdpPercap }\SpecialCharTok{+}\NormalTok{ lifeExp }\SpecialCharTok{+}\NormalTok{ pop, }\AttributeTok{data =}\NormalTok{ gapminder)}
\end{Highlighting}
\end{Shaded}

The above code is what actually creates the table I want to export. First, I name the object whatever I want. Then I use the \texttt{tableby} function. We will learn what the tilde, \texttt{\textasciitilde{}}, does later. For now, know that it is required. Then, I list the variable I want included in the table, separating each with a plus sign. Lastly, I tell R which dataset to apply this function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{labels}\NormalTok{(sum.gapminder) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{continent =} \StringTok{"Continent"}\NormalTok{, }\AttributeTok{gdpPercap =} \StringTok{"GDP Per Capita"}\NormalTok{, }\AttributeTok{lifeExp =} \StringTok{"Life Expectancy"}\NormalTok{, }\AttributeTok{pop =} \StringTok{"Population"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Most datasets do not use variable names that would be appropriate for an external audience. The names in \texttt{gapminder} are not bad; most readers could make sense of what the names imply about the data, but it is simple enough (though tedious) to provide a more polished look.

Therefore, in the above code I use the \texttt{labels} function on the \texttt{sum.gapminder} table I just created, then assign each variable I told R to include in the table a label that will replace the name when it prints.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(sum.gapminder, }\AttributeTok{title =} \StringTok{"Summary Stats for Gapminder Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lc@{}}
\caption{Summary Stats for Gapminder Data}\tabularnewline
\toprule
& Overall (N=1704)\tabularnewline
\midrule
\endfirsthead
\toprule
& Overall (N=1704)\tabularnewline
\midrule
\endhead
\textbf{Continent} &\tabularnewline
~~~Africa & 624 (36.6\%)\tabularnewline
~~~Americas & 300 (17.6\%)\tabularnewline
~~~Asia & 396 (23.2\%)\tabularnewline
~~~Europe & 360 (21.1\%)\tabularnewline
~~~Oceania & 24 (1.4\%)\tabularnewline
\textbf{GDP Per Capita} &\tabularnewline
~~~Mean (SD) & 7215.327 (9857.455)\tabularnewline
~~~Range & 241.166 - 113523.133\tabularnewline
\textbf{Life Expectancy} &\tabularnewline
~~~Mean (SD) & 59.474 (12.917)\tabularnewline
~~~Range & 23.599 - 82.603\tabularnewline
\textbf{Population} &\tabularnewline
~~~Mean (SD) & 29601212.325 (106157896.744)\tabularnewline
~~~Range & 60011.000 - 1318683096.000\tabularnewline
\bottomrule
\end{longtable}

This last line of code actually prints the summary table when I knit my document. The previous two steps can be included in the same code chunk, but \textbf{this function needs to have its own code chunk because}

\begin{quote}
you need to include a specific code chunk option, \texttt{results=\textquotesingle{}asis\textquotesingle{}}, in order for the table to export properly. To be clear, in the top line of a code chunk that contains \texttt{\{r\}} by default, you need to change it to \texttt{\{r,\ results=\textquotesingle{}asis\textquotesingle{},\ echo=FALSE\}}. I also include the \texttt{echo=FALSE} option assuming we do not want our reader to see our code.
\end{quote}

\begin{learncheck}
\textbf{Exercise 4:} Replicate the code shown above to create a summary
table for the \texttt{States} data using the Arsenal package. Be sure to
relabel the variables to something relatively understandable and brief.
Labeling is tedious but you only need to do it once. I suggest you knit
your document now to see what you just made.
\end{learncheck}

In three relatively short bits of code, we already have a decent summary table that would have taken excruciatingly long to input manually. But it can be made better.

\hypertarget{adjustments-to-arsenal}{%
\subsection{Adjustments to Arsenal}\label{adjustments-to-arsenal}}

\hypertarget{decimal-digits}{%
\subsubsection{Decimal digits}\label{decimal-digits}}

The biggest aesthetic issue with my table is that it includes so many decimals. None of these variables have such a small range that rounding to integers masks useful information. Obviously, if a variable only ranges between 0 and 1, we would not want to round to an integer.

Specifying the number of decimals is quite easy with Arsenal. Because arsenal tries to be as flexible as possible, we have to specify the number of decimals separately for numerical and percentage measures. The following code sets the number of decimals to zero for the gapminder data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.gapminder2 }\OtherTok{\textless{}{-}} \FunctionTok{tableby}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ continent }\SpecialCharTok{+}\NormalTok{ gdpPercap }\SpecialCharTok{+}\NormalTok{ lifeExp }\SpecialCharTok{+}\NormalTok{ pop, }\AttributeTok{data =}\NormalTok{ gapminder, }\AttributeTok{digits =} \DecValTok{0}\NormalTok{, }\AttributeTok{digits.pct =} \DecValTok{0}\NormalTok{)}

\FunctionTok{labels}\NormalTok{(sum.gapminder2) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{continent =} \StringTok{"Continent"}\NormalTok{, }\AttributeTok{gdpPercap =} \StringTok{"GDP Per Capita"}\NormalTok{, }\AttributeTok{lifeExp =} \StringTok{"Life Expectancy"}\NormalTok{, }\AttributeTok{pop =} \StringTok{"Population"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(sum.gapminder2, }\AttributeTok{title =} \StringTok{"Summary Stats for Gapminder Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lc@{}}
\caption{Summary Stats for Gapminder Data}\tabularnewline
\toprule
& Overall (N=1704)\tabularnewline
\midrule
\endfirsthead
\toprule
& Overall (N=1704)\tabularnewline
\midrule
\endhead
\textbf{Continent} &\tabularnewline
~~~Africa & 624 (37\%)\tabularnewline
~~~Americas & 300 (18\%)\tabularnewline
~~~Asia & 396 (23\%)\tabularnewline
~~~Europe & 360 (21\%)\tabularnewline
~~~Oceania & 24 (1\%)\tabularnewline
\textbf{GDP Per Capita} &\tabularnewline
~~~Mean (SD) & 7215 (9857)\tabularnewline
~~~Range & 241 - 113523\tabularnewline
\textbf{Life Expectancy} &\tabularnewline
~~~Mean (SD) & 59 (13)\tabularnewline
~~~Range & 24 - 83\tabularnewline
\textbf{Population} &\tabularnewline
~~~Mean (SD) & 29601212 (106157897)\tabularnewline
~~~Range & 60011 - 1318683096\tabularnewline
\bottomrule
\end{longtable}

\begin{learncheck}
\textbf{Exercise 5:} Replicate the code shown above to create a second
summary table for the \texttt{States} data with no decimals. Note that
you can simply copy-and-paste the labels code.
\end{learncheck}

\hypertarget{reporting-median-and-iqr}{%
\subsubsection{Reporting median and IQR}\label{reporting-median-and-iqr}}

Instead of the mean and standard deviation, we may want to report the median, first quartile, and third quartile for our numerical variables. We can control the descriptive measures using the following code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.gapminder3 }\OtherTok{\textless{}{-}} \FunctionTok{tableby}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ continent }\SpecialCharTok{+}\NormalTok{ gdpPercap }\SpecialCharTok{+}\NormalTok{ lifeExp }\SpecialCharTok{+}\NormalTok{ pop, }\AttributeTok{data =}\NormalTok{ gapminder, }\AttributeTok{digits =} \DecValTok{0}\NormalTok{, }\AttributeTok{digits.pct =} \DecValTok{0}\NormalTok{, }\AttributeTok{numeric.stats =} \FunctionTok{c}\NormalTok{(}\StringTok{"median"}\NormalTok{, }\StringTok{"q1q3"}\NormalTok{, }\StringTok{"range"}\NormalTok{))}

\FunctionTok{labels}\NormalTok{(sum.gapminder3) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{continent =} \StringTok{"Continent"}\NormalTok{, }\AttributeTok{gdpPercap =} \StringTok{"GDP Per Capita"}\NormalTok{, }\AttributeTok{lifeExp =} \StringTok{"Life Expectancy"}\NormalTok{, }\AttributeTok{pop =} \StringTok{"Population"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(sum.gapminder3, }\AttributeTok{title =} \StringTok{"Summary Stats for Gapminder Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lc@{}}
\caption{Summary Stats for Gapminder Data}\tabularnewline
\toprule
& Overall (N=1704)\tabularnewline
\midrule
\endfirsthead
\toprule
& Overall (N=1704)\tabularnewline
\midrule
\endhead
\textbf{Continent} &\tabularnewline
~~~Africa & 624 (37\%)\tabularnewline
~~~Americas & 300 (18\%)\tabularnewline
~~~Asia & 396 (23\%)\tabularnewline
~~~Europe & 360 (21\%)\tabularnewline
~~~Oceania & 24 (1\%)\tabularnewline
\textbf{GDP Per Capita} &\tabularnewline
~~~Median & 3532\tabularnewline
~~~Q1, Q3 & 1202, 9325\tabularnewline
~~~Range & 241 - 113523\tabularnewline
\textbf{Life Expectancy} &\tabularnewline
~~~Median & 61\tabularnewline
~~~Q1, Q3 & 48, 71\tabularnewline
~~~Range & 24 - 83\tabularnewline
\textbf{Population} &\tabularnewline
~~~Median & 7023596\tabularnewline
~~~Q1, Q3 & 2793664, 19585222\tabularnewline
~~~Range & 60011 - 1318683096\tabularnewline
\bottomrule
\end{longtable}

\begin{learncheck}
\textbf{Exercise 6:} Replicate the code shown above to create a summary
table for the \texttt{States} data that reports median and the first and
third quartiles.
\end{learncheck}

\hypertarget{across-groups}{%
\subsubsection{Across groups}\label{across-groups}}

Finally, instead of reporting summary statistics for the entire sample, we may want to report them separately for each level of a categorical variable. This is a common way to make comparisons.

We can have Arsenal report across groups by adding the categorical variable to the left side of the formula in the \texttt{tableby} code. The code below reports the \texttt{gapminder} data across continents. Note that the tilde \texttt{\textasciitilde{}} is used to separate grouping variables on the left side from the variables we wish to summarize on the right side.

By default, Arsenal tests for correlations across groups and reports a p-value. This is not a common part of a summary table (at least for fields in which I am familiar), so I turn this feature off with the \texttt{test\ =\ FALSE} within the code below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.gapminder4 }\OtherTok{\textless{}{-}} \FunctionTok{tableby}\NormalTok{(continent }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gdpPercap }\SpecialCharTok{+}\NormalTok{ lifeExp }\SpecialCharTok{+}\NormalTok{ pop, }\AttributeTok{data =}\NormalTok{ gapminder, }\AttributeTok{digits =} \DecValTok{0}\NormalTok{, }\AttributeTok{digits.pct =} \DecValTok{0}\NormalTok{, }\AttributeTok{test =} \ConstantTok{FALSE}\NormalTok{)}

\FunctionTok{labels}\NormalTok{(sum.gapminder4) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{continent =} \StringTok{"Continent"}\NormalTok{, }\AttributeTok{gdpPercap =} \StringTok{"GDP Per Capita"}\NormalTok{, }\AttributeTok{lifeExp =} \StringTok{"Life Expectancy"}\NormalTok{, }\AttributeTok{pop =} \StringTok{"Population"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(sum.gapminder4, }\AttributeTok{title =} \StringTok{"Summary Stats for Gapminder Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lcccccc@{}}
\caption{Summary Stats for Gapminder Data}\tabularnewline
\toprule
\begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.18}}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
Africa (N=624)\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
Americas (N=300)\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
Asia (N=396)\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
Europe (N=360)\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
Oceania (N=24)\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
Total (N=1704)\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.18}}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
Africa (N=624)\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
Americas (N=300)\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
Asia (N=396)\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
Europe (N=360)\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
Oceania (N=24)\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
Total (N=1704)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.18}}\raggedright
\textbf{GDP Per Capita}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.18}}\raggedright
~~~Mean (SD)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
2194 (2828)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
7136 (6397)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
7902 (14045)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
14469 (9355)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
18622 (6359)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
7215 (9857)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.18}}\raggedright
~~~Range\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
241 - 21951\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
1202 - 42952\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
331 - 113523\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
974 - 49357\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
10040 - 34435\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
241 - 113523\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.18}}\raggedright
\textbf{Life Expectancy}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.18}}\raggedright
~~~Mean (SD)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
49 (9)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
65 (9)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
60 (12)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
72 (5)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
74 (4)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
59 (13)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.18}}\raggedright
~~~Range\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
24 - 76\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
38 - 81\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
29 - 83\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
44 - 82\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
69 - 81\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
24 - 83\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.18}}\raggedright
\textbf{Population}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.18}}\raggedright
~~~Mean (SD)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
9916003 (15490923)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
24504795 (50979430)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
77038722 (206885205)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
17169765 (20519438)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
8874672 (6506342)\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
29601212 (106157897)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.18}}\raggedright
~~~Range\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
60011 - 135031164\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
662850 - 301139947\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
120447 - 1318683096\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
147962 - 82400996\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.13}}\centering
1994794 - 20434176\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 6\tabcolsep) * \real{0.14}}\centering
60011 - 1318683096\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{learncheck}
\textbf{Exercise 7:} Replicate the code shown above to create a summary
table for the \texttt{States} data that reports across regions.
\end{learncheck}

\hypertarget{export-to-csv}{%
\subsection{Export to CSV}\label{export-to-csv}}

Knitting your notebook to HTML, Word, or PDF should produce a summary table in the appropriate format. Depending on our or others' workflow, we may want to export our summary table to CSV in order to open in Excel or other spreadsheet software. Arsenal can easily handle this.

To export my \texttt{gapminder} summary to CSV, I need to create a new object that contains the actual summary table. Below, I save the last summary to the object named \texttt{sum.table}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.table }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(sum.gapminder4, }\AttributeTok{title =} \StringTok{"Summary Stats for Gapminder Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, I need to convert this table into a data frame using the \texttt{as.data.frame()} function like so.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.table }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(sum.table)}
\end{Highlighting}
\end{Shaded}

Lastly, I just need to save this data frame as a CSV file using the \texttt{write.csv()} function like so.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(sum.table, }\AttributeTok{file =} \StringTok{"sumtable.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

R will save the CSV file to my project folder. Otherwise, R will save the file to my current working directory.

\hypertarget{correlation-coefficient}{%
\section{Correlation Coefficient}\label{correlation-coefficient}}

As mentioned in Chapter \ref{descriptive-statistics}, the correlation coefficient quantifies the direction and strength of association between two numerical variables. It is rare to see correlations used in any table. Instead, correlations are typically used as an exploratory tool to inform a more advanced analysis like regression. Nevertheless, we may want to report a specific correlation coefficient in our prose.

To calculate the correlation coefficient between two variables, we can use the \texttt{cor()} function like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(gapminder}\SpecialCharTok{$}\NormalTok{gdpPercap, gapminder}\SpecialCharTok{$}\NormalTok{lifeExp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5837062
\end{verbatim}

where I include two variables from the \texttt{gapminder} dataset.

To calculate correlation coefficient between all numerical variables in a dataset, we can simply include the dataset in \texttt{cor} without specifying any variable. Note that I must first remove the variables that are not numeric.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{country, }\SpecialCharTok{{-}}\NormalTok{continent) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{cor}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                year    lifeExp         pop   gdpPercap
year      1.00000000 0.43561122  0.08230808  0.22731807
lifeExp   0.43561122 1.00000000  0.06495537  0.58370622
pop       0.08230808 0.06495537  1.00000000 -0.02559958
gdpPercap 0.22731807 0.58370622 -0.02559958  1.00000000
\end{verbatim}

\begin{learncheck}
\textbf{Exercise 8:} Calculate the correlation coefficients between all
the variables in \texttt{States}. Which two variables have the stongest
correlation? What is the direction?
\end{learncheck}

\hypertarget{save-and-upload}{%
\section{Save and Upload}\label{save-and-upload}}

Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download.

\hypertarget{r-visualization}{%
\chapter{R Visualization}\label{r-visualization}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter you will learn how to make the following visualizations:

\begin{itemize}
\tightlist
\item
  Histogram
\item
  Box plot
\item
  Bar chart
\item
  Line graph
\item
  Scatter plot
\end{itemize}

The code used to make the above visualizations in \ref{data-visualization} will be provided and explained. Then, you will be asked to replicate the visualization using different data.

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Start a R Markdown document
\item
  Change the YAML to the following:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}{-}{-}}
\NormalTok{title}\SpecialCharTok{:} \StringTok{\textquotesingle{}R Chapter 5\textquotesingle{}}
\NormalTok{author}\SpecialCharTok{:} \StringTok{\textquotesingle{}Your name\textquotesingle{}}
\NormalTok{output}\SpecialCharTok{:} 
\NormalTok{  html\_document}\SpecialCharTok{:}
\NormalTok{    theme}\SpecialCharTok{:}\NormalTok{ spacelab}
\NormalTok{    df\_print}\SpecialCharTok{:}\NormalTok{ paged}
\NormalTok{    toc}\SpecialCharTok{:}\NormalTok{ true}
\NormalTok{    toc\_float}\SpecialCharTok{:}
\NormalTok{      toc\_collapsed}\SpecialCharTok{:}\NormalTok{ false}
\NormalTok{    fig\_width}\SpecialCharTok{:} \DecValTok{6}
\NormalTok{    fig\_height}\SpecialCharTok{:} \DecValTok{4}
\NormalTok{    fig\_align}\SpecialCharTok{:} \StringTok{"center"}
\SpecialCharTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

The newest additions to the YAML header are worth explaining. The \texttt{toc} stands for table of contents, which works really well for HTML output but not so much Word or PDF. Each level of the table of contents is dictated by the headings you include in your document. The arguments beginning with \texttt{fig} dictate the size and alignment of each figure your code produces. You can override these global arguments by including with code chunk options.

\begin{itemize}
\tightlist
\item
  Load the following packages and data
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(data.table) }\CommentTok{\# contains fread function to import from URL}
\FunctionTok{library}\NormalTok{(fpp2)}
\NormalTok{countyComplete }\OtherTok{\textless{}{-}} \FunctionTok{fread}\NormalTok{(}\StringTok{\textquotesingle{}http://openintro.org/data/tab{-}delimited/county\_complete.txt\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For everything but the line graph, we will use the \texttt{countyComplete} data within the \texttt{openintro} package. This dataset contains 3,143 counties and 53 variables. For the line graph, we will use the \texttt{prisonLF} data within the \texttt{fpp2} package. This dataset is a quarterly time series of prisoner numbers in Australia from 2005 to 2016, split by sex, state, and legal status.

\hypertarget{grammar-of-graphics}{%
\section{Grammar of graphics}\label{grammar-of-graphics}}

R uses the grammar of graphics to make visualizations. You need to define \textbf{three} essential elements to produce a graph:

\begin{itemize}
\tightlist
\item
  \texttt{data}: defines the dataset containing our variable(s) of interest
\item
  \texttt{aes}: defines the variables used to generate the plot and how they are used
\item
  \texttt{geom}: defines the kind of plot
\end{itemize}

We plot variables from \texttt{data} to \texttt{aes}thetic attributes of \texttt{geom}etric objects. The function we use to do this is called \texttt{ggplot}. The generic code below shows the essential elements that will produce a default plot. We replace \texttt{data} with the name of the dataset. Within the \texttt{aes} parentheses, we tell R to assign one or more variables to a variety of attributes, such as \texttt{x} or \texttt{y} or \texttt{color}. What we include within \texttt{aes} depends on the type of plot we want to make, which is determined by the second line which always includes \texttt{geom\_} followed by the type of plot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ variable, ...)) }\SpecialCharTok{+}
  \FunctionTok{geom\_type}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

For example, the below code takes the variable \texttt{gdpPercap} from the dataset \texttt{gapminder} and maps it to the x axis \texttt{aes}thetic of a chosen \texttt{geom}etric object called a histogram.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gapminder, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdpPercap)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-132-1} \end{center}

Data, aesthetics, and geometries are the essential elements needed to generate a graph. If you tell R these three elements correctly, it will produce a graph. There are additional elements that can be added to make graphs be more effective or look better that will be covered in class.

Aesthetics take variables in your data and assign them to attributes that correspond to the geometric object you intend to use. That is, \texttt{aes} and \texttt{geom} work together and must be compatible.

For example, you can't generate a scatter plot--\texttt{geom\_point}--if you only define an \texttt{x} aesthetic. You must define an \texttt{x} and \texttt{y} aesthetic for a scatter plot. Below is a list of available aesthetics and what they control:

\begin{itemize}
\tightlist
\item
  \texttt{x}: x axis
\item
  \texttt{y}: y axis
\item
  \texttt{color}: differentiate groups by color; change color of outlines of shapes
\item
  \texttt{size}: diameter of points based on values of a variable; static size of points or thickness of lines
\item
  \texttt{fill}: fill a shape with color
\item
  \texttt{alpha}: transparency
\item
  \texttt{linetype}: line pattern
\item
  \texttt{labels}: uses text instead of plot points; adds text to axes
\item
  \texttt{shape}: differentiate groups by shape of plot points
\end{itemize}

The type of your variable also informs which aesthetic(s) to use.

Continuous variables:

\begin{itemize}
\tightlist
\item
  x and y
\item
  size
\item
  fill
\end{itemize}

Categorical variables:

\begin{itemize}
\tightlist
\item
  labels
\item
  color and/or fill
\item
  shape
\item
  linetype
\item
  size
\end{itemize}

\hypertarget{histogram}{%
\section{Histogram}\label{histogram}}

Here is the code used to make the histogram from Chapter \ref{data-visualization}. The dataset is named \texttt{college\_grad\_students} and the variable assigned to the x axis aesthetic is named \texttt{grad\_median}, which is the median earnings of full-time employees with various graduate school majors. Then, a \texttt{geom\_histogram} is added. The rest of the code inside the histogram parentheses, the \texttt{labs} parentheses (stands for labels), and the \texttt{theme\_classic} is optional and used to make the histogram look more polished.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(college\_grad\_students, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ grad\_median)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{30}\NormalTok{, }\AttributeTok{fill =} \StringTok{\textquotesingle{}steelblue\textquotesingle{}}\NormalTok{, }\AttributeTok{color =} \StringTok{\textquotesingle{}white\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}Median earnings\textquotesingle{}}\NormalTok{, }\AttributeTok{y =} \StringTok{\textquotesingle{}Count of graduate majors\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/gradmedhistrep-1} 

}

\caption{Histogram of full-time median earnings for different graduate school majors}\label{fig:gradmedhistrep}
\end{figure}

Here is the code for the same histogram without the optional code. This is all that is needed to generate a histogram. In general, all plots require very little code if we do not care what they look like.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(college\_grad\_students, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ grad\_median)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-133-1} \end{center}

\begin{learncheck}
\textbf{Exercise 1:} Add a heading \texttt{\#\ Histogram}. In the
\texttt{countyComplete} dataset, there is a variable named
\texttt{bachelors\_2010} that measures the percent of the county
population with a bachelor's degree between 2006-2010. Suppose we want
to visualize the distribution of \texttt{bachelors} with a histogram.
Generate a simple, default histogram (no optional code unless you want
to add it).
\end{learncheck}

\hypertarget{box-plot}{%
\section{Box plot}\label{box-plot}}

Below is the code used for the box plot in Chapter \ref{data-visualization}. Like the histogram, a box plot visualizes the distribution of one variable but uses descriptive measures median, first and third quartile, and identifies extreme values. Therefore, we only need to tell R which variable to assign to the either the x or y axis. Note that I assign \texttt{grad\_median} to the y axis so that the box plot is vertical, which is merely a stylistic choice. Assigning \texttt{grad\_median} to the x axis would make the box plot horizontal. Then, \texttt{geom\_boxplot} is used to tell R to make a boxplot from \texttt{grad\_median}.

Again, all the code after \texttt{geom\_boxplot} is optional and was used to make the box plot look more polished. The \texttt{fill\ =\ \textquotesingle{}steelblue\textquotesingle{}} changes the color of the box, \texttt{labs} is used to help the reader understand what \texttt{grad\_median} measures, \texttt{theme\_classic} is one of several themes built within R that changes the look of a plot, and the code inside the \texttt{theme} function removes all of the ink related to the x axis due to it being unnecessary. The \texttt{theme} function allows you to control every element of a plot. Unique themes can be created and saved for replication, saving time and avoiding errors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(college\_grad\_students, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ grad\_median)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{fill =} \StringTok{\textquotesingle{}steelblue\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{\textquotesingle{}Median earnings\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.line.x =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.text.x =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.ticks.x =} \FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/gradmedboxrep-1} 

}

\caption{Box plot of full-time median earnings for different graduate school majors}\label{fig:gradmedboxrep}
\end{figure}

Again, if we do not care how the box plot looks, all we need to make the plot is shown in the code below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(college\_grad\_students, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ grad\_median)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-136-1} \end{center}

\begin{learncheck}
\textbf{Exercise 2:} Add another heading \texttt{\#\ Boxplot}. Generate
a simple, default box plot for \texttt{bachelors\_2010} (no optional
code unless you want to add it).
\end{learncheck}

\hypertarget{bar-chart}{%
\section{Bar chart}\label{bar-chart}}

There are two functions that make bar graphs: \texttt{geom\_bar} and \texttt{geom\_col}. Recall that a bar chart is used to show counts or proportions of levels within a categorical variable. In Chapter \ref{data-visualization}, a bar chart was used to show the counts and proportions of graduate majors defined as having either high or low unemployment. The table below shows a few rows and variables from the data. Note that these data are \textbf{disaggregated} with respect to the count of majors belonging to high or low unemployment. That is, we need our bar chart to count the number of rows with ``high'' and ``low'' in the \texttt{unemp\_cat} column. In this case, we should use \texttt{geom\_bar}.

\begin{tabular}{l|r|r|l|r}
\hline
major & grad\_total & grad\_unemployment\_rate & unemp\_cat & grad\_median\\
\hline
Public Administration & 42661 & 0.059 & high & 75000\\
\hline
Political Science And Government & 695725 & 0.039 & low & 92000\\
\hline
International Relations & 69355 & 0.045 & low & 86000\\
\hline
Public Policy & 15284 & 0.031 & low & 89000\\
\hline
\end{tabular}

Below is the code used to generate the side-by-side or dodged bar chart from Chapter \ref{data-visualization}. Note the use of \texttt{geom\_bar}, which requires either an x or y aesthetic to be defined. Here, I assign the categorical variable \texttt{unemp\_cat} to the x aesthetic, making the bar chart vertical. Assigning it to the y aesthetic would make the bar chart horizontal. Again, all the code past \texttt{geom\_bar} is optional.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gradschool, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ unemp\_cat)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill =} \StringTok{\textquotesingle{}steelblue\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{\textquotesingle{}Count of degrees\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.line.x =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.ticks.x =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.title.x =} \FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-139-1} \end{center}

Here is what the bar chart looks like without the optional code.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gradschool, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ unemp\_cat)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-140-1} \end{center}

Below is the code used to generate the stacked bar chart showing counts, Figure \ref{fig:empbar}. The code within \texttt{aes} is less intuitive. Since \texttt{geom\_bar} requires an x or y aesthetic to be defined, I have to assign x to nothing via the blank quotation marks. The \texttt{fill\ =\ unemp\_cat} tell R to stack the bar chart, filling the bar with the counts of high and low.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gradschool, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \StringTok{""}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ unemp\_cat)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{\textquotesingle{}Count of degrees\textquotesingle{}}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{\textquotesingle{}Unemployment\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.line.x =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.text.x =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.ticks.x =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.title.x =} \FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-141-1} \end{center}

Below is what the stacked bar chart looks like by default.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gradschool, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \StringTok{""}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ unemp\_cat)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-142-1} \end{center}

Lastly, the code below is used to show proportions rather than counts. The only substantive difference between this code and the code above is the use of \texttt{position=\textquotesingle{}fill\textquotesingle{}} within the \texttt{geom\_bar} function. This tells R to show proportions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gradschool, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \StringTok{""}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ unemp\_cat)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{\textquotesingle{}fill\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{\textquotesingle{}Proportion of degrees\textquotesingle{}}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{\textquotesingle{}Unemployment\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.line.x =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.text.x =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.ticks.x =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.title.x =} \FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-143-1} \end{center}

\begin{learncheck}
\textbf{Exercise 3:} Add a heading \texttt{\#\ Bar\ Chart}. Suppose we
want to visualize how many counties each state has. That is, we want to
count how many rows belong to each \texttt{state} in the
\texttt{countyComplete} data using a bar chart. Generate a bar chart
that achieves this. Choose the type of bar chart you deem best.
\end{learncheck}

When should we use \texttt{geom\_col} instead? When our counts are already aggregated in our data. Refer back to the table above. Note that \texttt{grad\_total} and \texttt{grad\_median} contain the count of graduates within each major and their median pay, respectively. Therefore, we do not need R to count the number of rows in our data, but rather report each number already included in the data. The \texttt{geom\_col} function takes these counts and visualizes them using a bar (or column) chart.

The below code shows how to generate Figure \ref{fig:spiapay}, which visualized the median pay for the four majors in the data related to those offered by SPIA. Median pay is simply a number in the data that does not need counting, thus the code uses \texttt{geom\_col}, which requires both an x and y aesthetic to be defined. To allow room for the long names of each major, I made the bar chart horizontal by assigning \texttt{major} to the y aesthetic. Each bar visually represents the numbers for \texttt{grad\_median} in the above table.

There is a new piece of code in the below chunk that can be used to reorder bars in ascending or descending order, which is generally preferred over random order of peaks and valleys. The \texttt{reorder(major,\ -grad\_median)} code tells R to reorder the majors in the plot in ascending because of the minus sign; removing it would reverse the order to descending.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gradschool\_spia, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =} \FunctionTok{reorder}\NormalTok{(major, }\SpecialCharTok{{-}}\NormalTok{grad\_median), }\AttributeTok{x =}\NormalTok{ grad\_median)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{fill =} \StringTok{\textquotesingle{}steelblue\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}Median pay\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{axis.title.y =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{axis.line.y =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{axis.ticks.y =} \FunctionTok{element\_blank}\NormalTok{()}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-145-1} \end{center}

Below is what the bar chart looks like without the optional code.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gradschool\_spia, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ major, }\AttributeTok{x =}\NormalTok{ grad\_median)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-146-1} \end{center}

\hypertarget{scatter-plot}{%
\section{Scatter plot}\label{scatter-plot}}

The code below shows how the scatter plot in Chapter \ref{data-visualization} was generated. This scatter plot actually contains two geometric objects. The \texttt{geom\_point} function generates the scatter plot, and the \texttt{geom\_smooth} function generates the regression/trend line. Note the assignment of x and y aesthetics that every scatter plot requires. Everything beyond \texttt{geom\_point} is optional.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gradschool, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ grad\_median, }\AttributeTok{y =}\NormalTok{ grad\_total)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =} \StringTok{\textquotesingle{}steelblue\textquotesingle{}}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }
              \AttributeTok{linetype =} \StringTok{\textquotesingle{}dashed\textquotesingle{}}\NormalTok{, }\AttributeTok{color =} \StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_log10}\NormalTok{(}\AttributeTok{label=}\NormalTok{scales}\SpecialCharTok{::}\FunctionTok{comma\_format}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{\textquotesingle{}Total degrees\textquotesingle{}}\NormalTok{,}
       \AttributeTok{x =} \StringTok{\textquotesingle{}Median pay\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-147-1} \end{center}

Below is the scatter plot without optional code.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gradschool, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ grad\_median, }\AttributeTok{y =}\NormalTok{ grad\_total)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-148-1} \end{center}

\begin{learncheck}
\textbf{Exercise 4:} Add a heading \texttt{\#\ Scatterplot}. Pick two
variables in the \texttt{countyComplete} data and plot their
relationship using a simple scatter plot.
\end{learncheck}

\hypertarget{line-graph}{%
\section{Line graph}\label{line-graph}}

Line graphs are best for visualizing variables over time (i.e.~time series). The \texttt{prisonLF} data separate prisoner counts by male vs.~female, remanded vs.~sentenced, and state. Therefore, there are four times series for each state. The code below generates a line graph for the time series of female prisoners who were sentenced in each Australia state.

Note how this code is different from the code before because I need to manipulate it before creating the plot. Specifically, I pipe the \texttt{prisonLF} data into the \texttt{filter} verb, which keeps only the rows with \texttt{Female} and \texttt{Sentenced}. Then, I pipe that result into the typical \texttt{ggplot}. However, I do not need to specify the dataset because it is already being piped into \texttt{ggplot}. Therefore, \texttt{ggplot} only needs \texttt{aes} and \texttt{geom} to be defined. Time \texttt{t} is assigned to the \texttt{x} aesthetic, \texttt{count} is assigned to the \texttt{y} aesthetic, and \texttt{state} is assigned to the \texttt{color} aesthetic. The \texttt{color} aesthetic is a common way to plot multiple groups. It also provides a legend by default. The \texttt{geom\_line} function generates a line graph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prisonLF }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(gender }\SpecialCharTok{==} \StringTok{\textquotesingle{}Female\textquotesingle{}} \SpecialCharTok{\&}\NormalTok{ legal }\SpecialCharTok{==} \StringTok{\textquotesingle{}Sentenced\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ t, }\AttributeTok{y =}\NormalTok{ count, }\AttributeTok{color =}\NormalTok{ state)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{\textquotesingle{}State\textquotesingle{}}\NormalTok{, }\AttributeTok{y =} \StringTok{\textquotesingle{}Female Sentenced Prisoners\textquotesingle{}}\NormalTok{, }\AttributeTok{x =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-150-1} \end{center}

\begin{learncheck}
\textbf{Exercise 5:} Add a heading \texttt{\#\ Linegraph}. Create a line
graph for male prisoners who were sentenced by state.
\end{learncheck}

\hypertarget{save-and-upload}{%
\section{Save and Upload}\label{save-and-upload}}

Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download.

\hypertarget{r-regression}{%
\chapter{R Regression}\label{r-regression}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Run the following regression models:

  \begin{itemize}
  \tightlist
  \item
    Continuous outcome and explanatory variable(s)
  \item
    Categorical explanatory variable
  \item
    Binary categorical (i.e.~dummy) outcome (linear probability model)
  \item
    Interaction of two explanatory variables
  \end{itemize}
\item
  Generate tables of key results
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Start a R Markdown document
\item
  Change the YAML to at least the following. Feel free to add arguments.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}{-}{-}}
\NormalTok{title}\SpecialCharTok{:} \StringTok{\textquotesingle{}R Chapter 6\textquotesingle{}}
\NormalTok{author}\SpecialCharTok{:} \StringTok{\textquotesingle{}Your name\textquotesingle{}}
\NormalTok{output}\SpecialCharTok{:} 
\NormalTok{  html\_document}\SpecialCharTok{:}
\NormalTok{    theme}\SpecialCharTok{:}\NormalTok{ spacelab}
\NormalTok{    df\_print}\SpecialCharTok{:}\NormalTok{ paged}
\SpecialCharTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(moderndive)}
\FunctionTok{library}\NormalTok{(Stat2Data)}
\FunctionTok{library}\NormalTok{(carData)}
\end{Highlighting}
\end{Shaded}

We will use the \texttt{TeenPregnancy} dataset within the \texttt{Stat2Data} package and the \texttt{Salaries} dataset within the \texttt{carData} package. While data in most packages is available in the background once the package is loaded, we need to manually load datasets from \texttt{Stat2Data} in order to use them. Run the following code, and the dataset should show up in your Environment pane in the top-right.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"TeenPregnancy"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Be sure to view the documentation for these data by clicking on the package name under the packages tab in the bottom-right pane, then click on the dataset.

\hypertarget{running-regression}{%
\section{Running Regression}\label{running-regression}}

Chapters \ref{simple-and-multiple-regression} and \ref{categorical-variables-and-interactions} cover the following regression models:

\begin{itemize}
\tightlist
\item
  Simple linear regression with two numerical variables
\item
  Multiple linear regression with all numerical variables
\item
  Including a categorical explanatory variable (parallel slopes)
\item
  Regression with a categorical explanatory interacted with a numerical variable
\item
  Regression with a binary categorical outcome (linear probability model)
\end{itemize}

While our interpretation of results may need to adjust according to which of the above regression models we run,

\begin{quote}
the code to run a linear regression is the same regardless of the number and type of explanatory variables we include and whether the outcome variable is continuous or a binary categorical variable. With the exception of including an interaction, running the regression models listed above can be done with the same code structure shown below.
\end{quote}

\hypertarget{general-syntax}{%
\subsection{General Syntax}\label{general-syntax}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_object\_name }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ exp\_1 }\SpecialCharTok{+}\NormalTok{ exp\_2 }\SpecialCharTok{+}\NormalTok{ ... }\SpecialCharTok{+}\NormalTok{ exp\_k, }\AttributeTok{data =}\NormalTok{ name\_of\_dataset)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  We name a new object that will hold the results of our regression
\item
  \texttt{lm} is the function for linear regression (acronym for linear model)
\item
  We replace \texttt{outcome} with the name of our outcome variable that should be either numerical or binary
\item
  The tilde \texttt{\textasciitilde{}} separates the outcome on the left-hand side of the regression equation from the explanatory variables on the right-hand side
\item
  We replace the \texttt{exp\_1} to \texttt{exp\_k} with the names of however many explanatory variables we wish to include, each separated by a plus sign \texttt{+}
\item
  We replace \texttt{name\_of\_dataset} with the name of the dataset that contains the variables for the regression model.
\end{itemize}

\hypertarget{continuous-outcome-and-continuous-or-categorical-explanatory-variables}{%
\subsection{Continuous outcome and continuous or categorical explanatory variables}\label{continuous-outcome-and-continuous-or-categorical-explanatory-variables}}

Recall the following multiple regression model from Chapter \ref{simple-and-multiple-regression}.

\begin{equation}
FedSpend = \beta_0 + \beta_1Poverty + \beta_2HomeOwn + \beta_3Income + \epsilon
\label{eq:multregexrep}
\end{equation}

I ran this regression using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fedpov2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(fed\_spend }\SpecialCharTok{\textasciitilde{}}\NormalTok{ poverty }\SpecialCharTok{+}\NormalTok{ homeownership }\SpecialCharTok{+}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ selcounty)}
\end{Highlighting}
\end{Shaded}

That's all there is to it. I named the model \texttt{fedpov2} to remind myself it was the second model I ran to examine the relationship between federal spending and poverty rate. Note that the code within the \texttt{lm} function mimics Equation \eqref{eq:multregexrep}. No matter if the explanatory variables happen to be numerical or categorical, the regression works the same in R. Lastly, I did some behind-the-scenes cleaning of the original \texttt{county} data discussed in Chapter \ref{simple-and-multiple-regression} and named it \texttt{selcounty}. Therefore, I told R to use that dataset when running the regression.

\texttt{TeenPregnancy} is a dataset with 50 observations on the following 4 variables.

\begin{itemize}
\tightlist
\item
  \texttt{State} State abbreviation
\item
  \texttt{CivilWar} Role in Civil War (B=border, C=Confederate, O=other, or U=union)
\item
  \texttt{Church} Percentage who attended church in previous week (from a state survey)
\item
  \texttt{Teen} Number of pregnancies per 1,000 teenage girls in state
\end{itemize}

\begin{learncheck}
\textbf{Exercise 1:} Suppose we want to use the \texttt{TeenPregnancy}
dataset to examine whether state teen pregnancy rates are associated
with church attendance and a state's role in the Civil War, which is a
categorical variable with four levels (admittedly an odd variable to
include but let's think of it as a proxy for region). The model would be
represented using the following formula:

\begin{equation}
Teen = \beta_0 + \beta_1Church + \beta_2CivilWar + \epsilon
\end{equation}

Run this regression model.
\end{learncheck}

\hypertarget{interactions}{%
\subsection{Interactions}\label{interactions}}

Though we only cover interacting a numerical variable with a categorical variable in this course, we can interact two variables of any type using the same code. In theory, we can interact more than two variables. In any case, an interaction requires us to multiply the variables within the \texttt{lm} function.

Recall the regression model from Chapter \ref{categorical-variables-and-interactions} where \texttt{mrall} is traffic fatality rate, \texttt{vmiles} is the average miles driven, and \texttt{jaild} is whether a state imposes mandatory jail for drunk driving.

\begin{equation}
mrall = \beta_0 + \beta_1 vmiles + \beta_2 jaild + \beta_3 vmiles \times jaild + \epsilon
\end{equation}

I ran this regression using the following code

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{interactmod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mrall }\SpecialCharTok{\textasciitilde{}}\NormalTok{ vmiles }\SpecialCharTok{+}\NormalTok{ jaild }\SpecialCharTok{+}\NormalTok{ vmiles}\SpecialCharTok{*}\NormalTok{jaild, }\AttributeTok{data =}\NormalTok{ trdeath)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Note that the only difference from the code in the previous example is \texttt{vmiles*jaild}, which tells R to interact the two variables by multiplying them together. Once again, the code reflects the equation.
\end{quote}

\texttt{Salaries} is a dataset with 397 observations recording rank (AsstProf, AssocProf, Prof), discipline (A = theoretical, B = applied), years since their Ph.D., years of experience, sex, and salary.

\begin{learncheck}
\textbf{Exercise 2:} Suppose we want to use the \texttt{Salaries}
dataset to examine whether professor salary is associated with their sex
and how long they have worked at the institution. Furthermore, suppose
we theorize that the association between salary and how long they have
worked at the insitution differs by sex, thus warranting an interaction.
Therefore, we have the following model:

\begin{equation}
salary = \beta_0 + \beta_1sex + \beta_2yrs.service + \beta_3 sex \times yrs.service + \epsilon
\end{equation}

Run this regression model.
\end{learncheck}

\hypertarget{dummy-outcome}{%
\subsection{Dummy outcome}\label{dummy-outcome}}

While a regression with a dummy variable as the outcome does not require any special coding, we do need to make sure the dummy variable is coded as 1/0 in the data. Sometimes a dummy variable will be coded like this already in which case we don't need to do anything to run the regression. Other times, the dummy variable will be coded using text like ``yes'' and ``no'' or ``Male'' and ``Female'' in the case of a dummy variable for sex.

Recall in Table \ref{tab:trdeath} the coding for \texttt{jaild} is yes/no. Also recall the regression equation \eqref{eq:lpmex} for the linear probability model example restated below:

\[Pr(jaild=1)=\beta_0+\beta_1vmiles+\beta_2region+\epsilon\]

I ran this regression using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lpm\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(jaild }\SpecialCharTok{\textasciitilde{}}\NormalTok{ mrall }\SpecialCharTok{+}\NormalTok{ region, }\AttributeTok{data =}\NormalTok{ trdeath2)}
\end{Highlighting}
\end{Shaded}

But this won't work if we include \texttt{jaild} in our regression code without recoding it to 1/0. This can be done using the following code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trdeath2 }\OtherTok{\textless{}{-}}\NormalTok{ trdeath }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{jaild =} \FunctionTok{if\_else}\NormalTok{(jaild }\SpecialCharTok{==} \StringTok{\textquotesingle{}yes\textquotesingle{}}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

This code creates a new dataset named \texttt{trdeath2} which is a copy of the \texttt{trdeath} dataset except for changing the values for \texttt{jaild} using the \texttt{mutate} verb. Inside \texttt{mutate}, I name the ``new'' variable \texttt{jaild}, which overwrites the existing \texttt{jaild} variable based on what follows the equal sign.

The \texttt{if\_else} function can be used for a variety of purposes, but it is the simplest way to recode a dummy variable in text to 1/0. The first argument is the conditional. Observations that meet this conditional receive the second argument, while observations that do not receive the third argument. Using natural language, I'm telling R, ``If \texttt{jaild} equals''yes``, then code it as 1 or else code it as 0.''

\begin{learncheck}
\textbf{Exercise 3:} Let's keep using this \texttt{Salaries} data.
Suppose we wanted to predict \texttt{discipline}, which again is coded
as A = theoretical or B = applied. Suppose we wanted to predict
discipline using the following model:

\begin{equation}
Discipline = \beta_0 + \beta_1Sex + \beta_2YrsSincePhD + \epsilon
\end{equation}

Run this regression model.
\end{learncheck}

\hypertarget{reporting-regression-estimates}{%
\section{Reporting Regression Estimates}\label{reporting-regression-estimates}}

This section presents two ways to obtain results after running a regression. The first uses functions that load with the \texttt{moderndive} package and the second uses functions that load with R by default (i.e.~Base R). The \texttt{moderndive} functions are somewhat more intuitive and produce results that look nicer, but the base R functions are more robust to any variety of regression model.

\hypertarget{moderndive}{%
\subsection{Moderndive}\label{moderndive}}

To get a standard table of regression estimates using \texttt{moderndive}, we can use the \texttt{get\_regression\_table} function on our saved regression model results like so

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_regression\_table}\NormalTok{(fedpov2)}
\end{Highlighting}
\end{Shaded}

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

23.519

1.333

17.645

0.000

20.905

26.132

poverty

-0.056

0.021

-2.674

0.008

-0.097

-0.015

homeownership

-0.126

0.012

-10.736

0.000

-0.149

-0.103

income

-0.086

0.011

-7.723

0.000

-0.108

-0.064

To get goodness-of-fit measures, we can use the \texttt{get\_regression\_summaries} function like so

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_regression\_summaries}\NormalTok{(fedpov2)}
\end{Highlighting}
\end{Shaded}

r\_squared

adj\_r\_squared

mse

rmse

sigma

statistic

p\_value

df

nobs

0.064

0.063

20.72216

4.55216

4.555

71.055

0

3

3123

and if I only want the R-squared, Adjusted R-squared, and RMSE from this table, I can add the \texttt{select} function to the above code chunk like so

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_regression\_summaries}\NormalTok{(fedpov2) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(r\_squared, adj\_r\_squared, rmse)}
\end{Highlighting}
\end{Shaded}

r\_squared

adj\_r\_squared

rmse

0.064

0.063

4.55216

\begin{learncheck}
\textbf{Exercise 4:} Produce a standard table of regression estimates
and goodness-of-fit measures for each of your three regression models
using the \texttt{moderndive} functions.
\end{learncheck}

\hypertarget{base-r}{%
\subsection{Base R}\label{base-r}}

A comprehensive set of regression results can be obtained using the \texttt{summary} function on our regression model like so

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(fedpov2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
lm(formula = fed_spend ~ poverty + homeownership + income, data = selcounty)

Residuals:
   Min     1Q Median     3Q    Max 
-9.463 -2.502 -1.007  1.015 39.327 

Coefficients:
              Estimate Std. Error t value             Pr(>|t|)    
(Intercept)   23.51860    1.33290  17.645 < 0.0000000000000002 ***
poverty       -0.05597    0.02093  -2.674              0.00752 ** 
homeownership -0.12582    0.01172 -10.736 < 0.0000000000000002 ***
income        -0.08593    0.01113  -7.723   0.0000000000000152 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.555 on 3119 degrees of freedom
Multiple R-squared:  0.06397,	Adjusted R-squared:  0.06307 
F-statistic: 71.06 on 3 and 3119 DF,  p-value: < 0.00000000000000022
\end{verbatim}

which gives us most of the information from \texttt{get\_regression\_table} except for the confidence intervals.

The \texttt{summary} function also reports the R-squared and Adjusted R-squared at the bottom. The \texttt{esidual\ standard\ error} at the bottom is not \emph{exactly} the same as the RMSE above--it is actually equal to \texttt{sigma} in the full table from \texttt{get\_regression\_summaries}--but you can treat them the same.

To get the confidence intervals, we can use the \texttt{confint} function like so,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(fedpov2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                    2.5 %      97.5 %
(Intercept)   20.90515522 26.13203853
poverty       -0.09700107 -0.01493693
homeownership -0.14880274 -0.10284564
income        -0.10774450 -0.06411382
\end{verbatim}

which uses the 95\% confidence level by default.

\begin{learncheck}
\textbf{Exercise 5:} Produce a comprehensive set of results for each of
your three regression models using the Base R functions.
\end{learncheck}

\hypertarget{save-and-upload}{%
\section{Save and Upload}\label{save-and-upload}}

Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download.

\hypertarget{r-nonlinear-regression}{%
\chapter{R Nonlinear Regression}\label{r-nonlinear-regression}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Run a regression with a quadratic term
\item
  Run a regression with log transformations
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Start a R Markdown document
\item
  Change the YAML to at least the following. Feel free to add arguments.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}{-}{-}}
\NormalTok{title}\SpecialCharTok{:} \StringTok{\textquotesingle{}R Chapter 7\textquotesingle{}}
\NormalTok{author}\SpecialCharTok{:} \StringTok{\textquotesingle{}Your name\textquotesingle{}}
\NormalTok{output}\SpecialCharTok{:} 
\NormalTok{  html\_document}\SpecialCharTok{:}
\NormalTok{    theme}\SpecialCharTok{:}\NormalTok{ spacelab}
\NormalTok{    df\_print}\SpecialCharTok{:}\NormalTok{ paged}
\SpecialCharTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(moderndive)}
\FunctionTok{library}\NormalTok{(carData)}
\end{Highlighting}
\end{Shaded}

We will use the \texttt{Mroz} dataset within the \texttt{carData} package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar.

\hypertarget{quadratic-term}{%
\section{Quadratic term}\label{quadratic-term}}

Recall the below regression model from Chapter \ref{nonlinear-variables} that includes a squared term for \texttt{Age}, which allows our regression line to change directions once as \texttt{Age} changes. We included this term because Figure \ref{fig:quadscatter} suggested wages initially increase with age, then decreases.

\begin{equation}
Wage = \beta_0 + \beta_1Age + \beta_2Age^2 + \beta_3Educ + \epsilon
\end{equation}

The below code demonstrates how to include a quadratic term within the \texttt{lm} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quad\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Wage }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Age }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(Age}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ Educ, }\AttributeTok{data =}\NormalTok{ wages)}
\end{Highlighting}
\end{Shaded}

In this case, the code reflects the equation only somewhat; the \texttt{I()} is necessary to tell R that \texttt{Age\^{}2} is the squared version of \texttt{Age}. Otherwise, R would not recognize \texttt{Age\^{}2} in the data, thus excluding it from the regression.

Now we can obtain results in the usual manner.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_regression\_table}\NormalTok{(quad\_mod)}
\end{Highlighting}
\end{Shaded}

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

-22.722

3.023

-7.517

0

-28.742

-16.701

Age

1.350

0.134

10.077

0

1.083

1.617

I(Age\^{}2)

-0.013

0.001

-9.840

0

-0.016

-0.011

Educ

1.254

0.090

13.990

0

1.075

1.432

We need to alter the \texttt{Mroz} data slightly before running a regression. Run the following code that creates a new variable that equals 1 if \texttt{lfp} equals ``yes'' and 0 if \texttt{lfp} equals ``no.'' This is necessary because our outcome variable--even though categorical--must be represented numerically in order for the regression to work.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_Mroz }\OtherTok{\textless{}{-}}\NormalTok{ Mroz }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lfp\_numeric =} \FunctionTok{if\_else}\NormalTok{(lfp }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{learncheck}
\textbf{Exercise 1:} Suppose we want to examine factors that explain
whether married women participate in the labor force, which is a binary
outcome. We use the following model:

\begin{equation}
lfp = \beta_0 + \beta_1k5 + \beta_2age + \beta_3age^2 + \beta_4wc + \beta_5lwg + \beta_6inc + \epsilon
\end{equation}

Run this regression model and obtain the results.
\end{learncheck}

\hypertarget{log-transformation}{%
\section{Log Transformation}\label{log-transformation}}

In Chapter \ref{nonlinear-variables}, the following log-log regression model was run.

\begin{equation}
ln(LifeExp)=\beta_0 + \beta_1ln(GDPpercap) + \beta_2Continent + \epsilon
\end{equation}

The below code demonstrates how to transform a variable into its natural log within the \texttt{lm} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loglog }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(lifeExp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(gdpPercap) }\SpecialCharTok{+}\NormalTok{ continent, }\AttributeTok{data =}\NormalTok{ gapminder)}
\end{Highlighting}
\end{Shaded}

Note that all we need to do is place the appropriate variables within the \texttt{log()} function, which R interprets as the natural log. This \emph{temporarily} transforms the variables; it does not create new variables in the dataset equal to the natural log of the variables.

Now we can obtain results in the usual manner.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_regression\_table}\NormalTok{(loglog)}
\end{Highlighting}
\end{Shaded}

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

3.062

0.026

117.692

0

3.011

3.113

log(gdpPercap)

0.112

0.004

31.843

0

0.105

0.119

continentAmericas

0.133

0.011

12.519

0

0.112

0.154

continentAsia

0.110

0.009

12.037

0

0.092

0.128

continentEurope

0.166

0.012

14.357

0

0.143

0.189

continentOceania

0.152

0.029

5.187

0

0.095

0.210

\begin{learncheck}
\textbf{Exercise 2:} Suppose we decide we want to use the natural log of
family income exclusive of wife's income, \texttt{inc}, resulting in the
following model

\begin{equation}
lfp = \beta_0 + \beta_1k5 + \beta_2age + \beta_3age^2 + \beta_4wc + \beta_5lwg + \beta_6ln(inc) + \epsilon
\end{equation}

Run this regression model and obtain the results.
\end{learncheck}

\hypertarget{save-and-upload}{%
\section{Save and Upload}\label{save-and-upload}}

Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download.

\hypertarget{r-evaluations}{%
\chapter{R Evaluations}\label{r-evaluations}}

\hypertarget{learning-outcomes}{%
\section{Learning Outcomes}\label{learning-outcomes}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Conduct a chi-square test
\item
  Conduct an independent t-test
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Start a R Markdown document
\item
  Change the YAML to at least the following. Feel free to add arguments.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}{-}{-}}
\NormalTok{title}\SpecialCharTok{:} \StringTok{\textquotesingle{}R Chapter 8\textquotesingle{}}
\NormalTok{author}\SpecialCharTok{:} \StringTok{\textquotesingle{}Your name\textquotesingle{}}
\NormalTok{output}\SpecialCharTok{:} 
\NormalTok{  html\_document}\SpecialCharTok{:}
\NormalTok{    theme}\SpecialCharTok{:}\NormalTok{ spacelab}
\NormalTok{    df\_print}\SpecialCharTok{:}\NormalTok{ paged}
\SpecialCharTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(carData)}
\FunctionTok{library}\NormalTok{(MASS)}
\end{Highlighting}
\end{Shaded}

For the chi-square test, we will use the \texttt{MplsStops} dataset within the \texttt{carData} package. For the t-tests, we will use the \texttt{UScrime} dataset within the \texttt{MASS} package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar.

\hypertarget{chi-square-test}{%
\section{Chi-square test}\label{chi-square-test}}

A chi-square test, like the one demonstrated in Chapter \ref{hypothesis-testing}, requires two steps:

\begin{itemize}
\tightlist
\item
  Create a cross-tabulation table using the \texttt{table} function
\item
  Run the chi-square on the cross-tabulation using the \texttt{chisq.test} function
\end{itemize}

\hypertarget{cross-tab}{%
\subsection{Cross-tab}\label{cross-tab}}

Below is the code used to produce the cross-tab from Chapter \ref{hypothesis-testing}. I save the new table as \texttt{polltable}. Using the \texttt{table} function, I tell R which two variables from the \texttt{poll} dataset to cross-tabulate. The \texttt{\$} is how we identify a specific variable within a dataset. The levels of the first variable, \texttt{response}, will be tabulated by row, while the frequency of the levels of the second variable, \texttt{party}, will be tabulated by column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{polltable }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(poll}\SpecialCharTok{$}\NormalTok{response, poll}\SpecialCharTok{$}\NormalTok{party)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-188}Response by political party}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & Republican & Democrat & Independent\\
\hline
Apply for citizenship & 57 & 101 & 120\\
\hline
Guest worker & 121 & 28 & 113\\
\hline
Leave the country & 179 & 45 & 126\\
\hline
\end{tabular}
\end{table}

\hypertarget{run-chi-square}{%
\subsection{Run chi-square}\label{run-chi-square}}

Now that we have a cross-tabulation table, we can run the chi-square test. The code below demonstrates how.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(immigration\_poll)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	Pearson's Chi-squared test

data:  immigration_poll
X-squared = 100.95, df = 4, p-value < 0.00000000000000022
\end{verbatim}

Then, it is simply a matter of interpreting the results.

\begin{learncheck}
\textbf{Exercise 1:} Using the \texttt{MplsStops} data, suppose we
wanted to test whether receiving a citation after being stopped by the
police, \texttt{citationIssued}, is independent of \texttt{race.} Both
are nominal variables, so a chi-square test can be used. Run this
chi-square test.
\end{learncheck}

\begin{learncheck}
\textbf{Exercise 2:} Are the two variables independent? Why?
\end{learncheck}

\hypertarget{t-tests}{%
\section{T-tests}\label{t-tests}}

To reiterate, if the two groups in a t-test are comprised of different subjects, we use an independent t-test. If they are comprised of the same subjects, then we use a dependent t-test.

\hypertarget{independent-t-test}{%
\subsection{Independent t-test}\label{independent-t-test}}

The code below demonstrates how the independent t-test from Chapter \ref{hypothesis-testing} was conducted. The \texttt{t.test} function works a lot like the \texttt{lm} function in that the outcome is entered first, then we input the variable that identifies the groups, which is essentially an explanatory variable. The two variables are separated by \texttt{\textasciitilde{}}. Then, we tell R which dataset to use, which is called \texttt{jobtrain} in this case.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(earnings }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment, }\AttributeTok{data =}\NormalTok{ jobtrain)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	Welch Two Sample t-test

data:  earnings by treatment
t = -1.1921, df = 275.58, p-value = 0.2342
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -11629.708   2856.939
sample estimates:
mean in group 0 mean in group 1 
       21645.10        26031.49 
\end{verbatim}

\begin{learncheck}
\textbf{Exercise 3:} Using the \texttt{UScrimes} data, suppose we wanted
to test whether the probability of imprisonment, \texttt{Prob}, is
independent of between Southern and non-Southern states, \texttt{So}.
The outcome is numerical and the explanatory is nominal. Therefore, a
t-test can be used. Run this t-test.
\end{learncheck}

\begin{learncheck}
\textbf{Exercise 4:} Is there an association between the two variables?
Why?
\end{learncheck}

\hypertarget{dependent-t-test}{%
\subsection{Dependent t-test}\label{dependent-t-test}}

To conduct a dependent t-test, add the option \texttt{paired=TRUE} inside the \texttt{t.test} code like so

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(earnings }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment, }\AttributeTok{data =}\NormalTok{ jobtrain, }\AttributeTok{paired =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

However, this code will not work because the number of observations in the treatment and control groups are not equal. If we truly had a paired sample where the same subjects measured twice, then we should have the same number of observations in both groups.

\hypertarget{save-and-upload}{%
\section{Save and Upload}\label{save-and-upload}}

Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download.

\hypertarget{r-regression-diagnostics}{%
\chapter{R Regression Diagnostics}\label{r-regression-diagnostics}}

\hypertarget{learning-outcomes}{%
\section{Learning Outcomes}\label{learning-outcomes}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Produce residual vs.~fitted (RVFP) and residual vs.~leverage plots (RVLP)
\item
  Check for multicollinearity using variance inflation factor (VIF)
\item
  Exclude observations from a regression model
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Start a R Markdown document
\item
  Change the YAML to at least the following. Feel free to add arguments.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}{-}{-}}
\NormalTok{title}\SpecialCharTok{:} \StringTok{\textquotesingle{}R Chapter 9\textquotesingle{}}
\NormalTok{author}\SpecialCharTok{:} \StringTok{\textquotesingle{}Your name\textquotesingle{}}
\NormalTok{output}\SpecialCharTok{:} 
\NormalTok{  html\_document}\SpecialCharTok{:}
\NormalTok{    theme}\SpecialCharTok{:}\NormalTok{ spacelab}
\NormalTok{    df\_print}\SpecialCharTok{:}\NormalTok{ paged}
\SpecialCharTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(moderndive)}
\FunctionTok{library}\NormalTok{(car)}
\FunctionTok{library}\NormalTok{(gvlma)}
\FunctionTok{library}\NormalTok{(carData)}
\end{Highlighting}
\end{Shaded}

We will use the \texttt{States} dataset within the \texttt{carData} package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar.

\hypertarget{diagnostic-plots}{%
\section{Diagnostic Plots}\label{diagnostic-plots}}

Diagnostic plots provide us suggestive visual evidence that one or more regression assumptions have failed to hold in our model. Producing diagnostic plots is very easy. Chapters \ref{simple-and-multiple-regression} and \ref{r-regression}, the following regression was run.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fedpov2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(fed\_spend }\SpecialCharTok{\textasciitilde{}}\NormalTok{ poverty }\SpecialCharTok{+}\NormalTok{ homeownership }\SpecialCharTok{+}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ selcounty)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_regression\_table}\NormalTok{(fedpov2)}
\end{Highlighting}
\end{Shaded}

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

23.519

1.333

17.645

0.000

20.905

26.132

poverty

-0.056

0.021

-2.674

0.008

-0.097

-0.015

homeownership

-0.126

0.012

-10.736

0.000

-0.149

-0.103

income

-0.086

0.011

-7.723

0.000

-0.108

-0.064

Once we have our regression results saved to an object, all we need to do is use the \texttt{plot} function, as shown in the code below. The \texttt{plot} function produces four plots. The first plot is the RVFP and the fourth plot is the RVLP. These two plots alone can be used to investigate your regression model's LINE assumptions and influential data, but the second and third plots are useful too.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(fedpov2)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-202-1} \end{center}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-202-2} \end{center}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-202-3} \end{center}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-202-4} \end{center}

We want to see no obvious pattern in the RVF plot and a relatively straight line running along the 0 reference line. For this RVF plot, we can see an obvious pattern where the positive residuals are much greater than the negative residuals. This is a classic sign that the regression model violates assumption \textbf{N}. Whether the red line trends in one direction or another away from 0 tells us whether assumption \textbf{L} is violated. This assumption appears to be relatively OK.

The second plot is the Normal Q-Q plot. As the name suggests, it is especially useful for checking assumption \textbf{N}, which was already a concern based on the RVF plot. We want the points of a Normal Q-Q plot to track along the straight, dotted line. We see clear evidence now that assumption \textbf{N} has failed. We will need to address this in our model in order to have valid estimates.

The RVF plot did not exhibit an obvious fanning out that would indicate a violation of assumption \textbf{E} (i.e.~heteroskedasticity). The third plot is the Scale-Location plot. It is especially useful for checking assumption \textbf{E}. We want to see a straight line. Here, we see some indication that the variance in our residuals is not equal as our fitted/predicted values increase.

Finally, the RVL plot tells us whether any observations impose problematic influence on our regression results. As with all of the diagnostic plots produced by \texttt{plot}, the three most problematic observations are identified by their row number in the data. We can see that observation 2907 is the largest outlier (i.e.~has the largest residual), but has only a moderate amount of leverage. The other two observations are not as much of outliers as other observations, but their leverage combined with their residual makes them more influential than the other outliers. It does not appear as though any observations have a Cook's distance high enough to warrant removal.

\begin{learncheck}
\textbf{Exercise 1:} Using the \texttt{States} data, run a regression
model where either \texttt{SATV} or \texttt{SATM} is the outcome. Once
you have the model, produce its diagnostic plots. Do any assumptions
appear to be a concern? Do any particular observations appear
problematic?
\end{learncheck}

\hypertarget{variance-inflation-factor}{%
\section{Variance Inflation Factor}\label{variance-inflation-factor}}

VIF is a common way to check for excessive multicollinearity. There is no strict rule for identifying multicollinearity, but a VIF between 5 and 10 signals a potential problem with multicollinearity. A VIF greater than 10 is a strong indicator of multicollinearity. To obtain the VIF, we can use the \texttt{vif} function from the \texttt{car} package like so.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vif}\NormalTok{(fedpov2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      poverty homeownership        income 
       2.6846        1.2016        2.4083 
\end{verbatim}

None of the VIF values for the explanatory variables come close to 5. Therefore, we can be confident that multicollinearity is not an issue.

\begin{learncheck}
\textbf{Exercise 2:} Obtain VIF values for your regression model. Is
multicollinearity a concern?
\end{learncheck}

\hypertarget{statistical-test-on-assumption}{%
\section{Statistical test on assumption}\label{statistical-test-on-assumption}}

Visuals may be all we want or need, but we can actually conduct hypothesis testing on the critical regression assumptions for linearity, normality, and equal variance. This is also quite easy to do with the \texttt{gvlma} function from the \texttt{gvlma} package (gvlma stands for global violation of linear model assumptions).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gvlma}\NormalTok{(fedpov2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
lm(formula = fed_spend ~ poverty + homeownership + income, data = selcounty)

Coefficients:
  (Intercept)        poverty  homeownership         income  
     23.51860       -0.05597       -0.12582       -0.08593  


ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS
USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:
Level of Significance =  0.05 

Call:
 gvlma(x = fedpov2) 

                       Value      p-value                   Decision
Global Stat        32128.558 0.0000000000 Assumptions NOT satisfied!
Skewness            4772.305 0.0000000000 Assumptions NOT satisfied!
Kurtosis           27323.830 0.0000000000 Assumptions NOT satisfied!
Link Function          4.702 0.0301309727 Assumptions NOT satisfied!
Heteroscedasticity    27.721 0.0000001401 Assumptions NOT satisfied!
\end{verbatim}

Clearly, there are some serious issues with this model. That first \texttt{Global\ Stat} line is like a holistic judgment of the model based on the lower four tests. Sometimes, a model might slightly violate one assumption but not the others, resulting in a satisfied global stat.

The \texttt{Skewness} and \texttt{Kurtosis} tests pertain to the normality of the residuals. As we already knew from the plots, our residuals are not normal. The \texttt{Link\ Function} pertains to the linearity of the model. This is a sign that our model is simply misspecified, perhaps requiring some nonlinear transformations or a more complicated model outside the scope of this chapter. Lastly, \texttt{Heteroscedasiticity} tests the assumption of equal variance in the residuals. This assumption wasn't quite as clear from the plot. Here we receive a clear message that this too is a problem.

\hypertarget{excluding-observations}{%
\section{Excluding observations}\label{excluding-observations}}

We should be careful and transparent when deciding to exclude observations from an analysis. When in doubt, do not exclude observations. In this running example, I would not exclude any observations. The problems with the model are not due to one or a few observations. Sometimes, the diagnostic plots will provide clear evidence that removing a few observations will solve the problems.

First, I may want to know which counties were identified in my diagnostic plots. The code below does this via subsetting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selcounty[}\FunctionTok{c}\NormalTok{(}\DecValTok{2907}\NormalTok{, }\DecValTok{2874}\NormalTok{, }\DecValTok{1991}\NormalTok{, }\DecValTok{2898}\NormalTok{, }\DecValTok{2831}\NormalTok{),]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|r|r|r|r}
\hline
  & name & state & fed\_spend & poverty & homeownership & income\\
\hline
2907 & Fairfax city & Virginia & 45.08154 & 5.0 & 72.1 & 97.900\\
\hline
2874 & Prince George County & Virginia & 45.70920 & 6.7 & 75.4 & 64.171\\
\hline
1991 & Foster County & North Dakota & 45.84445 & 7.3 & 75.8 & 41.066\\
\hline
2898 & Alexandria city & Virginia & 33.05986 & 7.8 & 45.7 & 80.847\\
\hline
2831 & Fairfax County & Virginia & 26.64889 & 5.1 & 71.9 & 105.416\\
\hline
\end{tabular}

Interesting that most of the most problematic counties come from Virginia. Perhaps something deeper is going on with Virginia, or perhaps this is a meaningless coincidence.

If we decide an exclusion of observations is defensible, then we can exclude observations directly within the \texttt{lm} function to avoid the need to create a new dataset. In the code below, I exclude the observations in the above table from the regression model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fedpov3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(fed\_spend }\SpecialCharTok{\textasciitilde{}}\NormalTok{ poverty }\SpecialCharTok{+}\NormalTok{ homeownership }\SpecialCharTok{+}\NormalTok{ income, }
              \AttributeTok{data =}\NormalTok{ selcounty[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{2907}\NormalTok{, }\DecValTok{2874}\NormalTok{, }\DecValTok{1991}\NormalTok{, }\DecValTok{2898}\NormalTok{, }\DecValTok{2831}\NormalTok{),])}
\end{Highlighting}
\end{Shaded}

This data has over 3,000 observations, so it is unlikely that removing 5 will have any notable impact on the results, but let's check.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_regression\_table}\NormalTok{(fedpov3)}
\end{Highlighting}
\end{Shaded}

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

24.159

1.284

18.817

0.000

21.642

26.676

poverty

-0.066

0.020

-3.250

0.001

-0.105

-0.026

homeownership

-0.123

0.011

-10.940

0.000

-0.145

-0.101

income

-0.103

0.011

-9.490

0.000

-0.124

-0.081

The point estimates have changes a little, but the hypothesis tests are the same. Has this changed whether assumptions are violated?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gvlma}\NormalTok{(fedpov3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
lm(formula = fed_spend ~ poverty + homeownership + income, data = selcounty[-c(2907, 
    2874, 1991, 2898, 2831), ])

Coefficients:
  (Intercept)        poverty  homeownership         income  
      24.1589        -0.0655        -0.1232        -0.1026  


ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS
USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:
Level of Significance =  0.05 

Call:
 gvlma(x = fedpov3) 

                       Value p-value                   Decision
Global Stat        23865.871 0.00000 Assumptions NOT satisfied!
Skewness            4106.412 0.00000 Assumptions NOT satisfied!
Kurtosis           19751.553 0.00000 Assumptions NOT satisfied!
Link Function          5.449 0.01958 Assumptions NOT satisfied!
Heteroscedasticity     2.457 0.11699    Assumptions acceptable.
\end{verbatim}

Globally, no, although heteroskedasticity appears to no longer be a problem. The other tests could be due to a variety of complicated issues. Perhaps the theoretical relationships implied by the model are totally wrong. Perhaps the residuals among counties within each state are strongly correlated, thus violating assumption \textbf{I}.

The most straightforward \emph{potential} solution in this case is to try a log transformation the outcome at least and perhaps one or more explanatory variables. In the below model, I log-transform federal spending and income.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fedpov4 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(fed\_spend) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ poverty }\SpecialCharTok{+}\NormalTok{ homeownership }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(income), }
              \AttributeTok{data =}\NormalTok{ selcounty[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{2907}\NormalTok{, }\DecValTok{2874}\NormalTok{, }\DecValTok{1991}\NormalTok{, }\DecValTok{2898}\NormalTok{, }\DecValTok{2831}\NormalTok{),])}
\end{Highlighting}
\end{Shaded}

\texttt{Error\ in\ lm.fit(x,\ y,\ offset\ =\ offset,\ singular.ok\ =\ singular.ok,\ ...)\ :\ NA/NaN/Inf\ in\ \textquotesingle{}y\textquotesingle{}}

It appears some counties have federal spending that is 0 or negative, which cannot cannot be log-transformed. Let's see what those are.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selcounty[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{2907}\NormalTok{, }\DecValTok{2874}\NormalTok{, }\DecValTok{1991}\NormalTok{, }\DecValTok{2898}\NormalTok{, }\DecValTok{2831}\NormalTok{),] }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(fed\_spend }\SpecialCharTok{\textless{}=} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|r|r|r|r}
\hline
name & state & fed\_spend & poverty & homeownership & income\\
\hline
Skagway & Alaska & 0 & 10.8 & 59.1 & 73.500\\
\hline
Wrangell & Alaska & 0 & 8.3 & 78.7 & 50.389\\
\hline
\end{tabular}

Fortunately, only two counties were the problem. Now I'll go ahead create a separate dataset to keep things clear.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selcounty2 }\OtherTok{\textless{}{-}}\NormalTok{ selcounty[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{2907}\NormalTok{, }\DecValTok{2874}\NormalTok{, }\DecValTok{1991}\NormalTok{, }\DecValTok{2898}\NormalTok{, }\DecValTok{2831}\NormalTok{),] }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(fed\_spend }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And rerun the regression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fedpov4 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(fed\_spend) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ poverty }\SpecialCharTok{+}\NormalTok{ homeownership }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(income), }
              \AttributeTok{data =}\NormalTok{ selcounty2)}
\end{Highlighting}
\end{Shaded}

Does this fix our assumptions?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gvlma}\NormalTok{(fedpov4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
lm(formula = log(fed_spend) ~ poverty + homeownership + log(income), 
    data = selcounty2)

Coefficients:
  (Intercept)        poverty  homeownership    log(income)  
      6.72532       -0.01675       -0.01254       -0.89687  


ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS
USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:
Level of Significance =  0.05 

Call:
 gvlma(x = fedpov4) 

                          Value p-value                   Decision
Global Stat        1210.5140072  0.0000 Assumptions NOT satisfied!
Skewness            452.0579572  0.0000 Assumptions NOT satisfied!
Kurtosis            758.3481943  0.0000 Assumptions NOT satisfied!
Link Function         0.1076608  0.7428    Assumptions acceptable.
Heteroscedasticity    0.0001948  0.9889    Assumptions acceptable.
\end{verbatim}

This appears to have fixed the issue with linearity, but normality of the residuals is still an issue. Let's produce a new set of diagnostic plots to visualize the difference all this has made.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(fedpov4)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-219-1} \end{center}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-219-2} \end{center}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-219-3} \end{center}

\begin{center}\includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/unnamed-chunk-219-4} \end{center}

With the exception of the Normal Q-Q plot, all of the plots look much better. Unfortunately, we have exhausted the options at our disposal for fixing our regression (insofar as this course is covers).

As you can see from this example, regression diagnostics can take you down some interesting paths of investigation. Sometimes the solution is obvious. Other times the solution still eludes you after several iterations.

\begin{learncheck}
\textbf{Exercise 3:} Try to correct your regression model based on your
diagnostic results. Maybe exclude one or more observations from your
regression model.
\end{learncheck}

\hypertarget{save-and-upload}{%
\section{Save and Upload}\label{save-and-upload}}

Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download.

\hypertarget{appendix-appendix}{%
\appendix \addcontentsline{toc}{chapter}{\appendixname}}


\hypertarget{coding-tips}{%
\chapter{Coding Tips}\label{coding-tips}}

\hypertarget{keyboard-shortcuts}{%
\section{Keyboard Shortcuts}\label{keyboard-shortcuts}}

There are three things you will do often in this course for which there are keyboard shortcuts that will save you time and energy over the long run.

\begin{itemize}
\tightlist
\item
  Insert a code chunk: \texttt{Cmd+Opt+I} on Mac or \texttt{Ctrl+Alt+I} on Windows
\item
  Run the current line or selection of code: \texttt{Cmd+Return} on Mac or \texttt{Ctrl+Enter} on Windows
\item
  Knit document: \texttt{Cmd+Shift+K} on Mac or \texttt{Ctrl+Shift+K} on Windows
\end{itemize}

There are many more keyboard shortcuts. Accessing keyboard shortcuts has a keyboard shortcut! It is \texttt{Opt+Shift+K} on Mac or \texttt{Alt+Shift+K} on Windows.

\hypertarget{specifying-datasets-and-variables}{%
\section{Specifying Datasets and Variables}\label{specifying-datasets-and-variables}}

\hypertarget{datasets}{%
\subsection{Datasets}\label{datasets}}

R can store multiple datasets or objects at a time for you to work with. Therefore, you must tell R the dataset on which to run a function. Suppose I want R to provide a quick preview of a dataset named \texttt{gapminder}. I can use the \texttt{glimpse()} function for this like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(gapminder)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 1,704
Columns: 6
$ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, ...
$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia...
$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992...
$ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.8...
$ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 1488...
$ gdpPercap <dbl> 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 78...
\end{verbatim}

Failure to tell R a dataset that has been loaded in your environment will result in an error. For example, suppose I misspell the dataset so that R looks for an object that does not exist.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(gasminder)}

\NormalTok{Error }\ControlFlowTok{in} \FunctionTok{glimpse}\NormalTok{(gasminder) }\SpecialCharTok{:}\NormalTok{ object }\StringTok{\textquotesingle{}gasminder\textquotesingle{}}\NormalTok{ not found}
\end{Highlighting}
\end{Shaded}

\hypertarget{variables}{%
\subsection{Variables}\label{variables}}

Some functions pertain not to an entire dataset but to a specific variable within a dataset. Suppose I wanted to compute an average using the \texttt{mean()} function. Only specifying the dataset results in an error because the mean of a dataset makes no sense.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(gapminder)}

\NormalTok{argument is not numeric or logical}\SpecialCharTok{:}\NormalTok{ returning }\ConstantTok{NA}\NormalTok{[}\DecValTok{1}\NormalTok{] }\ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

Instead, I need to specify a variable for which I want the average. To specify a variable within a dataset, we use the \texttt{\$} operator. Suppose I want to compute the average life expectancy, \texttt{lifeExp}, in the \texttt{gapminder} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(gapminder}\SpecialCharTok{$}\NormalTok{lifeExp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 59.47444
\end{verbatim}

\hypertarget{assignment-and-pipe-operators}{%
\section{Assignment and Pipe Operators}\label{assignment-and-pipe-operators}}

\hypertarget{assignment-operator}{%
\subsection{Assignment Operator}\label{assignment-operator}}

Whenever we run a function we have the option of saving the result as a new object to reference for future use. To save a the result of anything to a new object, we use the assignment operator, \texttt{\textless{}-}. Whatever happens on the right side of \texttt{\textless{}-} is assigned to whatever name we give the new object on the left side.

I just computed average life expectancy. Suppose I want to save that result. In that case, I would use the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{avg\_lifeExp }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(gapminder}\SpecialCharTok{$}\NormalTok{lifeExp)}
\end{Highlighting}
\end{Shaded}

This \texttt{avg\_lifeExp} will now show up in my environment pane (top-right) as a single value. This works not just for specific values but for \emph{anything} we want to save to use later in our code.

Note that R did not print out the result like it did when I ran \texttt{mean(gapminder\$lifeExp)} above. This is because R assumes I do not want the printout because I am saving it. If I want R to print the result, I can simply run the object name like so

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{avg\_lifeExp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 59.47444
\end{verbatim}

or I can wrap the code originally assigning the new object in parentheses

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(avg\_lifeExp }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(gapminder}\SpecialCharTok{$}\NormalTok{lifeExp))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 59.47444
\end{verbatim}

You will use the assignment operator often in this course. The keyboard shortcut for it is \texttt{Opt\ +\ -}, that is option and the minus sign key on Mac, or \texttt{Alt\ +\ -} on Windows.

\hypertarget{pipe-operator}{%
\subsection{Pipe Operator}\label{pipe-operator}}

We do not have to do one thing at a time in a code chunk, nor do we need to save a new object with each function we apply to an existing object.

Suppose I want a dataset that includes only 1952 as well as country and life expectancy. I could use code like so (this involves functions you may not know yet):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gap\_1952 }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(gapminder, year }\SpecialCharTok{==} \DecValTok{1952}\NormalTok{)}
\NormalTok{gap\_1952\_lifeExp }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{(gap\_1952, country, lifeExp)}
\end{Highlighting}
\end{Shaded}

The first line of the above code uses the \texttt{filter} function to save a new object named \texttt{gap\_1952} that contains \texttt{gapminder} observations only for which \texttt{year} equals 1952. The second line of the above code uses the \texttt{select} function to save a new object named \texttt{gap\_1952\_lifeExp} that includes only the \texttt{country} and \texttt{lifeExp} variables from the \texttt{gap\_1952} dataset.

That code includes unnecessary intermediate object/dataset. It is also difficult to read and follow because you have to track which datasets are used in each step. The pipe operator makes this sort of iterative process much easier to code and read.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gap\_1952\_lifeExp }\OtherTok{\textless{}{-}}\NormalTok{ gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(year }\SpecialCharTok{==} \DecValTok{1952}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(country, lifeExp)}
\end{Highlighting}
\end{Shaded}

The pipe operator pipes/feeds the result of what precedes it to the next line and so on. In the code above, I start by naming a new object, \texttt{gap\_1952\_lifeExp}. This new object is determined by taking the \texttt{gapminder} dataset, then piping it to the \texttt{filter} function that keeps observations for which year equals 1952, then piping the result of that -- which is equivalent to the intermediate \texttt{gap\_1952} dataset from before -- to the \texttt{select} function that keeps only the \texttt{country} and \texttt{lifeExp} variables.

Now we can compute the average life expectancy in 1952 like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(gap\_1952\_lifeExp}\SpecialCharTok{$}\NormalTok{lifeExp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 49.05762
\end{verbatim}

Note how much easier it is to read the code using the pipe operator compared to the code that does not. With the pipe operator, you can read code from left-to-right to understand what is being done. Also, recall that you must specify the dataset for any function.

\begin{quote}
If you use the pipe operator, you do not need to specify the dataset in every function included in the pipe because you will have already fed the dataset to the next line containing the function.
\end{quote}

For example, the \texttt{filter} function by default requires us to specify the object like so

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{(gapminder, year }\SpecialCharTok{==} \DecValTok{1952}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This is the same code that produced the above intermediate dataset, \texttt{gap\_1952}. But note how when using pipes, one does not need to specify the dataset within the \texttt{filter} function. This is because the pipe operator already feeds the \texttt{gapminder} dataset to the \texttt{filter} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gap\_1952 }\OtherTok{\textless{}{-}}\NormalTok{ gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(year }\SpecialCharTok{==} \DecValTok{1952}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Forgetting to exclude the dataset from a function when using the pipe operator will result in an error.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gap\_1952 }\OtherTok{\textless{}{-}}\NormalTok{ gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(gapminder, year }\SpecialCharTok{==} \DecValTok{1952}\NormalTok{)}

\NormalTok{Error}\SpecialCharTok{:}\NormalTok{ Problem with }\StringTok{\textasciigrave{}}\AttributeTok{filter()}\StringTok{\textasciigrave{}}\NormalTok{ input }\StringTok{\textasciigrave{}}\AttributeTok{..1}\StringTok{\textasciigrave{}}\NormalTok{.}
\NormalTok{x Input }\StringTok{\textasciigrave{}}\AttributeTok{..1$country}\StringTok{\textasciigrave{}}\NormalTok{ must be a logical vector, not a factor}\SpecialCharTok{\textless{}}\NormalTok{bf6dc}\SpecialCharTok{\textgreater{}}\NormalTok{.}
\end{Highlighting}
\end{Shaded}

The keyboard shortcut for the pipe operator is \texttt{Cmd+Shift+M} for Mac or \texttt{Ctrl+Shift+M} for Windows.

\hypertarget{appendixB}{%
\chapter{Wrangle and Tidy Reference}\label{appendixB}}

Unless data are already perfectly prepared, the most time consuming part of data analysis is wrangling and tidying data. It is impossible to cover all scenarios one may encounter when preparing raw data for an analysis. Even for advanced users of R, it is not uncommon to search for an unknown solution to a new problem via the web, texts, or manuals. Attempting to memorize the plethora of functions in R that could serve as solutions would quickly result in diminishing returns. Instead, it is more realistic to obtain enough familiarity with basic wrangle and tidy problems and solutions that one knows how and where to effectively search for the solution.

\hypertarget{cheatsheets}{%
\section{Cheatsheets}\label{cheatsheets}}

RStudio provides numerous \href{https://rstudio.com/resources/cheatsheets/}{cheatsheets} to help R users reference commonly used and helpful functions. Below is a list of cheatsheets that pertain to wrangling and tidying.

This is the most relevant cheatsheet for what you will encounter in the course:

\begin{itemize}
\tightlist
\item
  \href{https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf}{Data transformation}
\end{itemize}

Others that are less relevant:

\begin{itemize}
\tightlist
\item
  \href{https://github.com/rstudio/cheatsheets/raw/master/factors.pdf}{Factors}
\item
  \href{https://github.com/rstudio/cheatsheets/raw/master/strings.pdf}{Working with string variables}
\item
  \href{https://github.com/rstudio/cheatsheets/raw/master/lubridate.pdf}{Dates and times}
\end{itemize}

Knowing just a handful of functions can help you make considerable progress in many situations. The remainder of this chapter serves as a sort of cheatsheet for problems you may encounter during the course. Functions are demonstrated using the \texttt{gapminder} data.

The \texttt{tidyverse} package is actually a collection of several \texttt{packages} designed to make the wrangle, tidy, and data exploration process as intuitive and consistent as possible. You should almost always load \texttt{tidyverse}, as it contains every function you may need to wrangle and tidy data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\hypertarget{wrangle-verbs}{%
\section{Wrangle Verbs}\label{wrangle-verbs}}

\begin{itemize}
\tightlist
\item
  \textbf{filter:} extract rows/cases
\item
  \textbf{select:} extract columns/variables
\item
  \textbf{mutate:} alter existing variables or create new variables
\item
  \textbf{if\_else:} use a conditional to create a new variable equal to one value if an observation meets the conditional and another value if it does not; often combined with \texttt{mutate}
\item
  \textbf{arrange:} reorder rows in ascending or descending order of one or more variables
\item
  \textbf{head/tail:} extract the top/bottom number of rows
\item
  \textbf{summarize:} collapses data into a table of summary statistics
\item
  \textbf{group\_by:} tells R to apply functions to each group separately; common to use with summarize
\end{itemize}

\hypertarget{filter}{%
\subsection{Filter}\label{filter}}

Use \texttt{filter} to extract rows from a dataset. Inversely, one can think of \texttt{filter} as a way to remove rows. However, remember that \texttt{filter} keeps the rows that meet the condition on which you filter. Therefore, you want to use a condition that keeps the rows you want.

Note there are 1,704 rows in the \texttt{gapminder} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(gapminder)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 1,704
Columns: 6
$ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, ...
$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia...
$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992...
$ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.8...
$ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 1488...
$ gdpPercap <dbl> 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 78...
\end{verbatim}

Suppose I want to keep only countries in Asia. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(continent }\SpecialCharTok{==} \StringTok{\textquotesingle{}Asia\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 396
Columns: 6
$ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, ...
$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia...
$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992...
$ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.8...
$ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 1488...
$ gdpPercap <dbl> 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 78...
\end{verbatim}

The result is a new dataset with 396 rows. \textbf{Note the use of double equal signs \texttt{==} to tell R it is a conditional (``if equal to'') rather than setting something equal to something else, which would not make sense in this case.}

Suppose I want countries in Asia \textbf{AND} in the year 1952. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(continent }\SpecialCharTok{==} \StringTok{\textquotesingle{}Asia\textquotesingle{}} \SpecialCharTok{\&}\NormalTok{ year }\SpecialCharTok{==} \DecValTok{1952}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 33
Columns: 6
$ country   <fct> "Afghanistan", "Bahrain", "Bangladesh", "Cambodia", ...
$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia...
$ year      <int> 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952...
$ lifeExp   <dbl> 28.801, 50.939, 37.484, 39.417, 44.000, 60.960, 37.3...
$ pop       <int> 8425333, 120447, 46886859, 4693836, 556263527, 21259...
$ gdpPercap <dbl> 779.4453, 9867.0848, 684.2442, 368.4693, 400.4486, 3...
\end{verbatim}

This results in a new dataset with 33 rows. \textbf{Note the use of the ampersand \texttt{\&} to code the ``and'' conditional.}

Suppose I want countries in Asia with a life expectancy less than or equal to 40 in 1952. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(continent }\SpecialCharTok{==} \StringTok{\textquotesingle{}Asia\textquotesingle{}} \SpecialCharTok{\&}\NormalTok{ year }\SpecialCharTok{==} \DecValTok{1952} \SpecialCharTok{\&}\NormalTok{ lifeExp }\SpecialCharTok{\textless{}=} \DecValTok{40}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 10
Columns: 6
$ country   <fct> "Afghanistan", "Bangladesh", "Cambodia", "India", "I...
$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia...
$ year      <int> 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952...
$ lifeExp   <dbl> 28.801, 37.484, 39.417, 37.373, 37.468, 36.319, 36.1...
$ pop       <int> 8425333, 46886859, 4693836, 372000000, 82052000, 200...
$ gdpPercap <dbl> 779.4453, 684.2442, 368.4693, 546.5657, 749.6817, 33...
\end{verbatim}

Suppose I all countries in 1952 except those in Asia. There are a few options to do this. Which option is most efficient depends on the specific case. In this case:

\textbf{Option 1: Using the ``or'' conditional \texttt{\textbar{}} (least efficient)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(continent }\SpecialCharTok{==} \StringTok{\textquotesingle{}Africa\textquotesingle{}} \SpecialCharTok{|}\NormalTok{ continent }\SpecialCharTok{==} \StringTok{\textquotesingle{}Americas\textquotesingle{}} \SpecialCharTok{|}\NormalTok{ continent }\SpecialCharTok{==} \StringTok{\textquotesingle{}Europe\textquotesingle{}} \SpecialCharTok{|}\NormalTok{ continent }\SpecialCharTok{==} \StringTok{\textquotesingle{}Oceania\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 1,308
Columns: 6
$ country   <fct> Albania, Albania, Albania, Albania, Albania, Albania...
$ continent <fct> Europe, Europe, Europe, Europe, Europe, Europe, Euro...
$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992...
$ lifeExp   <dbl> 55.230, 59.280, 64.820, 66.220, 67.690, 68.930, 70.4...
$ pop       <int> 1282697, 1476505, 1728137, 1984060, 2263554, 2509048...
$ gdpPercap <dbl> 1601.056, 1942.284, 2312.889, 2760.197, 3313.422, 35...
\end{verbatim}

\textbf{Option 2: Using the shortcut \texttt{\%in\%} for multiple ``or'' conditionals (moderately efficient)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(continent }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Africa\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Americas\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Europe\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Oceania\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 1,308
Columns: 6
$ country   <fct> Albania, Albania, Albania, Albania, Albania, Albania...
$ continent <fct> Europe, Europe, Europe, Europe, Europe, Europe, Euro...
$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992...
$ lifeExp   <dbl> 55.230, 59.280, 64.820, 66.220, 67.690, 68.930, 70.4...
$ pop       <int> 1282697, 1476505, 1728137, 1984060, 2263554, 2509048...
$ gdpPercap <dbl> 1601.056, 1942.284, 2312.889, 2760.197, 3313.422, 35...
\end{verbatim}

\textbf{Option 3: Use the ``not equal to'' conditional \texttt{!=} (most efficient)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(continent }\SpecialCharTok{!=} \StringTok{\textquotesingle{}Asia\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 1,308
Columns: 6
$ country   <fct> Albania, Albania, Albania, Albania, Albania, Albania...
$ continent <fct> Europe, Europe, Europe, Europe, Europe, Europe, Euro...
$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992...
$ lifeExp   <dbl> 55.230, 59.280, 64.820, 66.220, 67.690, 68.930, 70.4...
$ pop       <int> 1282697, 1476505, 1728137, 1984060, 2263554, 2509048...
$ gdpPercap <dbl> 1601.056, 1942.284, 2312.889, 2760.197, 3313.422, 35...
\end{verbatim}

\hypertarget{select}{%
\subsection{Select}\label{select}}

Suppose I want a dataset that contains only country, continent, year, and life expectancy. There are multiple options. Which is more efficient depends on the specific case. In this case:

\textbf{Option 1: List the variables I want to keep (least efficient)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(country, continent, year, lifeExp) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 1,704
Columns: 4
$ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, ...
$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia...
$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992...
$ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.8...
\end{verbatim}

\textbf{Option 2: List the variables I don't want to keep (moderately efficient)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{pop, }\SpecialCharTok{{-}}\NormalTok{gdpPercap) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 1,704
Columns: 4
$ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, ...
$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia...
$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992...
$ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.8...
\end{verbatim}

\textbf{Option 3: Use \texttt{:} to specify the range of variables, which only works because the variables I want happen to be stored next to each other (most efficient)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(country}\SpecialCharTok{:}\NormalTok{lifeExp) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 1,704
Columns: 4
$ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, ...
$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia...
$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992...
$ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.8...
\end{verbatim}

\hypertarget{mutate}{%
\subsection{Mutate}\label{mutate}}

The \texttt{mutate} function allows you to mutate your dataset by either changing an existing variable or creating a new one.

Suppose I wanted to change GDP per capita so that it is expressed in thousands of dollars instead of dollars. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gdpPercap =}\NormalTok{ gdpPercap}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 1,704
Columns: 6
$ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, ...
$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia...
$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992...
$ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.8...
$ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 1488...
$ gdpPercap <dbl> 0.7794453, 0.8208530, 0.8531007, 0.8361971, 0.739981...
\end{verbatim}

\textbf{Note that I use the name of an existing variable on the left-hand side of the equation. This overwrites the data according to the function I have specified.} You can scroll up to previous glimpses to confirm that gdpPercap has indeed been divided by 1,000.

Suppose I wanted a new variable that measures total GDP to have in addition to GDP per capita expressed in thousands. Since GDP per capita equals GDP divided by population, I can simply use the inverse of this calculation. Thus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gdpPercap =}\NormalTok{ gdpPercap}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{,}
         \AttributeTok{gdp =}\NormalTok{ gdpPercap}\SpecialCharTok{*}\NormalTok{pop) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 1,704
Columns: 7
$ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, ...
$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia...
$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992...
$ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.8...
$ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 1488...
$ gdpPercap <dbl> 0.7794453, 0.8208530, 0.8531007, 0.8361971, 0.739981...
$ gdp       <dbl> 6567086, 7585449, 8758856, 9648014, 9678553, 1169765...
\end{verbatim}

Since \texttt{mutate} applies mathematical functions, there are way too many possible uses to cover here. The second page of the \href{https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf}{Data transformation} cheatsheet lists numerous common functions used with \texttt{mutate} under the ``Vector Functions'' header. Also, the first page of the cheatsheet lists a few different versions of \texttt{mutate} that can come in handy. One particularly helpful variation is \texttt{mutate\_at}.

Suppose there are multiple variables you want to mutate using the same formula. A common example is when a bunch of variables are expressed as proportions between 0 and 1 when you want them all to be expressed as percentages between 0 and 100. You could list each mutate individually, but this quickly becomes tedious. Instead, you can use \texttt{mutate\_at} to list the variables you want to mutate, then define the function you want applied to them.

For example, suppose I wanted to multiply all of the numerical variables in \texttt{gapminder} by 100 (doesn't make sense but just go with it). Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(year, lifeExp, pop, gdpPercap), }\FunctionTok{funs}\NormalTok{(.}\SpecialCharTok{*}\DecValTok{100}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 1,704
Columns: 6
$ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, ...
$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia...
$ year      <dbl> 195200, 195700, 196200, 196700, 197200, 197700, 1982...
$ lifeExp   <dbl> 2880.1, 3033.2, 3199.7, 3402.0, 3608.8, 3843.8, 3985...
$ pop       <dbl> 842533300, 924093400, 1026708300, 1153796600, 130794...
$ gdpPercap <dbl> 77944.53, 82085.30, 85310.07, 83619.71, 73998.11, 78...
\end{verbatim}

The period \texttt{.} inside \texttt{funs} is a generic placeholder, telling R to multiply each of the variables inside \texttt{vars} by 100.

\hypertarget{combining-filter-select-and-mutate}{%
\subsection{Combining filter, select, and mutate}\label{combining-filter-select-and-mutate}}

You can do some serious wrangling efficiently with filter, select, and mutate. Suppose I wanted a new dataset of GDP (in billions) for European countries in 2007. \textbf{Recall that the pipe operator, \texttt{\%\textgreater{}\%}, makes code easier to read and write by feeding the result of what precedes it to the next line that follows and so on.}

In the code below, I create a new dataset named \texttt{euro\_gdp07} by first taking the \texttt{gapminder} dataset, then feeding it to the filter verb. The result is a dataset that includes only European countries in 2007, but this dataset is not created explicitly. Instead, it is fed to the \texttt{mutate} verb, which adds a variable named \texttt{gdp\_billions}. Finally, this dataset is fed to the \texttt{select} verb. Using the \texttt{glimpse} verb we can see the final result.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{euro\_gdp07 }\OtherTok{\textless{}{-}}\NormalTok{ gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(continent }\SpecialCharTok{==} \StringTok{\textquotesingle{}Europe\textquotesingle{}} \SpecialCharTok{\&}\NormalTok{ year }\SpecialCharTok{==} \DecValTok{2007}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gdp\_billions =}\NormalTok{ (gdpPercap}\SpecialCharTok{*}\NormalTok{pop)}\SpecialCharTok{/}\DecValTok{1000000000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(country, year, gdp\_billions)}

\FunctionTok{glimpse}\NormalTok{(euro\_gdp07)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 30
Columns: 3
$ country      <fct> Albania, Austria, Belgium, Bosnia and Herzegovina...
$ year         <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2...
$ gdp_billions <dbl> 21.376411, 296.229401, 350.141167, 33.897027, 78....
\end{verbatim}

\hypertarget{combining-mutate-and-if_else}{%
\subsection{Combining Mutate and If\_Else}\label{combining-mutate-and-if_else}}

There are two common cases for using the combination of \texttt{mutate} and \texttt{if\_else}:

\begin{itemize}
\tightlist
\item
  Convert the values of a two-level categorical variable (i.e.~dummy variable) from text to numerical
\item
  Convert the values of a numerical variable or categorical variable with more than two levels to a two-level categorical variable
\end{itemize}

In either case, we can choose to create a new variable or overwrite the existing variable we wish to convert.

Suppose I want to create a new variable named \texttt{rich} equal to ``yes'' if a European country has a GDP greater than the average GDP and ``no'' if their GDP is less than or equal to the average.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{euro\_gdp07 }\OtherTok{\textless{}{-}}\NormalTok{ euro\_gdp07 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{rich =} \FunctionTok{if\_else}\NormalTok{(gdp\_billions }\SpecialCharTok{\textgreater{}} \FunctionTok{mean}\NormalTok{(gdp\_billions), }\StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The first line in the above code overwrites the \texttt{euro\_gdp07} dataset by using the same name on the left side of the assignment operator \texttt{\textless{}-}. The \texttt{euro\_gdp07} is fed/piped to the \texttt{mutate} verb. Inside \texttt{mutate}, a name a variable \texttt{rich}. Since \texttt{rich} does not currently exist in the \texttt{euro\_gdp07} dataset, a new variable will be added.

This new variable named \texttt{rich} is defined using the \texttt{if\_else} function. The first argument is the conditional. Here I define the conditional as ``if \texttt{gdp\_billions} is greater than the mean of \texttt{gdp\_billions}''. Observations that meet the conditional you specify receive the second argument. In this case, European countries with a GDP greater than the mean of GDP among all European countries will receive a value equal to ``yes''. Observations that do not meet the conditional you specify receive the third argument. In this case, European countries with a GDP less than or equal to the mean of GDP among all European countries will receive a value equal to ``no''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(euro\_gdp07)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 30
Columns: 4
$ country      <fct> Albania, Austria, Belgium, Bosnia and Herzegovina...
$ year         <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2...
$ gdp_billions <dbl> 21.376411, 296.229401, 350.141167, 33.897027, 78....
$ rich         <chr> "no", "no", "no", "no", "no", "no", "no", "no", "...
\end{verbatim}

Now suppose instead of using text (i.e.~string variable) for \texttt{rich}, I want to use a numerical coding of 1/0 where 1 denotes yes/true and 0 no/false.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{euro\_gdp07 }\OtherTok{\textless{}{-}}\NormalTok{ euro\_gdp07 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{rich =} \FunctionTok{if\_else}\NormalTok{(rich }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Since rich already exists in \texttt{euro\_gdp07}, I use the conditional ``if \texttt{rich} equals yes.'' If it does, the variable is overwritten with the value 1. If it does not, it is overwritten with the value 0.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(euro\_gdp07)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 30
Columns: 4
$ country      <fct> Albania, Austria, Belgium, Bosnia and Herzegovina...
$ year         <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2...
$ gdp_billions <dbl> 21.376411, 296.229401, 350.141167, 33.897027, 78....
$ rich         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0...
\end{verbatim}

\hypertarget{arrange}{%
\subsection{Arrange}\label{arrange}}

The \texttt{arrange} verb is useful if you want to identify cases that have the highest or lowest values for one or more variables. By default, \texttt{arrange} reorders rows in ascending order (i.e.~lowest to highest). In the previous glimpse, countries are arranged in alphabetical order. Suppose I wanted them arranged based on GDP.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{euro\_gdp07 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(gdp\_billions) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 30
Columns: 4
$ country      <fct> Montenegro, Iceland, Albania, Bosnia and Herzegov...
$ year         <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2...
$ gdp_billions <dbl> 6.336476, 10.924102, 21.376411, 33.897027, 51.774...
$ rich         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
\end{verbatim}

Now we see a few countries with the lowest GDP. If instead I wanted GDP arranged from highest to lowest, then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{euro\_gdp07 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(gdp\_billions)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 30
Columns: 4
$ country      <fct> Germany, United Kingdom, France, Italy, Spain, Ne...
$ year         <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2...
$ gdp_billions <dbl> 2650.87089, 2017.96931, 1861.22794, 1661.26443, 1...
$ rich         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...
\end{verbatim}

Now we see some of the wealthiest European countries.

\hypertarget{headtail}{%
\subsection{Head/Tail}\label{headtail}}

By default, the \texttt{head} and \texttt{tail} verbs extract the top and bottom 6 rows of a dataset, respectively. These verbs are useful if we want to show a reader a sample of the data in a familiar spreadsheet form, which can be useful. Though the output from \texttt{glimpse} is very useful, it does not look good in a report. The \texttt{head} and \texttt{tail} verbs allow us to provide similar information in a much more presentable format.

Suppose we wanted to show a reader the three wealthiest and poorest European countries (in absolute terms). We can specify the number of rows \texttt{head} or \texttt{tail} extract using \texttt{n=\#}. Thus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{euro\_gdp07 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(gdp\_billions)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n=}\DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{digits =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r}
\hline
country & year & gdp\_billions & rich\\
\hline
Germany & 2007 & 2651 & 1\\
\hline
United Kingdom & 2007 & 2018 & 1\\
\hline
France & 2007 & 1861 & 1\\
\hline
\end{tabular}

Note the use of \texttt{kable} in the last line. This function from the \texttt{knitr} package is a common way to print nicer looking tables. The \texttt{digits=} inside specifies how many digits to the right of the decimal to include in the table. In this case, I tell R to round to the nearest whole number.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{euro\_gdp07 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(gdp\_billions) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n=}\DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{digits =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r}
\hline
country & year & gdp\_billions & rich\\
\hline
Montenegro & 2007 & 6 & 0\\
\hline
Iceland & 2007 & 11 & 0\\
\hline
Albania & 2007 & 21 & 0\\
\hline
\end{tabular}

\hypertarget{summarize}{%
\subsection{Summarize}\label{summarize}}

Summarize creates a new dataset by collapsing all of the cases of a dataset into one or more summary statistics. It is useful for providing quick summary stat calculations in a somewhat presentable format. I do not recommend using \texttt{summarize} to produce the kind of summary stats table commonly found in reports because it can become tedious and the formatting is not good enough. I recommend using the \texttt{arsenal} package instead.

Suppose I wanted to report the average gdpPercap and lifeExp for 2007 in a rough and ready table. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(year }\SpecialCharTok{==} \DecValTok{2007}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\StringTok{\textquotesingle{}Average GDP per capita\textquotesingle{}} \OtherTok{=} \FunctionTok{mean}\NormalTok{(gdpPercap), }
            \StringTok{\textquotesingle{}Average life expectance\textquotesingle{}} \OtherTok{=} \FunctionTok{mean}\NormalTok{(lifeExp)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{digits =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r}
\hline
Average GDP per capita & Average life expectance\\
\hline
11680 & 67\\
\hline
\end{tabular}

The \texttt{summarize} verb works with numerous summary functions listed on the second page of the \href{https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf}{Data transformation} cheatsheet under the heading ``Summary Functions.''

\hypertarget{group_by}{%
\subsection{Group\_By}\label{group_by}}

The \texttt{group\_by} verb is most commonly used in tandem with \texttt{summarize}. If instead of calculating a summary stat for the entire dataset, you wanted to calculate the summary stat for each group of a categorical variable separately, use \texttt{group\_by} before using \texttt{summarize}.

Suppose I wanted average GDP per capita and life expectancy in 2007 for each continent. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(year }\SpecialCharTok{==} \DecValTok{2007}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(continent) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\StringTok{\textquotesingle{}Average GDP per capita\textquotesingle{}} \OtherTok{=} \FunctionTok{mean}\NormalTok{(gdpPercap), }
            \StringTok{\textquotesingle{}Average life expectance\textquotesingle{}} \OtherTok{=} \FunctionTok{mean}\NormalTok{(lifeExp)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r}
\hline
continent & Average GDP per capita & Average life expectance\\
\hline
Africa & 3089.033 & 54.80604\\
\hline
Americas & 11003.032 & 73.60812\\
\hline
Asia & 12473.027 & 70.72848\\
\hline
Europe & 25054.482 & 77.64860\\
\hline
Oceania & 29810.188 & 80.71950\\
\hline
\end{tabular}

Pretty powerful! Also, notice how the values in the table are reported to a fairly useless degree of precision because I did not specify \texttt{digits=0} inside of the \texttt{kable} function.

You can also use multiple grouping variables. Suppose I wanted these summary stats for each continent each year since 1997. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(year }\SpecialCharTok{\textgreater{}=} \DecValTok{1997}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(continent, year) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\StringTok{\textquotesingle{}Average GDP per capita\textquotesingle{}} \OtherTok{=} \FunctionTok{mean}\NormalTok{(gdpPercap), }
            \StringTok{\textquotesingle{}Average life expectance\textquotesingle{}} \OtherTok{=} \FunctionTok{mean}\NormalTok{(lifeExp)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{digits=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r}
\hline
continent & year & Average GDP per capita & Average life expectance\\
\hline
Africa & 1997 & 2379 & 54\\
\hline
Africa & 2002 & 2599 & 53\\
\hline
Africa & 2007 & 3089 & 55\\
\hline
Americas & 1997 & 8889 & 71\\
\hline
Americas & 2002 & 9288 & 72\\
\hline
Americas & 2007 & 11003 & 74\\
\hline
Asia & 1997 & 9834 & 68\\
\hline
Asia & 2002 & 10174 & 69\\
\hline
Asia & 2007 & 12473 & 71\\
\hline
Europe & 1997 & 19077 & 76\\
\hline
Europe & 2002 & 21712 & 77\\
\hline
Europe & 2007 & 25054 & 78\\
\hline
Oceania & 1997 & 24024 & 78\\
\hline
Oceania & 2002 & 26939 & 80\\
\hline
Oceania & 2007 & 29810 & 81\\
\hline
\end{tabular}

\hypertarget{tidy-verbs}{%
\section{Tidy Verbs}\label{tidy-verbs}}

As with wrangling, one can encounter numerous different tidying scenarios. However, most of the time tidying involves converting a wide dataset to a long dataset. The most common untidy data one encounters is a time series or panel data where each time period is stored across columns (i.e.~wide) rather than down rows (i.e.~long).

Let's begin with a simple time series of population taken from the \texttt{gapminder} data. Suppose we downloaded a dataset named \texttt{uspop} for U.S. population.

country

1997

2002

2007

United States

272911760

287675526

301139947

We don't want each year to be a variable. Rather, we want year to be one variable with separate levels/rows for each period. We can achieve this with \texttt{pivot\_longer}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{uspop }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \StringTok{\textquotesingle{}1997\textquotesingle{}}\SpecialCharTok{:}\StringTok{\textquotesingle{}2007\textquotesingle{}}\NormalTok{, }
               \AttributeTok{names\_to =} \StringTok{\textquotesingle{}year\textquotesingle{}}\NormalTok{,}
               \AttributeTok{values\_to =} \StringTok{\textquotesingle{}pop\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{format =} \StringTok{\textquotesingle{}html\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

country

year

pop

United States

1997

272911760

United States

2002

287675526

United States

2007

301139947

Note that \texttt{pivot\_longer} tries to make the code as intuitive as possible using natural language. First, we tell R which columns to pivot, then we tell R to name the new column `year', then we tell R to name the new column with the values for population `pop'.

Suppose we encountered a more difficult wide version of the \texttt{gapminder} data named \texttt{gap\_wide} shown below. This one has multiple variables listed wide for each year.

\hypertarget{htmlwidget-5b9a6b2b38f01f96e463}{}

Tidying \texttt{gap\_wide} will take two steps. First, we can separate the variable names \texttt{pop/lifeExp/gdpPercap} from the numeric year into two columns using \texttt{pivot\_longer}. This will result in a column that contains all three variables that precede the year and a column that contains year. We will also need to name a third column that will contain the values that the pivoted columns contained.

In the code below, I tell R which columns to pivot using \texttt{cols} and to name the two new columns `var' and `year'. I use \texttt{names\_sep} to tell that each of the columns should be separated using the underscore. Then, I give the new column that will contain the values the generic name `value' since this is an temporary column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gap\_long1 }\OtherTok{\textless{}{-}}\NormalTok{ gap\_wide }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =}\NormalTok{ pop\_1997}\SpecialCharTok{:}\NormalTok{gdpPercap\_2007,}
               \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}var\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}year\textquotesingle{}}\NormalTok{),}
               \AttributeTok{names\_sep =} \StringTok{\textquotesingle{}\_\textquotesingle{}}\NormalTok{,}
               \AttributeTok{values\_to =} \StringTok{\textquotesingle{}value\textquotesingle{}}\NormalTok{)}

\NormalTok{DT}\SpecialCharTok{::}\FunctionTok{datatable}\NormalTok{(gap\_long1, }\AttributeTok{rownames =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{options =} \FunctionTok{list}\NormalTok{(}\AttributeTok{pageLength =} \DecValTok{5}\NormalTok{, }\AttributeTok{scrollX=}\NormalTok{T))}
\end{Highlighting}
\end{Shaded}

\hypertarget{htmlwidget-7fc88bba3c0a6a827e77}{}

Now we need to convert the \texttt{var} column to wide using \texttt{pivot\_wider}. This will create new columns for each of the unique values contained in the `var' column. Since there are three unique values, the result will be three new columns. We also need to specify which column contains the values that will be transferred over to the three new columns.

In the code below, I tell R to pivot the `var' column wide and take the values from the `value' column. And voila; we are back to having our original, tidy data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gap\_long2 }\OtherTok{\textless{}{-}}\NormalTok{ gap\_long1 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ var,}
              \AttributeTok{values\_from =}\NormalTok{ value)}

\NormalTok{DT}\SpecialCharTok{::}\FunctionTok{datatable}\NormalTok{(gap\_long2, }\AttributeTok{rownames =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{options =} \FunctionTok{list}\NormalTok{(}\AttributeTok{pageLength =} \DecValTok{5}\NormalTok{, }\AttributeTok{scrollX=}\NormalTok{T))}
\end{Highlighting}
\end{Shaded}

\hypertarget{htmlwidget-12eb49391c984bdcbaff}{}

\hypertarget{appendixC}{%
\chapter{Goodness of Fit}\label{appendixC}}

\begin{quote}
The discussion below is an extension of Chapter \ref{simple-and-multiple-regression}'s coverage of goodness-of-fit.
\end{quote}

Regression draws the \emph{best} line through a set of data points of two or more variables. The best line in this case is the line with a slope and y-intercept that \textbf{minimizes the sum of squared residual} between the set of data points and said line. The procedure used to achieve such a line is called \textbf{ordinary least squares} (OLS). The type of regression covered in this book is sometimes referred to as OLS regression.

Recall that the variance of a variable is the sum of squared deviations from the mean, as depicted in Equation \eqref{eq:variance}. This should sound familiar. Instead of deviations from the mean, fitting the best line in regression concerns the deviations from the regression line, which by definition are the residuals. As with variance, we square the deviations (i.e.~residuals) for the data used to estimate the regression line, then we add these squared deviations together to obtain the sum of squared residual (SSR). Equation \eqref{eq:ssr} shows this process mathematically.

\begin{equation}
SSR=\sum _{i=1}^{n}(y_{i}-\hat{y})^2= (y_{1}-\hat{y})^2+(y_{2}-\hat{y})^2+\cdots +(y_{n}-\hat{y})^2
\label{eq:ssr}
\end{equation}

SSR quantifies the error in our regression and is what regression minimizes when predicting an outcome given the explanatory variables we have chosen to include.

The SSR also provides us what we need to compute the root mean squared error (RMSE). Recall that in order to compute the variance and standard deviation of a variable in Equations \eqref{eq:variance} and \eqref{eq:sd}, respectively, we divide the sum of squared deviations by the number of observations (or \(n-1\)) then take the square root. The SSR is a sum of squared deviations. The deviations in this case represent error. If we divide SSR by the number of observations, we now have the mean of the sum of squared error. Then, if we take the square root, we have the root mean squared error. Note that this is the same process used to obtain the standard deviation. Thus, the RMSE is the regression version of a standard deviation. Just as the standard deviation tells us the average deviation from the mean, the RMSE tells us the average deviation from the regression line, or the average error in our regression.

We can also quantify the extent to which our regression \emph{explains} the outcome. To do so, we need a benchmark against which to compare the reduction in error achieved by our regression. This benchmark is simply the average value of the outcome. If we had no explanatory variables to predict an outcome, the mean provides the typical value of the outcome. If we had to draw a random observation from a variable's distribution, the mean is our best guess of what that observation's value would be if we have no explanatory variables.

Figure \ref{fig:povfedscatter3} adds a reference line of average federal spending to our scatter plot. Note that because average federal spending is a constant number, it does not change as poverty changes; the red line has no slope. Also, note that the red line does slightly worse fitting the data, particularly toward the left and right extremes of poverty. Compared to the red line representing the mean, the data appear to be more centered around our regression line. As a result, our regression line has less error than the mean.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{data-apps-text_files/figure-latex/povfedscatter3-1} 

}

\caption{Federal spending and poverty among U.S. counties}\label{fig:povfedscatter3}
\end{figure}

The mean of the outcome is our benchmark for assessing how much the explanatory variables included in our regression model explains the total variation in the outcome. The difference, if any, between the values our regression predicts, \(\hat{y_i}\), and the mean, \(\bar{y}\), serves as the basis for quantifying the extent to which our regression model explains the total variation in the outcome. Just like with the SSR, we square the difference between each predicted value and the mean, then add them together. The result is called the \textbf{sum of squared explained} (SSE) and is represented mathematically in Equation \eqref{eq:sse}.

\begin{equation}
SSE=\sum _{i=1}^{n}(\hat{y}_{i}-\bar{y})^2= (\hat{y}_{1}-\bar{y})^2+(\hat{y}_{2}-\bar{y})^2+\cdots +(\hat{y}_{n}-\bar{y})^2
\label{eq:sse}
\end{equation}

We now have the sum of squared residuals (SSR) and sum of squared explained (SSE). Together, the SSR and SSE represent the \textbf{sum of squared total} (SST) variation in the outcome \(y\).

\begin{equation}
SST = SSR + SSE
\label{eq:sst}
\end{equation}

Recall that the \(R^2\) measures the percent of total variation in the outcome that is explained by our regression. To calculate any percent we take divide a proportion of the whole divided by the whole (e.g.~\(5/10 = 0.5\) or 50\%). Thus, to obtain the percent of variation in the outcome explained by our regression, we divide the SSE by SST.

\begin{equation}
R^2 = {\frac{SSE}{SST}}
\label{eq:r2}
\end{equation}

The better you understand the mechanics of simple linear regression, the easier it will be to understand the next section and subsequent chapters on regression models because they are mere extensions of this basic model.

\hypertarget{appendixD}{%
\chapter{Survey Sample Size and Weighting}\label{appendixD}}

Appendix C covers the following topics:

\begin{itemize}
\tightlist
\item
  Apply the margin or error or the confidence interval of an estimate when making conclusions
\item
  Compute the sample size required to achieve a desired margin of error or precision around an estimate given the necessary information for such a computation
\item
  Weight survey results given sufficient information
\end{itemize}

\hypertarget{sample-size}{%
\section{Sample size}\label{sample-size}}

Now that we understand how a sample of relatively small size allows us to make inferences about the population with a reasonable degree of confidence, let us next consider how to determine the size of sample we need to achieve a confidence interval with a specific degree of precision.

When President Trump's impeachment inquiry was in progress, numerous polls were conducted to gauge the sentiment of the U.S. electorate as to whether Trump should be impeached. One poll conducted by Fox News, using a sample of 1,000 voters, reported that 51\% of voters support impeaching Trump with a margin of error of 3 percentage points. These results garnered national attention as the first time a majority of U.S. voters supported the impeachment of Trump. Regardless of one's opinion on the matter or whether a national majority persuades elected officials, the claim that a majority of voters supported impeachment based on this poll is dubious.

A sample of U.S. voters was taken. From this sample, the proportion of voters in support of impeachment was calculated to serve as an estimate of the population parameter. This sample produced an estimate of 51 percent. The margin or error in surveys or polls, unless noted otherwise, refers to one-half of the 95\% confidence interval, or roughly two standard errors. Therefore, the 95\% confidence interval of the polling results was 48 to 54 percent. The confidence interval used to capture the unobserved population parameter includes a minority of voters supporting impeachment.

A common mistake made when interpreting estimates and confidence intervals is that the estimate is the most likely value within the confidence interval for the population parameter. \textbf{The population parameter is no more likely to equal the estimate than any other value within the confidence interval}. A confidence interval either captures the population parameter or it does not. Therefore, it was just as likely that 48 percent of voters supported the impeachment as it was 54 percent did or any percentage in between.

As long as our estimate is unbiased, we cannot influence its value. Any attempt to do so would be bias by definition. Thus, the pollsters were stuck with an estimate of 51 percent. The estimate of 51 percent was not the issue for making the conclusion that a majority of voters supported impeachment. The issue was the critical lack of precision around the estimate. Unlike the estimate, we \emph{can} influence the precision of the confidence interval.

How many voters would the pollsters have had to survey in order to achieve a margin of error of 1 percentage point and conclude a majority of voters support impeachment?

If the outcome is dichotomous or binary, such as whether or not a respondent supports impeachment, then the equation for determining the desired sample size is as follows

\begin{equation}
n=p(1-p)({\frac{Z}{E}})^2
\label{eq:sampsizeprop}
\end{equation}

\begin{itemize}
\tightlist
\item
  n is the sample size
\item
  p is the proportion of yes/true/success
\item
  Z is the number of standard deviations we set according to what confidence interval is desired
\item
  E is the desired margin of error
\end{itemize}

It is important to point out that the calculation in Equation \eqref{eq:sampsizeprop} occurs \emph{prior} to the poll. Therefore, we do not know the value of \(p\). Unless there is reason to expect \(p\) to equal a particular proportion, it is customary to input 0.50. If we want to use a 95\% confidence interval then we input 2 for \(Z\). Lastly, we replace \(E\) with how far we want each side of our chosen confidence interval to be below and above our estimate.

Suppose the primary purpose of the Fox News poll was to conclude a majority opinion. Then, a margin of error equal to 1 percentage point would allow a valid conclusion that a majority of voters support or oppose impeachment if the result of the poll were slightly below 49\% or above 51\% in favor. Choosing to use a 95\% confidence interval, then the sample size necessary to achieve a margin of error equal to 1 percentage point (0.01) is

\begin{equation}
n=0.5(1-0.5)({\frac{1.96}{0.01}})^2\\
n=9,604
\end{equation}

which is likely impractical for the kind of polling that news organizations tend to conduct.

If we wish to determine the sample size needed for estimating a 95\% confidence interval within a certain distance from a continuous estimate, such as the mean, we can use the following equation

\begin{equation}
n=(\frac{sZ}{E})^2
\label{eq:sampsizemean}
\end{equation}

where \(s\) is the sample standard deviation. Again, we do not have the sample standard deviation prior to obtaining the sample. We must rely on past analyses of the variable in question or a pilot study with a small sample in order to input the sample standard deviation.

\hypertarget{survey-weights}{%
\section{Survey weights}\label{survey-weights}}

Ideally, the demographic composition of survey respondents should match the demographic composition of the survey's intended population. Thanks to Census data, we have a reasonably accurate understanding of population demographics such as age, sex, race, and ethnicity for multiple geographic areas and government jurisdictions. Other organizations like Pew Research Center and Gallup provide population proportions of other various ways to form groups, such as political party or religious affiliation.

Unfortunately, it is unlikely for the demographic composition of survey respondents to match the population. Recipients choose not to respond and surveys tend to reach some demographics disproportionately more than others. This results in over- or under-representation of certain demographic groups in our survey, which limits our ability to generalize survey results and threatens the internal validity of any estimate.

Weighting is a way to correct for a demographic mismatch between the composition of respondents and the intended population. There are multiple methods of weighting, some of which are complex, but the basic method described below can work for most cases where maximum correction is not necessary or feasible.

Suppose a poll targeted to the general U.S. public asked if workers who have illegally entered the U.S. should be 1) allowed to keep their jobs and apply for citizenship, 2) allowed to keep their jobs as temporary guest workers but not allowed to apply for citizenship, and 3) lose their jobs and have to leave the country. The poll also asked for political party affiliation. A total of 890 responses were collected, generating the following results.

\begin{table}

\caption{\label{tab:unnamed-chunk-266}Illegal immigration poll results}
\centering
\begin{tabular}[t]{l|l|l}
\hline
  &                  response &         party\\
\hline
 & Apply for citizenship:278 & Republican :357\\
\hline
 & Guest worker         :262 & Democrat   :174\\
\hline
 & Leave the country    :350 & Independent:359\\
\hline
\end{tabular}
\end{table}

Based on the results, a plurality of 39\% of the U.S. public believes illegal immigrants should leave the country and 31\% believe they should be allowed to apply for citizenship. The question these estimates are biased by the composition of political party affiliation. About 40\% of the respondents are Republican and Independent, while about 20\% are Democrat. Suppose we find a national survey reporting that the U.S. is 30\% Republican, 36\% Independent, and 31\% Democrat. Therefore, Republicans and Independents are over-represented in our survey, while Democrats are under-represented. We need to correct for this using weights.

To calculate weights, we can use the following formula.

\begin{equation}
Weight = \frac{Population}{Sample}
\label{eq:weight}
\end{equation}

Using Equation \eqref{eq:weight}, we obtain the following weights for our survey

\begin{itemize}
\tightlist
\item
  Republican: 30/40 = 0.75
\item
  Independent: 36/40 = 0.9
\item
  Democrats: 31/20 = 1.55
\end{itemize}

These weights mean that each Republican response counts as only three-quarters of a response and each Democrat response counts as about 1.5 responses.

Next, we need to tabulate how many of each response was made by the three parties.

\begin{table}

\caption{\label{tab:unnamed-chunk-267}Response by political party}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & Republican & Democrat & Independent\\
\hline
Apply for citizenship & 57 & 101 & 120\\
\hline
Guest worker & 121 & 28 & 113\\
\hline
Leave the country & 179 & 45 & 126\\
\hline
\end{tabular}
\end{table}

Then, we multiply the values by their corresponding weight. For example, applying the weight for Republicans results in 134 responses for ``Leave the country'' (\(179 \times 0.75\)). This process gives us the following counts

\begin{table}

\caption{\label{tab:unnamed-chunk-269}Weighted survey counts}
\centering
\begin{tabular}[t]{l|l|r|r|r}
\hline
party & response & total & weight & w.total\\
\hline
Republican & Apply for citizenship & 57 & 0.75 & 43\\
\hline
Republican & Guest worker & 121 & 0.75 & 91\\
\hline
Republican & Leave the country & 179 & 0.75 & 134\\
\hline
Democrat & Apply for citizenship & 101 & 1.55 & 157\\
\hline
Democrat & Guest worker & 28 & 1.55 & 43\\
\hline
Democrat & Leave the country & 45 & 1.55 & 70\\
\hline
Independent & Apply for citizenship & 120 & 0.90 & 108\\
\hline
Independent & Guest worker & 113 & 0.90 & 102\\
\hline
Independent & Leave the country & 126 & 0.90 & 113\\
\hline
\end{tabular}
\end{table}

According to our weighted counts, 36\% of the U.S. believes illegal immigrants should leave the country, while 35\% believe they should be allowed to apply for citizenship. Weighting has changed a 8 percentage point gap in these two responses to a 1 point gap.

In case it was not obvious in the example, we have to ask survey recipients to provide the demographic information upon which we plan to weight. Survey administrators must consider what variables might bias the response(s) of interest if there is a mismatch between the sample and population. Wisely, the designers of the survey above suspected if a disproportionate number of Republicans or any other political party responded, this would bias their estimates. Presumably, race and ethnicity are correlated with this response, but we do not have this information to construct a weight.

\backmatter
\printindex

\end{document}
