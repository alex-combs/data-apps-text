[
["index.html", "Data Applications in Public Administration Using R to Learn Concepts and Skills Preface", " Data Applications in Public Administration Using R to Learn Concepts and Skills Alex Combs Last updated on 07 November 2020 Preface Disclaimer This is a companion resource for PADP 7120: Data Applications in Public Administration. This resource is not peer-reviewed nor is it intended to supplant published textbooks. It is meant for distribution only to those enrolled in PADP 7120. I chose not to adopt a textbook for the course and offer a written version of lecture and presentation notes instead. I do not benefit monetarily from this resource in any way. This resource was developed primarily out of uncertainty regarding the future of in-person vs. online teaching. If in-person instruction becomes untenable, then this resource provides an imperfect substitute for the time in class spent converting content from other textbooks into what I want and expect students to know. Regardless of what happens to instruction during class, this book provides a one-way lesson on relevant topics directly from the person in charge of evaluating your understanding and performance. Secondarily, I have harbored some disappointment with existing texts used to teach MPA students statistics and statistical software for several years. Curating sections and subsections of chapters from numerous sources in order to provide partially relevant information presented with inconsistent levels of rigor has proven counterproductive to teaching and learning. This book aims to provide a standalone resource that is appropriate and relevant for students in PADP 7120. Objective The objective of this book is to help students in public administration be as competitive as possible in their desired job markets via competency in statistics and statistical software. It aims to simultaneously teach students key concepts in statistics and applications of those concepts using R. This book is intended for students with minimal background in statistics or interest in pursuing a career in academic research. For students wanting to learn core statistical concepts and skills that are applicable to careers in public, non-profit, and health sectors with minimal need to sift through excessively theoretical or technical material, this book was developed with you in mind. Style and Structure Since this book is based on my lecture notes, most of the material is presented in a conversational tone. I have basically taken the core pieces of what I would say in class, adding examples and other supporting material so the content is self-contained. Rather than provide thorough coverage of complex statistical concepts, I take some liberties in presenting stylized facts for the benefit of the reader. When using statistical software, there are multiple options to achieve an intended outcome. I provide what I consider or understand to be the best option. This book is organized along two parallel tracks. The first track covers statistical concepts and is self-contained. The second track applies the concepts in the first track using R. The chapters in the applied track are referred to as R chapters, each of which corresponds to a conceptual chapter in the first track. For example, the R Data chapter corresponds to the Data chapter in the first track. The conceptual track is divided into four sections: Data and description Regression models Inference Advanced topics Software requirements This book provides examples and exercises using R. Students who intend to use a personal computer must download and install the following software: R RStudio Supplemental resources There are numerous free materials that teach statistics and R. The below list includes a few books and websites that offer broader and deeper treatments of some concepts and skills covered in this book. OpenIntro Statistics by David Diez, Mine Cetinkaya-Rundel, and Christopher Barr Quantitative Research Methods for Political Science, Public Policy and Public Administration (With Applications in R) - 3rd Edition by Hank Jenkins-Smith and Joseph Ripberger R for Data Science by Garrett Grolemund and Hadley Wickham Statistical Inference via Data Science by Chester Ismay and Albert Y. Kim Kahn Academy: Statistics and probability Data Visualization: A practical introduction by Kieran Healy R Markdown from RStudio R Markdown: The Definitive Guide by Yihui Xie, J.J. Allaire, and Garrett Grolemund Forecasting: Principles and Practice by Rob J. Hyndman and George Athanasopoulos "],
["1-intro.html", " 1 Introduction 1.1 Why statistics 1.2 Professional standards 1.3 Statistics in PA 1.4 Using R", " 1 Introduction “Data, data everywhere, and not a thought to think.” —John Allen Paulos 1.1 Why statistics Statistics converts raw information (i.e. data) into something useful. If we want to make evidence-based decisions, we need statistics. If we want to allow ourselves to be misled by nefarious or mistaken analyses of data, we should resist learning statistics. 1.2 Professional standards The Network of Schools of Public Policy, Affairs, and Administration (NASPAA) is the accrediting authority for MPA programs. NASPAA promotes the following universal competencies: to lead and manage in the public interest; to participate in, and contribute to, the policy process; to analyze, synthesize, think critically, solve problems and make evidence-informed decisions in a complex and dynamic environment; to articulate, apply, and advance a public service perspective; to communicate and interact productively and in culturally responsive ways with a diverse and changing workforce and society at large. Statistics will help you develop all of the above competencies. You would not sufficiently possess one or more of the above competencies without knowledge and skills in statistics. 1.3 Statistics in PA The use of statistics is ubiquitous in public administration. Agencies and nonprofits use statistics to describe their clients and assess their needs. Agencies like the Government Accountability Office and watchdog organizations use statistics to monitor performance and guard against fraud. Service-oriented organizations like schools and hospitals use statistics to evaluate services and communicate to stakeholders. The Congressional Budget Office, Office of Management and Budget, and employees at every level of government use statistics to assess finances and forecast trends. 1.4 Using R Before moving forward with this book, you need to learn how to operate R at a very basic level. The goal of this book is not to train you to become an expert in R or even a data scientist or analyst. Rather, the goal is to train you enough so R becomes a legitimate alternative to inferior spreadsheet software like Excel to perform tasks that may be expected of a master in public administration. MPA students may be reluctant to learn something referred to as a statistical computing language and its relevancy to their career goals may not be clear. I firmly believe that not training you to use statistical software in a course such as this would be doing you a disservice. Demand for those competent in statistical software like R continues to rise. Even if you plan to pursue a managerial role with minimal analytic tasks, chances are high that you will supervise or work with those who conduct analyses, and you will need to interpret their findings, applying your own managerial or subject matter expertise toward making an evidence-based decision. People in both roles–consumers and producers of statistical analyses–need to be able to communicate with the other. The best way to become a competent consumer of statistical information is to learn the basics of producing it. In addition, R is free, comes with many free educational resources, and is popular across many disciplines. If you study statistics and data applications for a semester, you might as well spend part of that semester learning software like R. There is only upside in doing so with respect to employment prospects. For a brief orientation to R, proceed to Chapter 16. "],
["2-data.html", " 2 Data 2.1 Learning objectives 2.2 Rectangular Data 2.3 Types of variables 2.4 Dataset structures 2.5 Key terms and concepts", " 2 Data Nothing exists except atoms and empty space; everything else is opinion. —Democritus We cannot effectively convert the raw material of knowledge into a useful product without first understanding the raw material. Therefore, learning statistics naturally begins with learning the types and structures of data. 2.1 Learning objectives Understand the organization of rectangular data Identify the unit of analysis within a dataset Identify and distinguish types of variables Identify and distinguish types of dataset structures 2.2 Rectangular Data Most data are rectangular, often represented using a standard spreadsheet organized by rows and columns. A rectangle of data is commonly referred to as a dataset. Table 2.1: Generic rectangular data ID Variable_1 Variable_2 Unit of Analysis Datum Datum Unit of Analysis Datum Datum Unit of Analysis Datum Datum A rectangular dataset has three components. Not all datasets will fit the below description because many datasets are not organized in a tidy manner. The elements of tidy data will be covered in a later chapter. Unit of analysis or observation: The generic entity or subject a row of data refers to. The unit of analysis uniquely identifies each row of a dataset. If we have a dataset of 50 states and some variables measured in 2020, then our unit of analysis is states. If you were told a specific state, then you could find the row in the dataset. If we have 50 states measured in 2019 and 2020, then the unit of analysis is state-year because you will need to know the state and year to find a specific row. Variable: A measured characteristic of the unit of analysis. State unemployment rate is a variable for a state unit of analysis. Datum: The intersection of a variable (column) and a unit of analysis (row) resulting in a cell. The datum is a particular piece of information. A cell could contain something like 4.8 as the unemployment rate for Georgia in 2020. 2.3 Types of variables The variables in a given dataset can be of several types. Types of variables are important to learn because the types of variables one is dealing with has consequences for data applications, such as description, visualization, and inference. A variable provides us raw information about the units of analysis. If statistics is a discipline to convert raw information into something useful, then it stands to reason that we should want to know what type of information a variable provides us, especially the specificity of that information. For example, suppose you ask two strangers to report their annual income. What options do they have for answers? If virtually any value, then you know to a precise degree the income each earns and can compute the precise difference between the two incomes. What if their choices are either more or less than $50,000? Then, you have a coarse understanding of how much they earn. If they provide different answers, you can only conclude whether one makes more than the other but not by how much. If they provide the same answer, then the two are grouped together even though it is highly unlikely they earn equal incomes. This makes a serious difference for statistical analysis. Figure 2.1: Variable Types All variables belong to one of two broad types: qualitative (or categorical) and quantitative (or numeric). Qualitative variables take on values that have no intrinsic numerical meaning. They are expressed in words. Quantitative variables take on values that do have intrinsic numerical meaning. 2.3.1 Qualitative variables Qualitative variables can be further differentiated into two types: nominal and ordinal. Nominal variables take on values that differ in name only. Ordinal variables take on values that can be ranked relative to each other but the difference between rankings has no numerical value. The values that categorical variables take on are commonly referred to as levels. Categorical variables can contain virtually any number of levels, though the number of levels is usually limited. A variable such as sex contains two levels: male and female. The variable sex is nominal, as its values have no numerical meaning and the two levels have no ranking. Race, state, country, political party, and any variable coded as yes/no such as unemployed, married, and below the federal poverty line are all examples of nominal variables. If you have ever participated in a customer satisfaction survey, then you have almost surely contributed data to an ordinal variable. Those scales that provide some number of options from “disagree” to “agree” are called Likert scales. Your answer has no intrinsic numerical value but it can be ranked against the answers of others. One respondent can be said to be more satisfied than another but not by how much. Moreover, one can only trust the results insofar as respondents have the same understanding or frame of reference–the service that satisfied one respondent may not have satisfied another. Other ordinal variables, such as education degree and income level do not have this issue. 2.3.2 Quantitative variables Quantitative variables can be further differentiated into two types: discrete and continuous. Discrete variables take on countable or indivisible values. Continuous variables take on infinitely divisible values (at least in theory). The distinction between discrete and continuous can be more difficult to discern but also less consequential for analysis. It is often the case that analytical models treat the two variables the same. However, for a purpose such as data visualization, the distinction can be informative. Any variable that is a count of persons, places, events, or things is a discrete variable, usually taking on integer values (e.g. 0, 1, 2, 3,…). By contrast, a continuous variable can contain values with an infinite number of decimal places. Even so, continuous variables take on a limited number of decimal places because either we measure phenomena with finite precision or it simply becomes impractical to include so many decimal places. 2.3.3 Index variables Index variables are continuous variables but warrant separate discussion. An index variable is a composite measure of multiple variables. They can be used to make a continuous variable out of multiple categorical variables or simplify multiple quantitative variables into one. Purposes such as ranking colleges, measuring poverty beyond income, and determining political ideology make use of index variables. Index variables mask underlying information. This can be helpful or harmful. In either case, it is important to consider how an index variable is constructed. Doing so can offer insight or uncover problems. An instructive example familiar to readers is college rankings. U.S. News and World Report describes how rankings are determined. What makes a college good? According to these rankings, five percent of what makes a college good is the percent of undergraduate alumni giving a donation as a proxy of student satisfaction. Another 20% is based on the opinions of administrators at peer institutions. Are these choices wise? This is difficult to say and besides the point. The point is that index variables involve choices made by people and are not naturally occurring data. They are synthetic materials of knowledge and worthy of our discernment. 2.4 Dataset structures Just as the type of variable one is dealing with impacts the kinds of visualizations or analyses one should use, so too does the structure of a dataset. Datasets come in three varieties depending on their unit of analysis. Cross-sectional Pooled cross-sectional Time series Panel or longitudinal Cross-sectional data is a snapshot in time measuring some size sample of units. One column serves as the identifier of the unit of analysis, such as the name or ID number of the unit. Notice in Table 2.2 that all one needs to know is the country in order to identify a specific row. Table 2.2: Cross-section example country continent year lifeExp pop gdpPercap Argentina Americas 2007 75.320 40301927 12779.380 Bolivia Americas 2007 65.554 9119152 3822.137 Brazil Americas 2007 72.390 190010647 9065.801 Pooled cross-sectional data could be considered a fourth structure but is simply multiple cross-sections stacked atop each other. The critical quality of pooled cross-sectional data is that each cross-section contains different units measured at different times, not the same units measured at different times. Notice in Table 2.3 that the countries included from 2002 are not the same as those included from 2007. Table 2.3: Pooled cross-section example country continent year lifeExp pop gdpPercap Algeria Africa 2002 70.994 31287142 5288.040 Angola Africa 2002 41.003 10866106 2773.287 Benin Africa 2002 54.406 7026113 1372.878 Botswana Africa 2002 46.634 1630347 11003.605 Argentina Americas 2007 75.320 40301927 12779.380 Bolivia Americas 2007 65.554 9119152 3822.137 Brazil Americas 2007 72.390 190010647 9065.801 Time series data measures one unit over multiple time periods. The unit of analysis in time series data is time, as it uniquely identifies each row. Notice in Table 2.4 that one country is tracked over multiple years. Table 2.4: Time series example country continent year lifeExp pop gdpPercap Argentina Americas 1977 68.481 26983828 10079.027 Argentina Americas 1982 69.942 29341374 8997.897 Argentina Americas 1987 70.774 31620918 9139.671 Argentina Americas 1992 71.868 33958947 9308.419 Argentina Americas 1997 73.275 36203463 10967.282 Argentina Americas 2002 74.340 38331121 8797.641 Argentina Americas 2007 75.320 40301927 12779.380 Panel (or longitudinal) data measures the same units over multiple time periods. The unit of analysis is pair of unit and time period. Notice in Table 2.5 that in order to identify a specific row, you would need to know the country and year. One could also think of panel data as numerous time series. Table 2.5: Panel example country continent year lifeExp pop gdpPercap Argentina Americas 1997 73.275 36203463 10967.282 Argentina Americas 2002 74.340 38331121 8797.641 Argentina Americas 2007 75.320 40301927 12779.380 Bolivia Americas 1997 62.050 7693188 3326.143 Bolivia Americas 2002 63.883 8445134 3413.263 Bolivia Americas 2007 65.554 9119152 3822.137 To learn how to examine data in R, proceed to Chapter 17. 2.5 Key terms and concepts Unit of analysis Variable Types of variables: qualitative, quantitative, nominal, ordinal, discrete, continuous, index Data structures: cross-sectional, pooled cross-sectional, time series, panel "],
["3-measurement-and-missing.html", " 3 Measurement and Missing 3.1 Learning objectives 3.2 Credible analysis 3.3 Missing data 3.4 Key terms and concepts", " 3 Measurement and Missing \"Will he not fancy that the shadows which he formerly saw are truer than the objects which are now shown to him? —Plato Once we know the types and structure of our data, we need to understand how the data relates to reality before drawing conclusions from it. Variables and the data they contain are measured representations of reality. They are shadows on the allegorical cave discussed in Plato’s Republic. We should not immediately assume these shadows are good representations of reality. 3.1 Learning objectives Assess the measurement validity of variables Assess the measurement reliability of variables Explain the difference between accuracy and precision 3.2 Credible analysis Measurement validity and reliability are the foundations of credible analysis, the components of which are depicted in Figure 3.1. Without the two, we have little or no basis to make conclusions from data. Figure 3.1: Components of credible analysis This book will address the remaining building blocks in subsequent chapters. 3.2.1 Measurement validity &amp; reliability The following discussion of measurement validity and reliability pertains mostly to data we did not generate ourselves via a survey or other instrument, but rather data generated by someone else that we may want to use for our purposes (i.e. administrative data). When we set out to measure a concept ourselves, it affords us the chance to consider additional aspects of validity and reliability that are not covered here. Measurement Validity: Does the variable accurately represent what it claims to represent? Are the values accurate representations of the intended concept/phenomenon? Measurement Reliability: Does the way the variable is measured generate the same value given the same reality? Given two real and identical conditions, will my data contain identical values? One way to visually represent the concepts of measurement validity and reliability is with the concentric circles of a target or a dart board. At the center of the target is the true concept of interest represented by a variable. At the center of a yes/no variable such as poverty is perhaps economic stress or eligibility for means tested government welfare programs. The proximity of a variable’s measurement to the center concept is the variable’s measurement validity. In addition to its proximity to the center of the target, there is also the issue of whether, given the same condition, repeated measures measures will result in the same value. If so, then the data points on the target should be clustered in close proximity. If two cars are speeding in different towns at 80 mph, should we think a dataset recording instances of speeding would report these two cars differently? If not, then we believe the variable to have measurement reliability. If we think the two towns procedures or equipment result in different speeds for two cars traveling at the same speed, then we believe the variable to be unreliable. The combination of validity and reliability presents four scenarios depicted in Figure 3.2 below. Figure 3.2: Representation of measurement validity and reliability Let us consider an example for each of the four combinations of measurement validity and reliability. First, consider a before-tax earnings as a measure of labor income. Provided payments are not made under the table, this measure should be valid, as its purpose is to quantify labor income for official government use. As well, the collection of earnings data is reliable; two individuals with equal labor income can be expected to have equal before-tax income reported on their W-2. What if we used self-reported income as a measure of labor income? Self-reported income is unlikely to be a valid measure because a non-trivial number of respondents may be inclined to provide inflated answers. Nor is there reason to expect exaggerations to follow a mathematical formula that results in equally invalid response given equal true income. Therefore, self-reported income is also an unreliable measure of income. How about before-tax earnings as a measure of total wealth? This measure is likely to be invalid because wealth includes assets like property and investments. Gross earnings is arguably a reliable measure because it at least captures income in a reliable fashion, and two individuals with equal income might be expected to have somewhat similar levels of wealth, though it is possible for those with high wealth to strategically lower their income. Lastly, what if we used income as a percentage of the federal poverty line (FPL) as a measure of poverty? Depending on how we define poverty, income relative to FPL may or may not be a valid measure. Let us suppose households earning 100% of the FPL are highly likely to be impoverished and in need of assistance. In this case, we have a valid measure. However, two households of equal impoverishment are unlikely to have equal income relative to the FPL. Costs of living differ across regions, jobs provide various levels of health coverage, and households have various needs with respect to medicine or nutrition. Therefore, this may not be a reliable measure. Key to this measure remaining valid is that the lack of reliability does not result in systematic over- or under-reporting of poverty. 3.2.1.1 Why this matters Let us consider another example where an agency needs to allocate resources to state governments according to the number of persons who are homeless in each state. Suppose the true count of homeless persons for two states is the same. The four combinations of measurement validity and reliability are depicted in Figure 3.3. Figure 3.3: Comparing measurement validity and reliability Though techniques to count the number of homeless persons are improving, one way to count has been to designate a specific day of the year (January 1st) where staff and volunteers conduct a census of homeless people. As a measure, this is known to be invalid and unreliable. In the case where a valid and reliable measure is taken, the two states receive equal and appropriate amounts of resources. If an invalid and reliable measure is used, the two states receive equal resources, but the amount of resources is less (or more) than what it should be. If a valid and unreliable measure is used, on average, the amount of resources provided is appropriate, but the two states receive different amounts. If an invalid and unreliable measure is used, the two states receive different amounts and the amount of resources provided is systematically less (or more) than what it should be. The moral of this story is that when you collect data you did not generate yourself for your own purpose, take the time to consider if those data are valid and reliable measures for your intended purpose and what the consequences could be if they are not. 3.3 Missing data It is not uncommon to encounter missing values in a data. Respondents skip or choose not to answer survey questions, administrators fail to contact respondents, entities that reported data last year may have dissolved or consolidated with another entity this year. Many reasons can lead to missing data. The key is to consider why data are missing and if it should affect your conclusions. Using the previous example of self-reported income, suppose there are numerous missing values in the responses. Should we assume they are missing at random, or that there is some underlying reason or pattern? Perhaps those with no or low income do not wish to report. If we were to dismiss these missing values, and draw conclusions from the non-missing values, we may severely overestimate the income of the target population. 3.3.1 Types of missing data Missing data come in two flavors: Explicit: data that we can see are missing in the data; cells containing a value that denotes missing Implicit: data that we would expect to be included based on data structure but are not; no obvious sign of missing Table 3.1 shows an example of data that are explicitly missing denoted by NA. Missing data is denoted in a variety of ways. For example, instead of NA, the cells could have been left empty, or filled with a period, or some other symbol. If data were obtained from an organization that regularly produces publicly available data, datasets are usually accompanied by a legend that explains what symbols denote missing. Table 3.1: Example of explicitly missing data country continent year lifeExp pop gdpPercap Argentina Americas 2007 NA 40301927 12779.380 Bolivia Americas 2007 65.554 9119152 NA Brazil Americas 2007 72.390 190010647 9065.801 Beware ambiguous missing values. For instance, some survey questions are dependent on previous questions. You do not want to conclude that a value is missing because a respondent chose not to answer when they were never asked the question. Or perhaps a value is missing when it should actually equal 0 or vice versa. If missing data are consequential to your analysis, then you may need to investigate further into how the data were collected or coded in order to eliminate such ambiguity. Table 3.2 shows an example of implicitly missing data. Argentina is observed in 1997, 2002, and 2007, but Bolivia is observed only in 1997 and 2007. What happened to the 2002 observation for Bolivia? This sort of entry and exit from the dataset is common in panel data where the same units are observed over multiple time periods. Table 3.2: Example of implicitly missing data country continent year lifeExp pop gdpPercap Argentina Americas 1997 73.275 36203463 10967.282 Argentina Americas 2002 74.340 38331121 8797.641 Argentina Americas 2007 75.320 40301927 12779.380 Bolivia Americas 1997 62.050 7693188 3326.143 Bolivia Americas 2007 65.554 9119152 3822.137 Note that the missing Bolivia observation was easy to spot because the dataset is extremely small. If we were dealing with a large dataset, this would not have been so obvious. A quick way to check whether there may be implicitly missing observations is to check the number of observations in your data. If you are under the impression that your data contains all 50 states for 10 years, then you should have 500 observations. If not, some states or years must be missing. To learn how to work with missing data in R, proceed to Chapter 18. 3.4 Key terms and concepts Measurement validity Measurement reliability Measurement precision Implicitly missing data Explicitly missing data "],
["4-descriptive-statistics.html", " 4 Descriptive Statistics 4.1 Learning objectives 4.2 Two kinds of statistics 4.3 Distributions 4.4 Descriptive Measures 4.5 Key terms and concepts", " 4 Descriptive Statistics \"Just the facts, ma’am. —Joe Friday, Dragnet 4.1 Learning objectives Explain how descriptive and inferential statistics differ in purpose and the information they provide Identify and differentiate population, sample, parameter, statistic in a given research proposal or question Explain what is a distribution of a random variable Recall the descriptive measures of center, spread, and association Choose the preferable measures of center and spread given a distribution and explain their strengths and weaknesses Determine the direction and strength of association given a scatterplot or correlation coefficient Explain the possible shortcomings of correlation 4.2 Two kinds of statistics There are two kinds of statistics, each having a specific goal: Descriptive statistics summarizes the qualities of observed data, typically describing distributions of variables or the relationship between two variables. Inferential statistics uses observed data in a sample to make inferences/conclusions about an unobserved population. Descriptive statistics concerns just the facts. Inferential statistics uses those facts to make educated, scientific guesses about a group of people, places, or things for which we don’t have data. The above definitions include some terms that warrant further explanation. Population: all members of a specified group pertaining to a research question Sample: a subset of that population In many cases, we cannot study an entire population because of logistics or cost. Instead, we take a sample of that population to make inferences about it. Sometimes our population is small or accessible enough to observe. If I wanted to know the average GPA of students in my class, my population is the students in my class, and I could compute the exact average for the entire population. Or if I worked in HR for an agency and wanted to know the racial diversity of a department, I could calculate percentages of each race for the population. We can describe a sample or a population. Inference is specifically using a sample to describe a population. When we compute measures of a population or sample, these measures have specific names: Parameter: a measure pertaining to a population Statistic: a measure pertaining to a sample If my population is students in my class, and I compute the average GPA for all of the students in my class, that measure is a population parameter. If my population is all students at a university, and I use the students in my class as a sample, then the average GPA of the students in my class is a sample statistic. In inference, a statistic is often referred to as an estimate because it is used to estimate a population parameter. The parameter in this example would be the average GPA of all students at the university. 4.3 Distributions The goal of descriptive statistics is to summarize characteristics of variable distributions. Before reviewing the measures used to summarize distributions, we should understand what a distribution is. A distribution tells us the (possible) values of a variable and the frequency at which those values occur. The values of a variable are the result of some random data-generating process. If it wasn’t random, and instead deterministic, then there would be no uncertainty in the world. You do not know if you will get a job that requires your degree before you get the degree. An HR department does not know if an implicit bias workshop will reduce the number of racial insensitivity complaints before providing the workshop and measuring the number of complaints, nor does it know how many complaints occur at all before they are made. All of these are variables with some range of possible values, each of which occurs at some frequency. These frequencies are revealed to us when we take measures of the variable. Sometimes we know all the possible values of a variable, or at least the range of possible values. We know a variable for biological sex has possible values of male or female. We know a variable for GPA has a possible range of 0 to 4, in most cases. Sometimes we know what the frequency of values for a variable should be. Genetics tells us to expect 50% males and females. Most of the time we don’t know the function that determines frequency, or it is too complex. For example, we have some idea of the factors that influence GPAs, but there will always be some randomness that goes unaccounted. This somewhat esoteric exposition underlies the main focus here: the distribution of a variable. To make this as concrete as possible, let’s consider a variable of something that is simple and familiar to all of us: a roll of a six-sided die. We know a roll of a six-sided die can take on a range of integers between 1 and 6. We also know the frequency of each value is the same at 1 in 6, or about 17%. Therefore, we know the distribution of this variable, which is depicted below in Figure 4.1. Figure 4.1: Probability distribution of a six-sided die Therefore, if we were to roll the die six times, we would expect the following data in Table 4.1. Table 4.1: Expected results of 6 rolls roll value 1 1 2 2 3 3 4 4 5 5 6 6 And we could represent this distribution by counting the number of times each value occurred using a histogram as shown in Figure 4.2. Figure 4.2: Expected distribution of 6 rolls Of course, this is just what is expected to happen, on average, given many rolls of a die. Anyone who has played a board game knows streaks can occur. Given a number or rolls, we probably will not observe a uniform number of values. Suppose we roll 12 times and record the value of each roll, as is shown in Table 4.2. Table 4.2: Observed results of 12 rolls roll value 1 5 2 2 3 4 4 2 5 5 6 1 7 1 8 4 9 3 10 2 11 5 12 1 We can visualize the distribution of these 12 rolls, as is done in Figure 4.3. Figure 4.3: Observed distribution of 12 die rolls Here we can see the randomness of the variable. Values 1, 2, and 5 occur more frequently than 3 and 4, and 6 does not occur at all. If we were to roll the die many more times, it would look more like the distribution we would expect. But for this sample of die rolls, the distribution is unique. This is exactly the point of descriptive statistics: whether or not we know what to expect in terms of a variable’s distribution, we want to know the characteristics of the distribution for a variable from a particular sample or population. When we ask for, say, a variable’s average, we are asking for the approximate midpoint of that variable’s distribution. Descriptive measures help us summarize characteristics of distributions and some serve as the building blocks for other descriptive measures as well as inferential statistics. 4.4 Descriptive Measures A die roll is uninteresting and unimportant. In our review of descriptive measures, let us consider them with respect to the distribution of the infant mortality rate across 222 countries in 2012. Infant mortality is the number deaths of infants under one year old per 1,000 live births. It is used as a measure of health in a country. Figure 4.4: Infant Mortality Rates We could simply show this distribution, but it is usually helpful to summarize key characteristics of it, as doing so help answer some specific questions. Distributions are typically described in one of three ways: Center: what is the typical value of this variable? Spread: how far away are values typically from the center? Association: what is the typical value or spread of the distribution given a value of another variable? The three types of descriptive measures above are defined using questions because there are multiple options for each, and which one is more appropriate to use depends on which one answers the question best given the shape of the distribution. 4.4.1 Measures of center Mean The mean or average takes the values of a variable, adds them, then divides that sum by the total count of values. \\[\\begin{equation} {\\displaystyle \\bar{x}={\\frac {1}{n}}\\sum _{i=1}^{n}x_{i}={\\frac {x_{1}+x_{2}+\\cdots +x_{n}}{n}}} \\tag{4.1} \\end{equation}\\] Infant mortality has 222 values, so \\(n\\) in equation (4.1) above would equal 222 in this case. If we were to pluck one country out of our pool of 222 countries at random, the mean tells us the infant mortality rate to expect. In other words, the mean tells us the typical infant mortality rate given our observed data. In this case, the average infant mortality rate is 26.7. Median If we took the 222 rates and listed them in ascending or descending numerical order, the median is the value that values exactly in the middle of that list. The median is also referred to as the 50th percentile because half of the values fall below it and half of the values fall above it. In the case of an even number of values, there is no naturally occurring middle value. In that case, we take the average of the two values in the middle. The median infant mortality rate is 15.6. Mode The mode is the value that occurs most frequently. If all values occur only once, then a variable has no mode. If two or more values occur an equal number of times and it is more than other values, then a variable has two or more modes. For instance, the modes for our 12 die rolls from before are 1, 2, and 5. The mode for infant mortality rates is 11.6. Choosing a center As Figure 4.5 shows, three measures of center have provided us three different typical infant mortality rates. The mean is represented by the red line, the median by the purple line, and the mode by the green line. Figure 4.5: Centers of infant mortality rates Which should we choose to report? We could report all three, but let us apply some professional judgment to this dilemma. After all, people generally do not like nuance and prefer one best number instead of deciphering the meaning from three, assuming they understand all three measures in the first place. For continuous variables reported at several decimal places, a value may not occur more than once because of the precision. More importantly, because of this low probability of repeat values, the mode is not guaranteed to represent a typical value. If it only takes two occurrences to qualify as the mode, that second occurrence could be an extreme value. Therefore, the mode is more commonly used to report frequencies of categorical or discrete variables for which there are relatively few possible values. When choosing between mean and median, it usually comes down to the presence of extreme values or the extent to which a distribution is skewed. Skew pertains to the tails of a distribution–the taper to the left and right of its center. If the right or left tail extends far out from the center, we consider the distribution to be right- or left-skewed. Our distribution of infant mortality rates is right-skewed. When a distribution is skewed, the median is generally a better choice for reporting its center. This is because the mean is sensitive to extreme values. Note in Figure 4.5 that the mean is being pulled to the right by the extreme values. As a result, the red line is to the right of the cluster of frequent values and may not be a good answer for the typical value of this distribution. The median is not sensitive to extreme values. No matter how far we stretch the values above the median to the right, the middle of the distribution stays put. If one were to use the median because of skew, one should also mention that in their report. 4.4.2 Measures of spread Reporting the center of a distribution does not tell us how tightly values are grouped around the mean. Put differently, if the center is a good guess of the value for a unit drawn randomly from our data, the measure of spread is a good guess of how far off that guess will be from the random draw. Variance Almost all values of our variable will not exactly equal the mean. This is referred to as deviation from the mean. The variance squares each observation’s deviation from the mean, sums all the deviations, and divides this sum by the total count of observations minus one. Equation (4.2) displays this process mathematically. \\[\\begin{equation} {\\displaystyle S^2={\\frac {1}{n-1}}\\sum _{i=1}^{n}(x_{i}-\\bar{x})^2={\\frac {(x_{1}-\\bar{x})^2+(x_{2}-\\bar{x})^2+\\cdots +(x_{n}-\\bar{x})^2}{n-1}}} \\tag{4.2} \\end{equation}\\] The mean infant mortality rate is 26.7. If we subtract this mean from each country’s rate, we have each country’s deviation from the mean, some of which is shown in Table 4.3. Then, we square these deviations as is also shown in the table. We then sum the 222 squared deviations and divide by 221. The variance for our infant mortality rates is 672.6. Table 4.3: Excerpt of variance calculations country inf_mort_rate deviate sq_deviate Afghanistan 121.63 94.93 9011.705 Niger 109.98 83.28 6935.558 Mali 109.08 82.38 6786.464 Somalia 103.72 77.02 5932.080 Central African Republic 97.17 70.47 4966.021 Standard deviation Variance is an important building block for inference, but it is virtually useless as a descriptive measure because it is in squared units. If someone asks how far values are spread out from the mean, it would not help much to report values deviate from the mean by 672 squared-deaths. The standard deviation is simply the square root of variance to return our units to their original meaning. \\[\\begin{equation} {\\displaystyle s = \\sqrt{S^2}} \\tag{4.3} \\end{equation}\\] The standard deviation of our infant mortality rates data is 25.9. This tells us that, on average, infant mortality rates are about 26 deaths above or below the mean. Interquartile range Recall that the median is the 50th percentile of a distribution–half of the values fall below the median and half fall above it. The interquartile range (IQR) is equal to the 75th percentile minus the 25th percentile, thus providing the range that captures the middle 50% of the values in the distribution. The IQR is the spread analog of the median. The IQR for our distribution is 35.6. Range The range is simply the maximum value in a distribution minus the minimum value of a distribution. Usually, the range is left implied in a table of summary statistics by reporting the maximum and minimum without differencing the two. The range of our distribution is 119.8. Choosing a spread The same logic applies to choosing a measure of spread as choosing a measure of center. The standard deviation is based on the mean, and so it is also sensitive to extreme values that, if present, could exaggerate the typical spread of the distribution. The IQR is based on percentiles just like the median. Therefore, IQR is not sensitive to extreme values. Figure 4.6 displays the mean and plus-and-minus one standard deviation using red dashed and solid lines, respectively. The median and the IQR (25th and 75th percentiles) are represented by the purple dashed and solid lines, respectively. Note how wide the area contained by the standard deviation is–it contains most of the distribution. It is arguably not ideal for conveying the typical deviation from the center, as it contains plenty of values that are rather atypical deviations from the center. In the case of describing the distribution of infant mortality rates, the median and IQR are likely a better choice. Figure 4.6: Center and spread of infant mortality rates In most cases, the range (or minimum and maximum values) should be reported along with either the mean and standard deviation or median and IQR (or 25th and 75th percentiles), especially when a distribution is skewed. In addition to signaling the skew of a distribution, the range helps convey what may be the possible values of a variable and how different the most different units in the data are with respect to that variable. In the case of infant mortality rates, we know the minimum possible value is 0 by definition, but the minimum value in our distribution is 1.8. Perhaps 0 deaths is unrealistic for any country. Moreover, the maximum value is 121.63. This range, along with the median and IQR, tells us the most different countries are very different. 4.4.3 The normal distribution As a brief aside, it should be mentioned that if a distribution is normal, then measures of center and spread will be similar to each other. This is one of several desirable features of the normal distribution. Figure 4.7 shows a simulated scenario in which the infant mortality rates in our 222 countries exhibit a normal distribution. Note the peaks in the center and symmetry. There is no skew. Figure 4.7: Simulated normal distribution of infant mortality rates Table 4.4 confirms the similarity between measures of center and spread for this simulated distribution. This is one reason it is important to visualize your distribution. If it appears approximately normal, then you should report the mean and standard deviation (along with minimum and maximum values), as the are more widely understood. Table 4.4: Center and spread measures of simulated data Mean Median Mode SD IQR 26 25.5 22.1 6.6 8.4 Again, the normal distribution has several desirable features that will be discussed further in the chapters pertaining to inference. However, you now know one desirable feature. If a distribution is approximately normal, then extreme values are not a concern and the mean and standard deviation are good measures of center and spread, respectively. Besides making our choice of measures convenient, why is this worth repeating? Because the mean and standard deviation are building blocks to the next category of descriptive measures: association. 4.4.4 Measures of association With association, we now consider the distributions of two variables at a time. That is, given the value within one variable’s distribution, what does the distribution of another variable look like? We need a second variable to continue our example involving infant mortality rates. Table 4.5 shows a preview of a dataset that adds two more variables to our previous infant mortality data. Table 4.5: First five rows of country data country inf_mort_rate lifeExp gdpPercap Afghanistan 121.63 43.828 974.5803 Niger 109.98 56.867 619.6769 Mali 109.08 54.467 1042.5816 Somalia 103.72 48.159 926.1411 Central African Republic 97.17 44.741 706.0165 Recalling that the mean infant mortality rate is about 26, the five countries included are in the right tail of the distribution. Also, you probably know enough about life expectancy to know that the values for these countries are quite low. Perhaps these two variables share a relationship? In fact, we know they do. Life expectancy in a given year is the average age at which people in that country died. If a country has a high frequency of infants dying, then that will pull the mean downward. A common misunderstanding of life expectancy is that people in that country tend to die at the age of the country’s life expectancy. This is certainly not the case if a country has a high infant mortality rate. While adults in countries with low life expectancy may die somewhat younger (or much younger if it is a war-torn country), adults tend to live longer than the average life expectancy. The key is making it out of infancy alive. Visual association As was the case with one variable, we want to visualize the distributions of two variables. When working with two continuous variables, the scatter plot is the most common choice to visualize association between two variables. Figure 4.8 plots the paired values of infant mortality rate and life expectancy for each country. Figure 4.8: Visualizing association between two continuous variables Note that I plotted infant mortality rate along the x axis and life expectancy on the y axis. This choice was deliberate. If we suspect that one variable influences or affects the value of another variable, then the variable doing the influencing should be plotted on the x axis. Plotting a variable on the y axis implies to the viewer that it responds to the variable on the x axis. Figure 4.8 confirms our suspicion that the two variables are associated. There appears to be a rather strong association such that as infant mortality rate increases, the lower a country’s life expectancy. Quantified association As was the case with one variable, we want to describe the association between two variables using quantitative measures. The association between two or more variables can be described in terms of Direction: when one variable increases, does the other variable increase or decrease? Strength: how much do the variables seem to be tied together? Magnitude: given an specific increase or decrease in one variable, by how much does the other variable increase or decrease? Again, we have several options to answer the above question. Covariance: measures direction of association between between two variables Correlation: measures direction and strength of association between two variables Regression coefficient: measures the direction and magnitude of association between an explanatory variable and an outcome variable Coefficient of determination (\\(R^2\\)): measures the strength of association between a set of one or more explanatory variables and an outcome variable The regression coefficient and coefficient of determination are discussed in Chapter ?? involving regression models. Let us briefly consider covariance and correlation. Covariance Covariance tells us when our x variable is above (below) its mean, whether our y variable tends to be above or below its mean. If our y variable tends to be above its mean when x is above its mean, then the two have a positive covariance and are positively associated. If our y variable tends to be below its mean when x is above its mean, then the two have a negative covariance and are negatively associated. A covariance of 0 indicates no association. Figure 4.9 adds references lines for the mean of each variable. Note that when infant mortality is above its mean (to the right of the red line), life expectancy is below its mean (below the purple line) in almost all cases. When infant mortality is below its mean, life expectancy is above its mean in almost all cases. Therefore, these two variables have a negative covariance and are negatively associated. In fact, the covariance between infant mortality rate and life expectancy is -312.7 Figure 4.9: Visualizing covariance Covariance is the association analog of variance. It is an important building block of other measures of association, but it is virtually useless for description because it only tells us direction. Correlation tells us direction and strength. Therefore, covariance is never used for description because correlation tells us twice as much information. Correlation Correlation tells us how much the paired values of two variables exhibit a straight line and whether that straight line is sloped positively or negatively. Correlation ranges between -1 and 1. If it is negative, then the two variables are negatively associated. If it is positive, the two variables are positively associated. The closer correlation is to -1 or 1, the more the two variables exhibit a straight line. A correlation equal to -1 or 1 indicates the two variables form a perfect straight line. A correlation of 0 indicates no association. Based on the covariance and the scatter plot, we know to expect a negative correlation between infant mortality rate and life expectancy. We also know the correlation will not be -1 because the points do not form a perfect straight line. Nevertheless they do form a fairly tight negative path, so we should expect a correlation closer to -1 than 0. It turns out that the correlation is equal to -0.9. Infant mortality rate and life expectancy exhibit a very strong, negative association. Another way to think about correlation is if we tried to draw a line through the data points on our scatter plot from left to right that could freely curve about however the data are scattered, would that line be a straight line and would it slope upward or downward? Figure 4.10 does exactly that with our data. Note that the data lead the line to be essentially straight until the end when it appears the association turns positively. Figure 4.10: Drawing a free line through the data Correlation has three qualities that can lead to misunderstandings or mistakes, some of which may have become apparent to you in the above discussion. First, correlation is sensitive to extreme values. A few dots on a scatter plot can have a substantial impact on the line that is drawn through the data. Second, correlation measures only the linear association, thus the repeated mentioning of straight lines. If two variables formed a perfect U-shape, they are almost certainly strongly associated. However, their correlation would suggest a weaker relationship because a straight line does not fit a U-shape well. Third, correlation is a necessary but not sufficient condition of causation. In order to validly claim that a change in the value of one variable causes the values of another variable to change, they must be correlated, but a few more conditions must also be met. Those conditions are discussed in Chapter 9. To learn how to produce a summary table for a publication, proceed to Chapter 19. 4.5 Key terms and concepts Descriptive statistics Inferential statistics Population Sample Parameter Statistic Estimate Distribution Mean Median Mode Skewed distribution Standard deviation Interquartile range Range Correlation "],
["5-data-visualization.html", " 5 Data Visualization 5.1 Learning objectives 5.2 So many choices 5.3 Distribution 5.4 Compostion of a category 5.5 Comparing between units 5.6 Association 5.7 Key terms and concepts", " 5 Data Visualization “The pen is mightier than the sword, especially if it draws a graph.” 5.1 Learning objectives Interpret the six visualizations included in this chapter Recommend an appropriate type of visualization given the intended message and data 5.2 So many choices The world of data visualization is incredibly diverse and detailed. You could spend a substantial amount of time learning how to construct the best visualization given particular data and the the intended message. Such depth is far beyond the scope of this book. For more coverage on data visualization, I recommend the following resources: Flowing Data Data Viz by Kieran Healy At a basic level, most choices of visualization can be determined based on: The kind of description we want to convey, and the kinds of variables we are working with. This decision tree for visualizations is depicted in Figure 5.1 below. Figure 5.1: Basic data viz decisions The above flowchart contains variations on six visualizations that cover most needs: Histogram or density plot Box plot Pie chart Bar chart or dot plot Line graph Scatter plot Sections that follow briefly explain each of these six visualizations. 5.3 Distribution 5.3.1 Histogram You have already seen several histograms. A histogram visualizes the distribution of a single variable by counting the number of occurrences for values that fall within a certain range. The frequency of occurrences within each range is represented by a vertical rectangle. Figure 5.2 shows the median earnings of those employed full time for different graduate degree majors. We can see that most graduate degrees result in a median pay for graduates of between 60 and 80 thousand dollars. There are a few graduate majors for which the median pay is above 100 thousand dollars. Figure 5.2: Histogram of full-time median earnings for different graduate school majors These rectangles are called bins and the range each rectangle covers is called a binwidth. We can specify the number of bins and/or the binwidth. If we have more than 150 observations of a continuous variable, we may want to specify as many as 100 bins but should experiment with this number depending on the particular distribution of the variable. If we have less than 30 observations, we should not use a histogram. If we have more than 30 but less than 150 observations, we should experiment with some number of bins between 30 and 100. Regarding binwidth, if our variable is discrete, then our binwidth should equal the natural integer width. For example, if our variable is a count of weeks, then our binwidth should equal 1 so that each bin contains one week. 5.3.2 Density A density plot is simply a variation of the histogram that requires many observations to work well. We should not use a density plot unless we have at least 1,000 observations. Instead of rectangular representations of the frequency of values, the density plot uses a smooth line, as if one traces a line over the tops of the rectangles of a histogram. One key difference between a density plot and histogram is that the height of a density plot is based on proportions of occurrences for values relative to the total number of observations, whereas a histogram counts the number of occurrences of values. Figure 5.3 is a density of the same data as in the histogram. These data have only 173 observations. This is why the proportions on the y axis are so small. Nevertheless, the density provides us the shape of the distribution. Figure 5.3: Density of full-time median earnings for different graduate school majors 5.3.3 Box plot A box plot (or box-and-whiskers plot) is similar to the histogram and density plot, but a box plot tries to combine a complete view of a distribution and several visual markers denoting some of the descriptive measures covered in Chapter 4. Figure 5.4 shows the median pay data. Figure 5.4: Box plot of full-time median earnings for different graduate school majors The line in the middle of the box denotes the median of the variable’s distribution. The top and bottom edges of the box denote the 75th and 25th percentiles, respectively. Therefore, the length of the box denotes the IQR of the variable’s distribution. The whiskers of a boxplot extend 1.5 times the length of the box (IQR). This 1.5*IQR is a standard threshold to identify extreme values also known as outliers. If a variable contains values beyond this threshold, a box plot will single them out with dots beyond the end of the whisker. 5.4 Compostion of a category Suppose we deemed a graduate degree for which 5% or more of its graduates are unemployed to be a “high” unemployment degree, and those with an unemployment rate less than 5% as a “low” unemployment degree. We have 173 graduate degree majors. Suppose we want to visualize the composition of this categorical unemployment variable. 5.4.1 Pie charts Pie charts are much derided. This derision is due to the fact that pie charts are often misused. Pie charts are acceptable if you want to show the composition of one categorical variable for which there are no more than 3 levels, though preferably no more than 2 levels. We should never use pie charts to compare the composition of a categorical variable between two groups or time periods. Figure 5.5 shows the composition of our unemployment variable. Figure 5.5: Graduate degrees with high/low unemployment 5.4.2 Bar chart Bar charts can be used to present the same information as a pie chart. Moreover, bar charts are easier to interpret, can handle any number of levels, can present data as proportions or total counts, can be used to compare across groups or time, and are easier to make. In short, bar charts are better than pie charts, and we should choose bar charts unless someone forces us to use a pie chart for some reason. The figures below show the three general types of bar charts. Figures 5.6 and 5.7 show the composition of our unemployment variable in terms of absolute counts. Figure 5.6 is commonly referred to as a stacked bar chart, while Figure 5.7 is referred to as dodged. Figure 5.8 shows the composition in terms of proportions. That is, we can see that slightly over 75% of graduate degrees have low unemployment. Figure 5.6: Graduate degrees with high/low unemployment Figure 5.7: Graduate degrees with high/low unemployment Figure 5.8: Graduate degrees with high/low unemployment 5.5 Comparing between units 5.5.1 Bar chart If we want to compare one variable across multiple groups or units that are not time, then a bar chart is a good choice. Suppose we wanted to compare the median pay between two or more graduate degrees. Figure 5.9: Comparison of median pay between degrees in public and international affairs 5.5.2 Dot plot Dot plots serve the same purpose as bar charts, but are more appropriate for variables that measure something we would not naturally stack up for counting purposes. That is, money is stackable–we could imagine each bar as a stack of cash. By contrast, unemployment rates are not somethings we would stack on top of each other. Figure 5.10: Comparison of unemployment rates between degrees in public and international affairs 5.5.3 Line graph If we want to compare values of a variable across units of time (i.e. change over time), then a line graph is probably the most common choice, though a bar chart or dot plot work too. The graduate degree data is cross-sectional, so there is no good way to make a line graph. I trust you have seen one before. 5.6 Association Associations involve two or more distributions. We can visualize multiple distributions using a scatter plot if both variables are continuous or discrete with many values, or we can use a histogram or box plot if we want to visualize how the distribution of a continuous variable changes for each level of a categorical variable. 5.6.1 Categorical and numerical Suppose we wanted to visualize the association between attaining a graduate degree or not and median pay. Whether to attain a graduate degree is a categorical variable with two levels. Therefore, we can use a box plot to visualize the distribution of median pay for employees with undergraduate degrees in the 173 majors in our data and the distribution of median pay for employees with a graduate degree in those same majors. Figure 5.11 below does just that. Figure 5.11: Median pay for undergraduate and graduate degrees of the same group of majors Overlaying histograms for each level could work too as is done in Figure 5.12. Figure 5.12: Median pay for undergraduate and graduate degrees of the same group of majors 5.6.2 Scatter plot The most common visualization for associations is the scatter plot, which you saw several times in Chapter 4. It is also common to overlay a simple regression line for the two variables, thus providing a reader the full scatter of the two distributions as well as a tracing of how the two variables move in tandem, on average. Suppose we wanted to visualize the relationship between median pay of graduate degrees and the total number of people with that graduate degree. Do more people tend to enroll in the programs that pay the most? Figure 5.13: Graduate degree median pay and total number of people with degree The logic of visualization choice discussed in this chapter applies regardless of what particular software one uses. To learn how to generate most of these graphs in R, proceed to Chapter 20. 5.7 Key terms and concepts Uses of a histogram Uses of a box plot Uses of a bar chart Uses of a scatter plot Distribution of a numerical variable Comparison of one variable between two or more units of analysis Composition of a categorical variable "],
["6-simple-and-multiple-regression.html", " 6 Simple and Multiple Regression 6.1 Learning objectives 6.2 Basic idea 6.3 Simple linear regression 6.4 Multiple regression 6.5 Key terms and concepts", " 6 Simple and Multiple Regression “You can lead a horse to water but you can’t make him enter regional distribution codes in data field 97 to facilitate regression analysis on the back end.” —John Cleese 6.1 Learning objectives Identify and explain the components of a population or sample regression model Explain the difference between a deterministic equation of a line and a statistical, probabilistic equation of a line Given regression results, provide the predicted change in the outcome given a change in the explanatory variable(s) Given regression results, provide the predicted value of the outcome given a value of the explanatory variable(s) Explain what the error term in a regression model represents Interpret measures of fit in a regression model and explain their relative strengths and weaknesses 6.2 Basic idea The basic idea of regression is really quite simple. Regression calculates a line through a scatter plot of two variables so that we can summarize how much our variable on the y axis changes given a change in the variable on our x variable. Or, we can use a given value for our x variable to predict a value for our y variable. That’s all it is–a line drawn to represent the association between two variables. We all learned the equation for a line back in middle school, which probably looked something like the following: \\[\\begin{equation} y = mx + b \\tag{6.1} \\end{equation}\\] where \\(m\\) is the slope of the line and \\(b\\) is the y-intercept. If we know the slope and intercept for a line, then, given a value for \\(x\\), we can compute \\(y\\). Given a change in \\(x\\), we can compute a change in \\(y\\) by multiplying the change in \\(x\\) by \\(m\\). Consider the following equation for an arbitrary line: \\[y = 5x + 10\\] Here are some questions we can now answer: How much does \\(y\\) change if \\(x\\) increases by 1? Answer: 5 How much does \\(y\\) change if \\(x\\) increases by 10? Answer: 50 How much does \\(y\\) change if \\(x\\) decreases by 10? Answer: -50 What does \\(y\\) equal if \\(x\\) equals 2? Answer: 20 What would \\(y\\) equal if \\(x\\) were 0? Answer: 10 If you understand how to answer the above questions, then you can interpret regression results for any given context because interpreting regression results involves either predicting the change in \\(y\\) given a change in \\(x\\) or predicting the value of \\(y\\) given a value of \\(x\\). What is different in regression is how the equation of the line is presented because there are population and sample versions of the relationship between \\(x\\) and \\(y\\). And, unlike a deterministic mathematical equation like the one above, because we generally use regression to measure relationships between social phenomena, there is inherent uncertainty in the line we calculate. This adds some complexity beyond solving a line’s equation, but the process of running a regression to estimate the slope and intercept of a line to then predict changes or values of an outcome is fundamentally the same as the simple equation above. 6.3 Simple linear regression Equation (6.2) presents the population regression model. \\[\\begin{equation} y=\\beta_0+\\beta_1x+\\epsilon \\tag{6.2} \\end{equation}\\] Only one element differs between Equations (6.1) and (6.2). That is the symbol at the end, which is the Greek letter epsilon and is used to denote the aforementioned uncertainty of predicting real-world, particularly social, phenomena (more on that later). The y-intercept denoted as \\(b\\) in Equation (6.1) has been moved to the front of the right-hand side in Equation (6.2) and is denoted by \\(\\beta_0\\) (pronounced beta-naught). The slope denoted as \\(m\\) in Equation (6.1) is now denoted as \\(\\beta_1\\) in Equation (6.2). These beta, \\(\\beta\\), symbols are simply the standard notation for population parameters in a statistical model and are used to signal that we intend to estimate these parameters using regression. Recall that a parameter is a statistical measure of a population. In most cases, our research questions concern a population so large or inaccessible such that we do not observe all members. Instead, we take a sample of the population. From this sample, we calculate sample statistics, or estimates of the parameters and use methods of inference to decide if these estimates are valid guesses of the parameter (more on this later). Equation (6.3) presents the sample regression equation. \\[\\begin{equation} \\hat{y}=b_0+b_1x \\tag{6.3} \\end{equation}\\] The carrot symbol atop our outcome variable \\(y\\) is called a hat, and so the term on the left-hand side is commonly referred to as “y-hat.” This is used to denote the fact that any value we calculate from Equation (6.3) is an estimate of what has been or will be observed. Similarly, the \\(b\\) symbols are the sample estimate analogs of the \\(\\beta\\) population parameters in Equation (6.2). Equation (6.3) is the equation we use to interpret our regression results in the same way as was demonstrated using the mathematical equation of a line. Again, the only difference is that we are dealing with a statistical or probabilistic equation of a line–the outcome we calculate is a prediction based on observed data. 6.3.1 Using regression Let’s pause the theory to consider a simple example using data for U.S. counties. Table 6.1 provides a preview of the data Table 6.1: Preview of county data name state fed_spend poverty homeownership income Traverse County Minnesota 20.038786 9.3 80.3 41287 Wabash County Illinois 7.422533 13.0 80.1 46026 Pike County Mississippi 9.091897 25.3 72.9 30779 Greenbrier County West Virginia 9.029030 19.4 75.0 33732 Ray County Missouri 5.795480 9.4 78.7 53343 Hamilton County Tennessee 10.188056 14.7 65.5 45408 Ballard County Kentucky 11.907989 13.0 83.3 41228 where fed_spending is the amount of federal funds allocated to the county per capita, poverty is the percent of the population in poverty, homeownership is the percent of the population that owns a home, and income is per capita income. There are 3,143 observations in this dataset. Suppose we wanted to examine the association between federal spending and poverty for U.S. counties such that poverty explains federal spending. After all, a substantial portion of federal dollars are dedicated to assist those in poverty. First, we might visualize the relationship between the two variables. Figure 6.1: Federal spending and poverty among U.S. counties If we were to trace a line through these points, it would clearly slope upward. Thus, this suggests to us that as the percent of the population of a county increases, the amount of federal spending it receives increases. But by how much? That is precisely what regression estimates for us. Equation (6.4) represents the relationship between federal spending and poverty using a simple linear regression population model. Note that we have chosen to model the two variables such that poverty explains or predicts federal spending. This aligns with the choice to plot poverty on the x axis and federal spending on the y axis in Figure 6.1. This is a critical choice in every regression and one that computers still need humans to help with (more on that later). \\[\\begin{equation} FedSpend = \\beta_0+\\beta_1Poverty + \\epsilon \\tag{6.4} \\end{equation}\\] We are going to use observed values of poverty and federal spending to estimate \\(\\beta_0\\) and \\(\\beta_1\\). Then, once we have those estimates, we can provide succinct answers regarding how federal spending tends to change given a change in poverty or a predicted level of federal spending given a particular level of poverty in a county. The \\(\\epsilon\\) represents all the other factors that explain or predict federal spending that are not in our model. If our world were such that the points in 6.1 literally formed a straight line, we would not need an \\(\\epsilon\\), but this is never the case with interesting questions of complex phenomena. This may or may not be a problem for whatever story we intend to tell about the the relationship between poverty and federal spending (more on that later). Running the regression as represented in Equation (6.4) produces Table 6.2 of results. Table 6.2: Regression results of poverty on federal spending term estimate std_error statistic p_value lower_ci upper_ci intercept 7.950 0.219 36.294 0 7.520 8.379 poverty 0.108 0.013 8.265 0 0.082 0.134 With the exception of Chapter @(causation-and-bias), this section on regression models focuses on understanding the methods used to generate the values in the estimate column immediately to the right of the variable names as well as how to interpret and apply these values to any question. The remaining columns in the above table pertain to inference. The values in the estimate column are commonly referred to as coefficients, which were first mentioned in Chapter 4. Regression coefficients measure the direction and magnitude of association between an explanatory variable and an outcome variable. Now that we have our results, we can plug them into our sample regression equation like so \\[\\begin{equation} \\hat{FedSpend}=7.95+0.11 \\times Poverty \\tag{6.5} \\end{equation}\\] and we are back to the first section of this chapter. Given a change in poverty we can predict the change in federal spending. Given any particular poverty level, we can predict a level of federal spending. As for a standard template to interpret regression results, we generally say or write the following, On average, as the percent of the population in poverty increases by 1 percentage point, federal spending per capita increases approximately 11 cents. A couple points about the above template: We always qualify using “on average” because that is exactly what regression does. Drawing a line through a scatter plot results in points above and below that line. The line drawn by regression traces how \\(y\\) responds to \\(x\\) on average. The standard change in \\(x\\) to use when reporting results is one unit. Poverty is in units of percent. Therefore, a one-unit change in a variable measured in percentages is one percentage point (e.g. 10% to 11%). However, we could report these results using any change or particular value in poverty germane to our original research question. For example, if we expected a county’s poverty rate to be 30%, then we could report a predicted level of federal spending per capita equal to 7.95+0.11*30 ## [1] 11.25 dollars per capita. Similarly, if we expected a decrease in the poverty rate of 5 percentage points, then we could predict a change in the level of federal spending per capita equal to 0.11*-5 ## [1] -0.55 dollars per capita. 6.3.2 The error term Back to theory. We need to address this \\(\\epsilon\\) that is present in the population regression model but disappears in the sample regression model and results. What gives? The \\(\\epsilon\\) term is commonly referred to as the error term or, for those who don’t like to insinuate some error was made in the regression, statistical noise. I prefer error term if for no other reason than to remind us to consider the myriad of errors we may be making in our regression model. As mentioned, the error term represents the inherent uncertainty of modeling an outcome based on a necessarily finite number of explanatory factors. Other factors affect our outcome. As a matter of principle, this does not prohibit our attempt to estimate the effect of a variable we care about or can alter with policy or programs on the outcome. Could we account for multiple factors (i.e. multiple regression)? Absolutely. Can we control for everything that affects our outcome? Definitely not if you subscribe to chaos theory or David Hume’s thoughts on causality. Even if not so extreme as to say our world is too complex to ever make decisions concerning one variable’s effect on another, the plausibility for us to collect data on every relevant factor is highly unlikely. So, where did the error term go? It never really left; it simply is not used when calculating the predicted outcome based on our regression results. Like our \\(\\beta\\) terms, the error term is a population parameter. However, unlike the \\(\\beta\\)s, we do not have observed data that corresponds to its estimation. In fact, the concept of the error term exists on the basis that we do not observe it. Therefore, it is necessarily excluded when we predict an outcome based on observed data, all the while we are careful to remind readers that the numbers we report are estimates subject to error. If the error term never left, where is it? Its sample analog exists as the difference between our estimated regression line and the observed data. Figure 6.2 below is the same as Figure 6.1 except that it displays the line based on our regression results in Table 6.2. Figure 6.2: Federal spending and poverty among U.S. counties Surely, it is apparent that our regression line does not intersect all points perfectly; many points lie above and others below it. Pick any point in Figure 6.2 and draw a vertical line to the regression line. The length of that line, which in this case is in units of dollars of federal spending per capita, and all other vertical distances between observed points and the regression line, is our sample version of the error term. Table 6.3 quantifies the error for each of a select few observations. Table 6.3: Comparing observed and predicted federal spending ID fed_spend poverty fed_spend_hat residual 1961 7.592 16.9 9.773 -2.181 2440 10.188 17.1 9.795 0.393 1738 15.784 8.8 8.899 6.885 2641 23.503 0.0 7.950 15.554 1471 8.152 25.0 10.647 -2.495 1362 7.649 14.0 9.460 -1.811 2792 7.933 13.5 9.406 -1.474 Our regression model uses observed values of poverty and federal spending to estimate the parameters of the regression line, which produced Equation (6.5). We can then plug the observed values of poverty into the equation to compute a predicted level of federal spending–fed_spend_hat. But, we know this prediction is not perfect in most cases. The right-most column of Table 6.3 shows the difference between predicted federal spending and observed federal spending. This difference is called the residual. Using the first row of Table 6.3 as an example, our regression predicts a county with 16.9 percent poverty to receive 9.77 dollars per capita in federal spending. However, this county actually received 7.59 dollars per capita. Our regression over-estimates federal spending for this county by 2.18 dollars. Thus, the residual for this county is -2.18 because the observed outcome is 2.18 less than the estimated outcome. The residual is represented mathematically by Equation (6.6) \\[\\begin{equation} e = y - \\hat{y} \\tag{6.6} \\end{equation}\\] where \\(e\\) is the sample analog of \\(\\epsilon\\). This is simply the equation behind the process of differencing the observed and predicted values of our outcome just described. 6.3.3 Goodness of fit Armed with an understanding of error and its sample analog, the residual, we can now consider goodness-of-fit. We must accept there will be error in our regression, but that does not mean we do not seek to minimize that error as much as possible. Assessing fit Table 6.4 provides a standard set of three goodness-of-fit measures often used to assess regression. Table 6.4: Goodness-of-fit measures r_squared adj_r_squared rmse 0.021 0.021 4.654482 The first column titled r_squared refers to the measure \\(R^2\\), also known as the coefficient of determination defined in 4. The \\(R^2\\) measures the strength of association between a set of one or more explanatory variables and an outcome variable. Specifically, it quantifies the percent of total variation in the outcome explained by our regression model. In this case, our regression using poverty explains 2.1% of the total variation in federal spending. The column titled rmse refers to root mean squared error (RMSE). The RMSE quantifies the typical deviation of the observed data points from the regression line and is particularly useful when predicting a value for our outcome. For example, if after predicting that a county with 30% poverty will receive 11.25 dollars of federal spending per capita, someone asks us how far off that prediction is likely to be, the RMSE suggests our prediction will tend to be off by plus or minus 4.65 dollars. Regression involves choices. We choose which variables to use to explain or predict an outcome and how to model their effect on the outcome. This menu of choices will become increasingly evident as we build our regression toolbox. As we make choices, competing regression models emerge from which we must choose the one we prefer to report for decision-making. The \\(R^2\\) and RMSE provide us the basis for choosing our preferred model. In general, we prefer the model with a higher \\(R^2\\) and/or a lower RMSE. In virtually all cases, these two measures will agree with each other; the model with the higher \\(R^2\\) will also have the lower RMSE. Understanding fit The material under this header is nonessential but is included because a deeper understanding of the mechanics of fit can be helpful. Regression draws the best line through a set of data points of two or more variables. The best line in this case is the line with a slope and y-intercept that minimizes the sum of squared residual between the set of data points and said line. The procedure used to achieve such a line is called ordinary least squares (OLS). The type of regression covered in this book is sometimes referred to as OLS regression. Recall that the variance of a variable is the sum of squared deviations from the mean, as depicted in Equation (4.2). This should sound familiar. Instead of deviations from the mean, fitting the best line in regression concerns the deviations from the regression line, which by definition are the residuals. As with variance, we square the deviations (i.e. residuals) for the data used to estimate the regression line, then we add these squared deviations together to obtain the sum of squared residual (SSR). Equation (6.7) shows this process mathematically. \\[\\begin{equation} SSR=\\sum _{i=1}^{n}(y_{i}-\\hat{y})^2= (y_{1}-\\hat{y})^2+(y_{2}-\\hat{y})^2+\\cdots +(y_{n}-\\hat{y})^2 \\tag{6.7} \\end{equation}\\] SSR quantifies the error in our regression and is what regression minimizes when predicting an outcome given the explanatory variables we have chosen to include. The SSR also provides us what we need to compute the root mean squared error (RMSE). Recall that in order to compute the variance and standard deviation of a variable in Equations (4.2) and (4.3), respectively, we divide the sum of squared deviations by the number of observations (or \\(n-1\\)) then take the square root. The SSR is a sum of squared deviations. The deviations in this case represent error. If we divide SSR by the number of observations, we now have the mean of the sum of squared error. Then, if we take the square root, we have the root mean squared error. Note that this is the same process used to obtain the standard deviation. Thus, the RMSE is the regression version of a standard deviation. Just as the standard deviation tells us the average deviation from the mean, the RMSE tells us the average deviation from the regression line, or the average error in our regression. We can also quantify the extent to which our regression explains the outcome. To do so, we need a benchmark against which to compare the reduction in error achieved by our regression. This benchmark is simply the average value of the outcome. If we had no explanatory variables to predict an outcome, the mean provides the typical value of the outcome. If we had to draw a random observation from a variable’s distribution, the mean is our best guess of what that observation’s value would be if we have no explanatory variables. Figure 6.3 adds a reference line of average federal spending to our scatter plot. Note that because average federal spending is a constant number, it does not change as poverty changes; the red line has no slope. Also, note that the red line does slightly worse fitting the data, particularly toward the left and right extremes of poverty. Compared to the red line representing the mean, the data appear to be more centered around our regression line. As a result, our regression line has less error than the mean. Figure 6.3: Federal spending and poverty among U.S. counties The mean of the outcome is our benchmark for assessing how much the explanatory variables included in our regression model explains the total variation in the outcome. The difference, if any, between the values our regression predicts, \\(\\hat{y_i}\\), and the mean, \\(\\bar{y}\\), serves as the basis for quantifying the extent to which our regression model explains the total variation in the outcome. Just like with the SSR, we square the difference between each predicted value and the mean, then add them together. The result is called the sum of squared explained (SSE) and is represented mathematically in Equation (6.8). \\[\\begin{equation} SSE=\\sum _{i=1}^{n}(\\hat{y}_{i}-\\bar{y})^2= (\\hat{y}_{1}-\\bar{y})^2+(\\hat{y}_{2}-\\bar{y})^2+\\cdots +(\\hat{y}_{n}-\\bar{y})^2 \\tag{6.8} \\end{equation}\\] We now have the sum of squared residuals (SSR) and sum of squared explained (SSE). Together, the SSR and SSE represent the sum of squared total (SST) variation in the outcome \\(y\\). \\[\\begin{equation} SST = SSR + SSE \\tag{6.9} \\end{equation}\\] Recall that the \\(R^2\\) measures the percent of total variation in the outcome that is explained by our regression. To calculate any percent we take divide a proportion of the whole divided by the whole (e.g. \\(5/10 = 0.5\\) or 50%). Thus, to obtain the percent of variation in the outcome explained by our regression, we divide the SSE by SST. \\[\\begin{equation} R^2 = {\\frac{SSE}{SST}} \\tag{6.10} \\end{equation}\\] The better you understand the mechanics of simple linear regression, the easier it will be to understand the next section and subsequent chapters on regression models because they are mere extensions of this basic model. 6.4 Multiple regression Of course, we are not limited to using only one variable to explain or predict an outcome. In fact, it is rather uncommon to use only one variable, but simple linear regression is useful for introducing the method of regression. Now, we can consider more realistic modeling method where we use multiple explanatory variables in our regression, which is aptly named multiple regression. Equation (6.11) provides the population model for multiple regression \\[\\begin{equation} y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots+\\beta_kx_k+\\epsilon \\tag{6.11} \\end{equation}\\] The only difference in this equation compared to Equation (6.2) is the inclusion of multiple explanatory variables. Each explanatory is numbered and has a corresponding parameter \\(\\beta\\) representing the marginal effect it has on the outcome. In theory, we can add however many explanatory variables we deem worth including, represented by the arbitrary \\(k\\). Equation (6.12) presents the sample equation for multiple regression. \\[\\begin{equation} \\hat{y}=b_0+b_1x_1+b_2x_2+\\cdots +b_kx_k \\tag{6.12} \\end{equation}\\] Again, nothing is different from before except for more explanatory variables and sample estimates of the parameters. 6.4.1 Using multiple regression Let’s return to our example of federal spending per capita in U.S. counties. Previously, we used only the percent of the population in poverty to explain or predict federal spending per capita. Let’s add the percent of the population that owns a home and per capita income to our model. Thus, our model can be written as such \\[\\begin{equation} FedSpend = \\beta_0 + \\beta_1Poverty + \\beta_2HomeOwn + \\beta_3Income + \\epsilon \\tag{6.13} \\end{equation}\\] which generates the following results Table 6.5: Multiple regression results term estimate std_error statistic p_value lower_ci upper_ci intercept 23.519 1.333 17.645 0.000 20.905 26.132 poverty -0.056 0.021 -2.674 0.008 -0.097 -0.015 homeownership -0.126 0.012 -10.736 0.000 -0.149 -0.103 income 0.000 0.000 -7.723 0.000 0.000 0.000 and the following goodness-of-fit measures Table 6.6: Fit of multiple regression r_squared adj_r_squared rmse 0.064 0.063 4.55216 Now we can discuss what is different with multiple regression. First, note that our coefficient or estimate for poverty has changed from 0.108 to 0.105. A small difference to be sure, but that is specific to the example used; sometimes the estimate can change dramatically. Why the change? Because we are controlling for other factors. A slight amount of the marginal effect we reported poverty had on federal spending in our simple regression model was misattributed from the marginal effects of homeownership and/or income on federal spending. This is a key feature of multiple regression: it estimates the marginal effect of a variable on an outcome, holding all other explanatory variables equal to their respective means. In other words, if we were omnipotent beings who could take each county in our data and set homeownership and income to the mean of homeownership and income according to the observed data, then pull some lever that makes poverty change and nothing else, the estimate for poverty in our multiple regression reports how much each percentage point in poverty changes federal spending. This is how we isolate the effect of one variable on an outcome despite knowing other variables simultaneously affect our outcome. The interpretation of multiple regression estimates is essentially the same as simple regression. In our example, we can interpret the homeownership estimate like so: On average, our results indicate that a one percentage point increase in the percent of the population that owns a home is associated with a decrease in federal spending per capita of approximately 9 cents, holding other factors constant. The part in bold is to point out the small difference between the two interpretations. Here, we are simply reminding a reader that we have controlled for other factors that presumably we have already explained, and our estimate for poverty accounts for those factors by holding them constant. Other common word choices for this part of the interpretation include “all else equal” or its Latin translation “ceteris paribus.” Again, we can answer any sort of question relevant to our original research question concerning the predicted change or level of federal spending by plugging in the numbers to our regression equation. \\[\\begin{equation} \\hat{FedSpend} = 13.50 + 0.105\\times Poverty - 0.093\\times HomeOwn + 0Income \\end{equation}\\] If we wanted to predict the change in federal spending given an 3 percentage point increase in poverty and a decline in home ownership of 4 percentage points, the our answer would be 0.105*3+(-0.093)*(-4) ## [1] 0.687 dollars per capita (on average and all else equal, of course). If we wanted to predict the level of federal spending per capita for a county with 12% poverty, a 80% home ownership rate, and $31,000 income per capita, then we would predict 13.50+0.105*12-0.093*80+0*31000 ## [1] 7.32 dollars per capita. Not so fast! This example provides a good opportunity to consider another aspect of the units our variables are in. Per capita income is in dollars. This means the estimate for income represents the effect of a one dollar change in per capita income on federal spending per capita. That’s a very small change that we would expect to have a very small effect on federal spending. This effect is so small that statistical software may round to 0. But what if we changed the units of income to thousands of dollars per capita instead of dollars per capita? Then we get the following results. term estimate std_error statistic p_value lower_ci upper_ci intercept 23.519 1.333 17.645 0.000 20.905 26.132 poverty -0.056 0.021 -2.674 0.008 -0.097 -0.015 homeownership -0.126 0.012 -10.736 0.000 -0.149 -0.103 income -0.086 0.011 -7.723 0.000 -0.108 -0.064 Now we see the effect of a one thousand dollar change in per capita income on federal spending per capita. Note that the estimates for poverty and homeownership are the same. Therefore, the predicted level of federal spending for our county is actually 13.50+0.105*12-0.093*80+0.057*31 ## [1] 9.087 6.4.2 Fit and adjusted R squared In addition to doing a better job isolating the marginal effect of one variable on an outcome, including additional explanatory variables can reduce the error in our regression, thus achieve more accurate and/or precise predictions of the outcome. We can assess this improvement in fit by comparing the results in Table 6.6 to those in Table 6.4. We have gone from an RMSE of 4.65 dollars to an RMSE 4.59 dollars. This means our predictions from the multiple regression model tend to be off by 6 cents fewer than the predictions of our simple regression model. The previous discussion on fit conspicuously skipped over the column titled adj_r_square because adjusted-\\(R^2\\) applies when comparing two or more models with a different number of explanatory variables. One caveat to using \\(R^2\\) to choose a preferred model is that it mechanically increases as the number of explanatory variables increases whether those additional variables improve the extent to which our regression explains the total variation in the outcome or not. Therefore, it is unfair to compare a model with one explanatory variable to a model with more than one explanatory variable. The adjusted-\\(R^2\\) accounts for this unfairness by applying a penalty to each additional explanatory variable. We can fairly compare models with different numbers of explanatory variables using their respective adjusted-\\(R^2\\). In our example, we have gone from explaining 2.1% of the total variation in federal spending to explaining 4.7% of its total variation. Adding home ownership and income has more than doubled the explanatory power of our model of federal spending. 6.4.3 Explanatory penalty Each explanatory variable we add to our regression model imposes a type of penalty on our results. Basically, for each explanatory variable included, we lose an observation in our data (not literally). This will be discussed further in the section on inference, but we need at least 33 observations to make valid inferences about a population based on sample estimates. If we had, say 50 observations in a dataset, and wanted to run a regression with 25 explanatory variables, then it is as though our regression model is based on only 25 observations (50 observations - 25 variables = 25 degrees of freedom). We will obtain results from such a model, but we should not use those results to make inferences. In case you were wondering why not simply add all the variables we can to a model rather than carefully consider which variables to include and exclude in a model, this penalty is one of the primary reasons. Fewer degrees of freedom jeopardizes our ability to make valid inference. It can also reduce the precision of our predictions. The goal is to maximize the explanatory or predictive power of our regression model at minimal cost (i.e. excluding superfluous variables). Choosing good regression models is where subject matter expertise plays a crucial role. Experience and knowledge within the context of the research question informs our choices. Statistics is the method by which we apply our expertise to data to make evidence-based decisions. To learn how to run regression in R, proceed to Chapter 21. 6.5 Key terms and concepts Line concepts y-intercept slope change in y versus value of y Regression model components outcome/dependent/response variable independent/explanatory variable error term/statistical noise residual population parameter sample coefficients/estimates Goodness of fit R-squared Adjusted R-squared root mean squared error (RMSE) Controlling for other factors in multiple regression "],
["7-categorical-variables-and-interactions.html", " 7 Categorical Variables and Interactions 7.1 Learning objectives 7.2 Parallel slopes model 7.3 Interaction model 7.4 Linear probability model 7.5 Key terms and concepts", " 7 Categorical Variables and Interactions “For how can one know color in perpetual green, and what good is warmth without the cold to give it sweetness?” —John Steinbeck, Travels with Charley 7.1 Learning objectives Explain why and how to extend simple or multiple regression models to a parallel slopes model Interpret results of a parallel slopes model Explain why and how to extend regression models to an interaction model Interpret results of an interaction model Provide advice on choosing between parallel slopes and interaction model Explain why and how to extend regression models to a linear probability model Interpret results of a linear probability model Chapter 6 introduced regression using models containing quantitative variables only. In this chapter, we build our regression toolbox to include models that contain qualitative variables. We will cover three models in particular: Parallel slopes model: including a categorical explanatory variable Interaction model: allowing the slope of the regression line for each level of a categorical variable to differ (i.e. not parallel) Linear probability model: including a two-level (binary) categorical outcome 7.2 Parallel slopes model To introduce the inclusion of categorical variables in regression, the simplest type of categorical variable will be used. The simplest categorical variable is commonly referred to as a dummy variable. A dummy variable is a two-level or binary categorical variable. It takes on values of either 1 or 0, where 1 corresponds to “yes” or “true” and 0 corresponds to “no” or “false”. For example, a common way to represent biological sex in data is to use a dummy variable where either male or female is coded as 1 and the other sex is coded as 0. It is common to name such a variable as the level coded as 1. For example, a dummy variable coded as 1 for male and 0 for female will often be named “male” in a dataset. A variable coded as 1 to denote a person attained a college degree and 0 to denote they did not might be named “college.” The parallel slopes model using a dummy variable is represented in Equation (7.1). \\[\\begin{equation} y = \\beta_0 + \\beta_1x + \\beta_2d + \\epsilon \\tag{7.1} \\end{equation}\\] where \\(d\\) is simply used to distinguish the variable as a dummy variable from the quantitative \\(x\\) variable. The sample regression equation for the parallel slopes model is represented by Equation (7.2). \\[\\begin{equation} \\hat{y} = b_0 + b_1x + b_2d \\tag{7.2} \\end{equation}\\] Knowing that \\(d\\) can equal only 1 or 0, we can plug these values into Equation (7.2) to understand the logic of the parallel slopes model before considering the results from an example. If \\(d=0\\), then \\(b_2\\) drops out of the equation because anything multiplied by 0 equals 0. In that case, our regression equation is, \\[\\begin{equation} \\hat{y} = b_0 + b_1x \\tag{7.3} \\end{equation}\\] and we can plug in our results to predict changes or values of \\(y\\) just like before. If \\(d=1\\), then \\(b_2\\) stays in the model. Anything multiplied by 1 is equal to itself, and since \\(d\\) can only equal 1 if not equal to 0, we can drop \\(d\\) out of the equation like so \\[\\begin{equation} \\hat{y} = b_0 + b_1x + b_2 \\tag{7.4} \\end{equation}\\] Note that \\(b_2\\) is not multiplied by the value of another variable like \\(b_1\\) is multiplied by some change or value of \\(x\\). Instead, it is a constant like the y-intercept \\(b_0\\). In fact, combining \\(b_0\\) and \\(b_2\\) gives us a new y-intercept as shown in Equation (7.5). \\[\\begin{equation} \\hat{y} = (b_0 + b_2) + b_1x \\tag{7.5} \\end{equation}\\] This volley of equations is not to meant to repulse the math averse. Rather, it is meant to show you that the logic of the parallel slopes model is quite simple. Including a dummy variable \\(d\\) is analogous to drawing two separate regression lines–one line through the observations for which \\(d=0\\) and another line through the observations for which \\(d=1\\). The second line will be either above or below the first regression line by a constant amount equal to \\(b_2\\), resulting in two regression lines running parallel to each other. 7.2.1 Using parallel slopes Suppose we are interested in whether state laws mandating a jail sentence for drunk driving affects traffic fatalities, presumably by deterring drunk driving. To investigate, we collect the following data, some of which is previewed in Table 7.1. Table 7.1: Sample of state traffic data state year mrall jaild vmiles mlda unrate region 41 1987 2.27606 yes 8.565328 21 6.2 West 22 1982 2.48916 yes 6.137799 18 10.3 South 4 1985 2.80201 yes 6.771263 21 6.5 West 23 1982 1.46127 yes 6.733286 20 8.6 N. East 27 1982 1.38156 no 7.059264 19 7.8 Midwest 8 1984 1.90596 no 7.707853 21 5.6 West 23 1984 2.00692 yes 8.083908 20 6.1 N. East where mrall is number of traffic deaths per 10,000 population, jaild is the dummy variable for whether the state has a mandatory jail sentence for drunk driving, vmiles is the average miles driven per driver in a state, mlda is the minimum legal drinking age at the time, and unrate is the state’s unemployment rate. There are 336 observations in this dataset (48 states from 1982 to 1988; panel data but used like a pooled cross-sectional for this example). Suppose we choose to use the following model \\[\\begin{equation} mrall = \\beta_0 + \\beta_1vmiles + \\beta_2jaild + \\epsilon \\tag{7.6} \\end{equation}\\] First, let’s visualize the relationship between these variables using a scatter plot with vmiles on the x axis and using color to differentiate states with and without mandatory jail sentences for drunk driving. Note in Figure 7.1 below there appears to be a positive relationship between the average miles people drive and the traffic fatalities. That makes sense. Now, imagine drawing a straight line through the red points that denote states with no mandatory jail for drunk driving and a separate line through the blue dots denoting states with mandatory jail sentencing. Do not force your imaginary lines to be parallel just yet. How do your two lines compare? Based on the data, our blue line should be above our red line, as the group of blue dots appear to be systematically higher than the group of red dots. The slopes are less obvious. It may be the case that our red line should have a steeper slope than our blue line, but it is difficult to tell. Figure 7.1: Relationship between miles driven and traffic deaths by whether state has mandatory jail for drunk driving The imaginary exercise you just went through is critical. Considering whether the slopes of our regression line to differ between the two groups is the difference between the parallel slopes model and the interaction model. Why not just add the interaction and if they are the same slope, so be it? Again, because we pay a penalty for adding superfluous explanatory variables. Moreover, an interaction model is more difficult to interpret and communicate to an audience. Most importantly, we should choose the model that reflects our theory that is based on our subject matter expertise. Choosing to use a parallel slopes model forces the slopes between the two groups to be drawn (i.e. estimated) as if they are parallel whether they actually are or not. Now having a sense of the relationship, we run our parallel slopes model, which generates the following results Table 7.2: Parallel slopes results term estimate std_error statistic p_value lower_ci upper_ci intercept -0.238 0.182 -1.304 0.193 -0.597 0.121 vmiles 0.281 0.023 12.107 0.000 0.236 0.327 jaildyes 0.265 0.056 4.726 0.000 0.155 0.376 Now we can plug our results into the sample regression equation to answer whatever questions we may have or encounter. \\[\\begin{equation} \\hat{mrall} = -0.238 + 0.281\\times vmiles + 0.265 \\times jaild \\end{equation}\\] I strongly recommend that you compare this equation to Equations 7.2-7.5. Otherwise, the suffering was in vain. The jaild variable is the \\(d\\) variable. It equals either 0 or 1. If a state does not have a mandatory jail sentence (jaild = 0), then we have \\[\\begin{equation} \\hat{mrall} = -0.238 + 0.281\\times vmiles \\end{equation}\\] and if a state does have a mandatory jail sentence, then we have \\[\\begin{equation} \\hat{mrall} = -0.238 + 0.281\\times vmiles + 0.265\\\\ \\hat{mrall} = (-0.238 + 0.265) + 0.281\\times vmiles\\\\ \\hat{mrall} = 0.027 + 0.281\\times vmiles \\end{equation}\\] The results tell us that, on average, states with mandatory jail sentencing have a higher traffic fatality rate of 0.265 per 10,000, all else equal. We already know how to interpret the estimate for vmiles in Table 7.2: The results suggest that, on average, as the average miles driven per driver in a state increases 1 mile, traffic fatalities per 10,000 increase 0.281, all else equal. To help make this concrete, Figure 7.2 visualizes the results of our parallel slopes model. Note that, as expected, the blue line is above the red line. Based on the results, we know the blue line is above the red line by 0.265. We also know that the slope for both lines is 0.281. Figure 7.2: Parallel slopes visualization 7.2.2 Variations We can include a categorical variable with more than two levels. Suppose we were interested in whether traffic fatalities differ across U.S. regions. From the variables in Table 7.1, we might choose to construct the following model. \\[\\begin{equation} mrall = \\beta_0 + \\beta_1vmiles + \\beta_2region + \\epsilon \\tag{7.7} \\end{equation}\\] where region is a four-level categorical variable containing South, West, N. East, and Midwest. Figure 7.2 visualizes this model. Note that there are now four regression lines, each corresponding to one of the four regions. There are clear differences between the regions. It appears states in the West and South are somehow different than states in the Midwest and Northeast with respect to traffic fatality rates. Figure 7.3: Parallel slopes for 4 groups Running this model produces the following results. Table 7.3: Parallel slopes for regions term estimate std_error statistic p_value lower_ci upper_ci intercept 0.260 0.167 1.553 0.121 -0.069 0.589 vmiles 0.188 0.021 8.895 0.000 0.147 0.230 regionN. East -0.076 0.065 -1.166 0.245 -0.204 0.052 regionSouth 0.519 0.056 9.283 0.000 0.409 0.630 regionWest 0.641 0.062 10.264 0.000 0.518 0.763 Note that Table 7.3 provides estimates for three of the four regions. This is no different from our previous model; jaild has two levels, yes and no, but Table 7.2 provides only one estimate for jaild=yes. No matter how many levels in a categorical variable, one of the levels is set such that \\(d=0\\) and so that level drops out of the equation. Just like with the previous model where the estimate for jaild=yes indicates how far above or below the regression line is relative to the line for jaild=no, the estimates for whatever levels remain in the equation indicate how far above or below the regression lines are relative to the excluded level. In Table 7.3, Midwest is excluded. Look at Figure 7.2, noting where the Midwest line is relative to the other regions. Northeast is below Midwest, while South and West are above it. The estimates for the three included regions in Table 7.3 tell us how far the regions are above and below Midwest. Therefore, we could report something like On average and controlling for average miles driven per driver, states in the Northeast region experience fewer traffic fatalities by approximately 0.08 per 10,000, while states in the South and West experience higher traffic fatality rates by 0.52 and 0.64, respectively. 7.3 Interaction model But what if we allowed the two slopes in Figure 7.2 to differ? If our expertise leads us to theorize the two slopes should differ between states with and without mandatory jail sentencing for drunk driving, and/or if the visualizing the data suggests they do, then we can choose to use an interaction model. To be clear, allowing the two slopes to differ means we allow the marginal effect of the average miles driven per driver to differ between the two groups of states. Figure 7.3 visualizes this added flexibility. Note that the slope of the red line is indeed slightly steeper than the blue line. Thus, this visualization suggests that average miles driven per mile in states without mandatory jail sentences for drunk driving increases the traffic fatality rate by slightly more than it does in states with such laws. Figure 7.4: Interaction model visualization This version of an interaction model involves interacting a categorical variable with a numerical variable. Equation (7.8) represents this version of the interaction model. \\[\\begin{equation} y = \\beta_0 + \\beta_1x + \\beta_2d + \\beta_3xd + \\epsilon \\tag{7.8} \\end{equation}\\] Equation (7.8) is similar to Equation (7.1) for the parallel slopes model. The difference in \\(\\beta_3xd\\). This is the interaction–multiplying two of the explanatory variables in our regression model. Equation (7.9) provides the sample equation of this interaction model. \\[\\begin{equation} \\hat{y} = b_0 + b_1x + b_2d + b_3xd \\tag{7.9} \\end{equation}\\] Following the same process as with the parallel slopes model, we can rearrange Equation (7.9) to examine the logic of this interaction model. If \\(d=0\\), then \\(b_2\\) and \\(b_3x\\) drop out of the model because they are multiplied by 0. Thus, we have the same sample regression equation as Equation (7.3). \\[\\begin{equation} \\hat{y} = b_0 + b_1x \\tag{7.10} \\end{equation}\\] If \\(d=1\\), then \\(b_2\\) and \\(b_3x\\) remain in our model. As with the parallel slopes model, \\(b_2\\) combines with \\(b_0\\). This shifts the y-intercept for the group for which \\(d=1\\) either above or below the group for which \\(d=0\\) by the amount equal to \\(b_2\\). Because the lines are not parallel, just because a line starts above or below another does not mean it will stay above or below. It depends on the value of the \\(b_3x\\). The term \\(b_3x\\) is combined with \\(b_1x\\). Thus, if \\(d=1\\), we have the following sample regression equation \\[\\begin{equation} \\hat{y} = b_0 + b_1x + b_2d + b_3xd\\\\ \\hat{y} = b_0 + b_1x + b_2 + b_3x\\\\ \\hat{y} = (b_0 + b_2) + (b_1 + b_3)x \\tag{7.11} \\end{equation}\\] In addition to the regression line for which \\(d=1\\) having an intercept above or below the regression line for which \\(d=0\\) by the amount \\(b_2\\), it will have a slope greater or lesser by the amount \\(b_3\\). Hopefully, it is clear how these equations correspond with the lines in Figure 7.4. 7.3.1 Using an interaction Running this interaction model on these data produces the following results Table 7.4: Interaction model results term estimate std_error statistic p_value lower_ci upper_ci intercept -0.384 0.221 -1.741 0.083 -0.819 0.050 vmiles 0.300 0.028 10.634 0.000 0.245 0.356 jaildyes 0.731 0.399 1.831 0.068 -0.054 1.516 vmiles:jaildyes -0.058 0.050 -1.178 0.240 -0.156 0.039 Once again, we can plug these values into our generic sample regression equation to obtain the following equation. \\[\\begin{equation} \\hat{mrall} = -0.38 + 0.3\\times vmiles + 0.73\\times jaild - 0.06\\times vmiles \\times jaild \\tag{7.12} \\end{equation}\\] For states without mandatory jail sentencing (jaild = 0), the equation is \\[\\begin{equation} \\hat{mrall} = -0.38 + 0.3\\times vmiles \\tag{7.13} \\end{equation}\\] and for states with mandatory jail sentencing (jaild = 1), the equation is \\[\\begin{equation} \\hat{mrall} = -0.38 + 0.3\\times vmiles + 0.73\\times jaild - 0.06\\times vmiles \\times jaild\\\\ \\hat{mrall} = 0.35 + 0.24\\times vmiles \\tag{7.14} \\end{equation}\\] 7.3.2 Variations Variations on interaction models are beyond the scope of this book, but suffice it to say we can interact any two variables we deem necessary (or more). If you suspect that the effect of a variable on an outcome depends on the value of another variable, then an interaction is how to model such a relationship. 7.4 Linear probability model We have now covered the inclusion of categorical variables on the explanatory side of a regression model. We can also include categorical variables as an outcome. In fact, many interesting question involve outcomes of a categorical nature, particularly binary. For example, Did the person graduate from college (yes or no)? Did the government default on its bond payments? Did the program participant get a job afterward? Did the nonprofit receive the grant it applied for? As before, we may want to explain or predict such outcomes based on a set of explanatory variables. A linear probability model (LPM) is just a special name we give the kind of regression we have already covered when the outcome is a dummy variable. The key difference between an LPM and what we have already done concerns probability. Whereas regression with a numerical outcome explains or predicts changes or values of the outcome in terms of the outcome’s units, the LPM explains or predicts changes or values in the probability that the dummy outcome equals 1. If the dummy outcome is coded such that \\(d=1\\) means the event did occur, then the LPM estimates the probability that the event in question occurs. Equation (7.15) shows the generic population LPM, which is the same as the generic multiple regression population model except for the left-hand side. All this equation is trying to denote is that our estimates on the right-hand side pertain to the probability (Pr) that \\(y=1\\). Equation (7.16) shows the sample LMP equation. \\[\\begin{equation} Pr(y=1)=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots+\\beta_kx_k+\\epsilon \\tag{7.15} \\end{equation}\\] \\[\\begin{equation} \\hat{Pr(y=1)}=b_0+b_1x_1+b_2x_2+\\cdots +b_kx_k \\tag{7.16} \\end{equation}\\] Again, nothing is different with respect to how we use the above equations to answer questions concerning the predicted change or value of the outcome. We simply need to remember that those changes or values will be expressed as probabilities that \\(y=1\\) and what it means for \\(y\\) to equal 1 in the context of our particular question. 7.4.1 Using LPM What if instead of modeling traffic fatality rates as an outcome dependent on miles driven and mandatory jail sentencing for drunk driving, we modeled whether a state has mandatory sentencing as an outcome dependent on traffic fatality rates and region? Perhaps states passed such laws because they have high traffic fatality rates. Equation (7.17) represents our model in this case. \\[\\begin{equation} Pr(jaild=1)=\\beta_0+\\beta_1vmiles+\\beta_2region+\\epsilon \\tag{7.17} \\end{equation}\\] Scatter plots for LPMs are not particularly useful for communicating to an audience, but they can provide insight to what it is we are trying to do with an LPM, if it is not yet clear. Figure ?? changes the coding of jaild from yes/no to 1/0 and plots it on the y axis against traffic fatality rates on the x axis. Regions are excluded for simplicity. Figure 7.5: LPM visualization Immediately, we should notice Figure 7.5 does not look like the typical scatter plots we have viewed thus far. This is because all observations fall into one of two values for jaild. It is also difficult to tell how our regression line is being drawn through the data. The points along the x axis are states for which jaild = 0, meaning they do not impose mandatory jail sentencing for drunk driving. The points at the top are states for which jaild = 1. Compared to the points at the bottom, note the slight shift to the right the points at the top seem to have made. This shift is what informs the regression line to slope upward. The pattern of these data suggests there is a positive association between traffic fatality rate and passing a mandatory jail sentencing for drunk driving. The values of \\(y\\) along the regression line are the predicted probabilities that a state has a mandatory jail law given the corresponding values for traffic fatality rate. For example, states with a rate of 3 appear to have a probability of 0.5 or 50% of passing a mandatory jail law. Running this model produces the following results Table 7.5: LPM results term estimate std_error statistic p_value lower_ci upper_ci intercept -0.066 0.102 -0.645 0.520 -0.267 0.135 mrall 0.117 0.054 2.171 0.031 0.011 0.222 regionN. East 0.064 0.070 0.913 0.362 -0.074 0.202 regionSouth 0.040 0.068 0.580 0.562 -0.094 0.173 regionWest 0.361 0.078 4.634 0.000 0.208 0.514 Plugging these results into our sample regression equation gives us \\[\\begin{equation} \\hat{Pr(jaild=1)}= -0.07 + 0.12 \\times vmiles + 0.06 \\times neast + 0.04 \\times south + 0.36 \\times west \\tag{7.18} \\end{equation}\\] Once again, we are back to plug-and-chug. What is the predicted probability that state in the Midwest with a traffic fatality rate of 2.5 has a mandatory jail sentence for drunk driving? Answer: -0.07 + 0.12*2.5 ## [1] 0.23 there is a 23% likelihood that such a state has such a law. How would an increase of 2 fatalities per 10,000 affect the probability that a state imposes a mandatory jail law? Answer: 0.12*2 ## [1] 0.24 an increase in probability by about 24%. Furthermore, our results suggest states in the West are substantially more likely to have this law. Compared to the Midwest, the West is 36% more likely to have a law and about 30% more likely than states in the South and Northeast. 7.4.2 Fit Because our outcome is a dummy variable, it does not have the same kind of variation we need to assess model fit like before using \\(R^2\\) or RMSE. Since we are explaining or predicting whether or not an outcome occurs, we can assess the fit of the model based on how well it predicts the observed outcomes. We could change this threshold, but suppose we decide that if our model predicts the likelihood an outcome at 50% or greater, then we say that our model predicts the outcome will occur \\(y=1\\) and so \\(y=0\\) otherwise. Table @ref(tab: lpmpointstab) shows a few rows applying this logic. Note that each row shows the observed data for each variable in our model, then the predicted probability in the jaild_hat column, then the rounding of that probability to 0 or 1 in the prediction column. Note the similarities and differences between the observed outcomes in jaild and the predicted outcomes prediction. Sometimes our model predicts correctly, and sometimes it does not. Table 7.6: Binary predictions from LPM ID jaild mrall region jaild_hat prediction 29 0 2.174 West 0.549 1 113 1 1.461 N. East 0.169 0 254 0 0.821 N. East 0.094 0 98 1 1.936 Midwest 0.160 0 68 0 2.575 West 0.596 1 241 1 2.080 West 0.538 1 182 0 1.825 N. East 0.211 0 Table 7.7 below is referred to as a confusion matrix. It is simply a cross-tabulation of the observed outcomes and the predicted outcomes with the predictions along the top as columns. Table 7.7: Confusion matrix for LPM 0 1 0 210 31 1 56 38 We can see that there are 248 cases where our model correctly predicts the outcome (210 + 38). There are 87 cases where our model incorrectly predicts the outcome. Specifically, there are 31 cases where our model predicts a state has a law but doesn’t and 56 cases where our model predicts a state does not have a law but does. We can also convert these to percentages like the table below. These confusion matrices help us assess and communicate how accurate our model is. Table 7.8: Confusion matrix for LPM (in proportions) 0 1 0 0.7894737 0.4492754 1 0.2105263 0.5507246 To Learn how to include interactions in a regression using R, proceed to Chapter 22. 7.5 Key terms and concepts Dummy variable Parallel slopes Interaction Difference between parallel slopes and interaction models Linear probability model (LPM) Confusion matrix "],
["8-nonlinear-variables.html", " 8 Nonlinear Variables 8.1 Learning objectives 8.2 Quadratic 8.3 Log models 8.4 Key terms and concepts", " 8 Nonlinear Variables “The shortest distance between two points is often unbearable.” —Charles Bukowski So far, we have repeatedly drawn straight lines through points. But, we know not all relationships are linear. Our income tends to rise and fall with age. Those in charge of the purchasing or production of something should know that average and marginal costs fall and then rise with quantity. Happiness tends to rise sharply with income but then plateaus at around $70,000 per year. If our goal is to draw the line that fits data best, why draw a straight line through data that is evidently nonlinear? In this chapter, we will cover two ways to incorporate nonlinear relationships: Include a quadratic Include a logarithmic transformation 8.1 Learning objectives Explain why and how to extend a regression model to include a quadratic relationship Interpret the coefficients associated with a quadratic term in a regression model Compute the value of the quadratic explanatory variable at which the outcome is at its maximum or minimum Explain the difference between percent change and percentage point change Explain why to log-transform variables in a regression model Interpret results from log-log, log-level, and level-log models 8.2 Quadratic If we theorize or see visual evidence that the association between an explanatory variable and an outcome is such that the outcome initially increases or decreases as the explanatory variable increases, then, at some value of the explanatory variable, the outcome decreases, then we may want to include a quadratic term of that explanatory variable in our regression model. That long-winded statement warrants an immediate visualization provided by Figure 8.1 below. Note that wage appears to initially increase with age, then decreases. The data present a pattern that resembles an inverted U, also known as a concave parabola. Age and wage is a classic example of a quadratic relationship. We should not force ourselves to fit a straight line to these data; we can estimate a better line. Figure 8.1: Wages by age Equation (8.1) presents a generic population regression model with a quadratic term. With respect to the math, the only difference between this and previous models is the choice to square one of the explanatory variables. This is just an example. Any number of explanatory variables can be squared if theory warrants it. \\[\\begin{equation} y = \\beta_0 + \\beta_1x_1 + \\beta_2x_1^2 + \\beta_3x_2 + \\cdots + \\beta_kx_k + \\epsilon \\tag{8.1} \\end{equation}\\] Thus, the sample equation is as follows \\[\\begin{equation} \\hat{y} = b_0 + b_1x_1 + b_2x_1^2 + b_3x_2 + \\cdots + b_kx_k \\tag{8.2} \\end{equation}\\] Fully understanding the logic of the above equations to answer questions we may encounter as we have done before involves calculus that this book will spare you. In order to report the marginal effect of a variable that has been squared on an outcome in regression, we use Equation (8.3) below. The result of this equation provides the predicted change in \\(y\\) given a one-unit change in \\(x_1\\). \\[\\begin{equation} b_1 + 2b_2x_1 \\tag{8.3} \\end{equation}\\] Note that had we not squared \\(x_1\\) the predicted change in \\(y\\) from a one-unit change in \\(x_1\\) would be \\(b_1\\), which is exactly the same as in previous models. However, now that we are drawing a curved line, the effect of a one-unit change in \\(x\\) on \\(y\\) is not constant; it changes depending on the value of \\(x\\). Another important question when a quadratic relationship is involved is at what value of \\(x\\) is \\(y\\) maximized or minimized. This can help decision-making, such as how to minimize costs or maximize profit, or maximize the probability of some desirable outcome. In order to report the value of \\(x\\) at which \\(y\\) reaches its maximum or minimum, we use Equation (8.4) below. The result of the equation gives us the optimal value of \\(x\\). \\[\\begin{equation} x = {\\frac{-b_1}{2b_2}} \\tag{8.4} \\end{equation}\\] 8.2.1 Using quadratics Suppose we collect the following data. Table 8.1: Preview of wages, age, and education Wage Educ Age 19.13 9 35 25.67 12 37 38.77 21 41 32.37 17 53 19.34 6 38 18.76 18 73 18.11 14 21 Therefore, our regression equation is as follows \\[\\begin{equation} Wage = \\beta_0 + \\beta_1Age + \\beta_2Age^2 + \\beta_3Educ + \\epsilon \\tag{8.5} \\end{equation}\\] Running this regression generates the following results Table 8.2: Quadratic model results term estimate std_error statistic p_value lower_ci upper_ci intercept -22.722 3.023 -7.517 0 -28.742 -16.701 Age 1.350 0.134 10.077 0 1.083 1.617 I(Age^2) -0.013 0.001 -9.840 0 -0.016 -0.011 Educ 1.254 0.090 13.990 0 1.075 1.432 In Table 8.2, note that there are two rows for Age–one for the linear or level term and a second for the quadratic term. When the quadratic relationship is an inverted U, or concave, the linear term will be positive and the quadratic will be negative. This corresponds with an initial positive relationship that eventually turns negative once the negative quadratic term overcomes the positive linear term. Plugging our results from Table 8.2 into the regression equation, we obtain the following equation \\[\\begin{equation} \\hat{Wage} = -22.7 + 1.4\\times Age - 0.01\\times Age^2 + 1.3\\times Educ \\tag{8.6} \\end{equation}\\] We can answer questions regarding the predicted value of Wage the same way as before. For example, the predicted wage of an individual who is 40 years old with 16 years of education is -22.7 + 1.4*40 - 0.01*40^2 + 1.3*16 ## [1] 38.1 dollars per hour. To predict the change in wage given a change in age, we need to know the beginning point for age. For example, if we were asked what is the predicted change is wage for a 24-year old two years later who consequently increases their education from 16 to 18 to get their masters degree, we plug this scenario into Equation (8.3) like so 2*(1.4 - 2*0.01*24) + 1.25*2 ## [1] 4.34 providing us the answer of a predicted increase of $4.34. Controlling for education, at what age do wages tend to reach their maximum? To answer, we plug the results into (8.4) like so -1.35/(2*-0.013) ## [1] 51.92308 According to our results, wages reach their maximum at around 52 years of age. 8.3 Log models Once again, we will forego the math of logarithms and instead focus on why we may want to use them in a regression model and how to interpret the results. In short, logarithms are used to express rates of change in a variable (i.e. percent change) rather than absolute change in a variable (i.e. unit change). 8.3.1 Logarithmic scales Graphs like Figure 8.2 below were commonplace during the initial COVID-19 spread. Take a look at the small note at the bottom explaining to readers the purpose of a logarithmic scale. Note how the values on the y axis are evenly dispersed, but each tick mark increases by a factor of 10 (i.e. the previous value multiplied by 10). The y-axis is in a log10 scale. Another common log scale for visualization is a log2 scale which increases each interval by a factor of 2. The use of a log scale allows us to compare states like New York and Wyoming by taking into account large differences in absolute numbers. It would be unfair to compare the absolute number of COVID cases in New York to the absolute number of cases in Wyoming. It would also be unfair to compare the absolute number of new cases each day between the two states. A non-trivial portion of those numbers is a result of the size of the population in each state. However, it is fair to compare the rate of growth between the two states. While it is obviously concerning that New York has over 300,000 deaths, the key feature of this graph is that we can compare the slopes of each state’s growth path because population size has been accounted for by the y-axis. Given New York’s population and population density, it was likely to have the most cases, but the state also had the fastest growth rate in cases from about day 5 to day 10. Figure 8.2: Growth in COVID-19 cases by state 8.3.2 Percent v percentage point change Rate of change typically refers to percent change. The equation for percent change is shown below. \\[\\begin{equation} PctChange = {\\frac{NewValue-OldValue}{OldValue}} \\times 100 \\tag{8.7} \\end{equation}\\] For example, if the number of COVID cases increase from 1,000 to 10,000 over the course of a week, the absolute change is 9,000. The percent change is 900%. A common cause of confusion is the difference between a percent change and a percentage point change. This occurs when we discuss the change in a variable that is already expressed as a percent. For example, if the U.S. unemployment rate increases from 4% to 15% during the pandemic, that’s an absolute change of 11 percentage points. The unemployment rate is expressed units of percentage points, so a unit change is a percentage point change. An increase from 4% to 15% is also a 275% percent change. As we have seen in previous examples of regression, when we include a variable that is not log-transformed, regression estimates the unit change in \\(y\\) given a unit change in \\(x\\). If a variable is expressed in units of percentages like unemployment or poverty, then a unit change for those variables is a percentage point change. Including a log-transformed variable in regression estimates percent changes in the variable(s) we transformed. One reason we may prefer to use percent change is if the variable in question has some underlying impact that differs depending on the initial value from which it changed. This too applies to measures of wealth or income. Suppose we estimate that a policy will, on average, increase peoples’ incomes by 12,000 dollars. This average unit change does not quite capture the benefit of the policy. Imagine a society of two people. One person earns 20,000 dollars per year and the other earns 80,000 dollars. That 12,000 likely has a greater positive impact on the low-income individual than it does the high-income individual. Consequently, this can be expressed in percent change. The 12,000 represents a 60% increase in income for the low-income individual and 15% for the high-income individual. 8.3.3 Why logs in regression To summarize, we may want to use logs in regression if it is preferable to express change in percentages rather than units a variable we intend to include has a skewed distribution we theorize the relationship between two variables follows a logarithmic path Let’s consider these last two reasons further. As was mentioned, log scales allow us to compare numbers that are very far apart, as seen in Figure 8.2. If the scale for COVID cases were left in constant units, New York and a few other states would be so far above most other states that it would be difficult to fit in a sensible graph. Using logs condensed or pulled those extreme numbers back to a more compact distribution. As will become clear in the next section on inference, using a sample to make valid conclusions about a population relies heavily on the normal distribution, which was introduced in Chapter 4. In a similar sense, we want the variables we use for inference to be approximately normally distributed because extreme values of a skewed distribution can impose undue influence on our results. Log-transformations can transform a skewed distribution to be more normal. For instance, variables that measure income or wealth tend to be right-skewed. Figure 8.3 shows the distribution of GDP per capita across most countries in multiple years. Clearly this distribution is not normal and skewed to the right. It is difficult to see because there are so few cases, but some countries have GDP per capita near or more than $120,000. Figure 8.4 shows the distribution if we convert GDP per capita to a log scale (log10 was used but any log scale will achieve the same normalization). Now we have a more normal distribution. This is desirable in statistics. Figure 8.3: Distribution of GDP per capita Figure 8.4: Distribution of log10 GDP per capita The third reason for using logs concerns theory, which should always inform the choices we make in statistics. When choosing how to model the relationship between an outcome and an explanatory variable, if past research, experience, visualization of data, or intuition tells us that the outcome changes dramatically at first, then begins to flatten, a logarithmic transformation should be used. For example, suppose we wish to examine the relationship between national wealth and life expectancy. Intuitively, we expect this relationship to be positive–as wealth increases, life expectancy should increase. Also, life expectancy has some natural ceiling, so it cannot increase indefinitely, and we may expect relatively small increases from low levels of wealth to have much greater impacts on life expectancy then similar increases from high levels of wealth. Figure 8.5 provides a scatter plot of GDP per capita and life expectancy in their original units. Note the rapid increase then plateau in life expectancy. A regression line would not fit these data well. Figure 8.5: Relationship between wealth and life expectancy using unit scale Using a log transformation in an association between two variables does not change the underlying data or relationship, but it does transform the pattern of points to be more linear, thus allowing a linear regression line to do a better job modeling the relationship. Figure 8.6 uses the same data but GDP per capita has been transformed to log scale. This simple change makes a big difference for the validity of any conclusions we may make regarding the relationship between wealth and life expectancy. Figure 8.6: Relationship between wealth and life expectancy using log scale 8.3.4 Using log models There are three variations of the log model: Level-log: log transforming one or more explanatory variables but not the outcome Log-level: log transforming the outcome but not an explanatory variable Log-log: log transforming the outcome and an explanatory variable Each model fits slightly different patterns of association best but they share the general pattern of a pronounced initial increase or decrease followed by a plateau. If past research, visuals, or theory does not lead us to choose one model over the other, one option is to compare the goodness-of-fit between the three, choosing the one with the highest \\(R^2\\) or lowest RMSE. One last point before presenting each of the models and how to interpret: using the logarithmic transformation uses a special log scale called the natural log, often denoted as \\(ln\\), as opposed to, say, \\(log_{10}\\) or \\(log_2\\). You do not need to concern yourself with the mathematical properties of the natural log. Just know that the natural log is what is used in regression to transform unit changes to percent changes. Log-log The log-log model is somewhat special among the three variations because it estimates a commonly used measure in economic or policy analyses–the elasticity. You may have already learned in policy analysis courses that the elasticity is the percent change in an outcome given a one percent change in the explanatory variable. Equation (8.8) presents a generic log-log model. Log-log is simply meant to convey that we logged our outcome and logged at least one explanatory variable. \\[\\begin{equation} ln(y)=\\beta_0 + \\beta_1ln(x_1) + \\beta_2x_2 + \\cdots + \\beta_kx_k + \\epsilon \\tag{8.8} \\end{equation}\\] Thus, the sample equation is \\[\\begin{equation} \\hat{ln(y)}=b_0 + b_1ln(x_1) + b_2x_2 + \\cdots + b_kx_k \\tag{8.9} \\end{equation}\\] When we obtain an estimate for \\(b_1\\) we can plug it into the following template On average, a one percent change in \\(x_1\\) is associated with a \\(b_1\\) percent change in \\(y\\), all else equal. Or, if we wanted to report using elasticity language, assuming our audience understands what we are talking about: According to the results, the \\(x_1\\) elasiticy of \\(y\\) is \\(b_1\\). Level-log Equation (8.10) presents a generic level-log model. Level-log is simply meant to convey that we logged at least one explanatory variable. \\[\\begin{equation} y=\\beta_0 + \\beta_1ln(x_1) + \\beta_2x_2 + \\cdots + \\beta_kx_k + \\epsilon \\tag{8.10} \\end{equation}\\] Thus, the sample equation is \\[\\begin{equation} \\hat{y}=b_0 + b_1ln(x_1) + b_2x_2 + \\cdots + b_kx_k \\tag{8.11} \\end{equation}\\] When we obtain an estimate for \\(b_1\\) we can plug it into the following template On average, a one percent change in \\(x_1\\) is associated with a \\(\\frac{b_1}{100}\\) unit change in \\(y\\), all else equal. Or, if dividing our estimate by 100 results in too small of a number to report, we can say the following On average, a doubling of \\(x_1\\) is associated with a \\(b_1\\) unit change in \\(y\\), all else equal. because a doubling is equal to a 100 percent increase. Multiplying \\(\\frac{b_1}{100}\\) by 100 cancels out the 100 in the denominator, leaving us with just \\(b_1\\). Log-level Equation (8.12) presents a generic log-level model. Log-level is simply meant to convey that we logged our outcome. \\[\\begin{equation} ln(y)=\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_kx_k + \\epsilon \\tag{8.12} \\end{equation}\\] Thus, the sample equation is \\[\\begin{equation} \\hat{ln(y)}=b_0 + b_1x_1 + b_2x_2 + \\cdots + b_kx_k \\tag{8.13} \\end{equation}\\] When we obtain an estimate for \\(b_1\\) we can plug it into the following template On average, a one unit change in \\(x_1\\) is associated with a \\(b_1 \\times 100\\) percent change in \\(y\\), all else equal. Example Let’s continue our investigation of national life expectancy using the various log models. Suppose we are interested in using the following base model for the three varieties of log models. Continent is included because perhaps we think it will capture some geographical, social, and/or cultural differences that impact life expectancy. \\[\\begin{equation} LifeExp=\\beta_0 + \\beta_1GDPpercap + \\beta_2Continent + \\epsilon \\tag{8.14} \\end{equation}\\] The following tables present results for each of the three log models. Table 8.3: Log-log results term estimate std_error statistic p_value lower_ci upper_ci intercept 3.062 0.026 117.692 0 3.011 3.113 log(gdpPercap) 0.112 0.004 31.843 0 0.105 0.119 continentAmericas 0.133 0.011 12.519 0 0.112 0.154 continentAsia 0.110 0.009 12.037 0 0.092 0.128 continentEurope 0.166 0.012 14.357 0 0.143 0.189 continentOceania 0.152 0.029 5.187 0 0.095 0.210 Table 8.4: Level-log results term estimate std_error statistic p_value lower_ci upper_ci intercept 2.317 1.359 1.704 0.088 -0.349 4.983 log(gdpPercap) 6.422 0.183 35.003 0.000 6.062 6.782 continentAmericas 7.015 0.554 12.652 0.000 5.927 8.102 continentAsia 5.912 0.477 12.400 0.000 4.977 6.847 continentEurope 9.577 0.604 15.855 0.000 8.392 10.762 continentOceania 9.213 1.536 5.999 0.000 6.201 12.226 Table 8.5: Log-level results term estimate std_error statistic p_value lower_ci upper_ci intercept 3.856 0.006 601.881 0 3.843 3.869 gdpPercap 0.000 0.000 16.374 0 0.000 0.000 continentAmericas 0.250 0.011 22.054 0 0.228 0.272 continentAsia 0.160 0.010 15.322 0 0.140 0.181 continentEurope 0.311 0.012 26.383 0 0.288 0.334 continentOceania 0.316 0.034 9.381 0 0.250 0.382 Our log-log results indicate that a one percent increase in GDP per capita is associated with a 0.11 percent increase in life expectancy, on average and controlling for continent. Our level-log results indicate that a one percent increase in GDP per capita is associated with an increase in life expectancy of 0.06 years. Our log-level results indicate that a one dollar increase in GDP per capita is associated with a indiscernible percent increase in life expectancy. Changing GDP per capita from dollars to something like thousands of dollars would probably give an estimate that doesn’t round to 0. The continent estimates can be interpreted in similar fashion, remembering that with categorical variables, the estimate of each level of the variable is relative to the base comparison excluded from the equation. In this example, Africa is the base comparison. Let’s focus on Asia for interpretation. Our log-log results indicate that life expectancy in Asia is 110% greater than life expectancy in Africa. Since a dummy variable can only change from 0 to 1, this is equivalent to a 100 percent change. Therefore, we must multiply our estimate by 100. Our level-log results indicate that life expectancy in Asia is 5.9 years greater than life expectancy in Africa. Lastly, our log-level results indicate that life expectancy in Asia is 160% greater than life expectancy in Africa. To learn how to include nonlinear variables in regression using R, proceed to Chapter 23. 8.4 Key terms and concepts Marginal effect Logarithmic scale Percent change Percentage point change Natural log Elasticity "],
["9-causation-and-bias.html", " 9 Causation and Bias 9.1 Learning objectives 9.2 Causality 9.3 Directed acyclical graphs 9.4 Direction of OVB 9.5 Usefulness 9.6 Key terms and concepts", " 9 Causation and Bias “All models are wrong but some are useful.” —George Box Our regression toolbox has grown considerably over the previous three chapters. We can now set out to answer a multitude of research questions that require us to accommodate different types of variables that may share linear or nonlinear relationships. Nevertheless, a model is a simplified version of our complex world. In this sense, all models are wrong. The goal is to make modeling choices that prevent our model from being so wrong that they are useless for decision-making. 9.1 Learning objectives Explain the difference between using regression to predict an outcome versus explain an outcome and the consequences each has on model choices Explain internal and external validity Explain the criteria to validly claim a causal relationship Use a directed acyclical graph (DAG) to represent a regression model Identify confounders and colliders in a DAG Identify the number of backdoor paths in a DAG and each are open or closed Given a DAG, identify the variables one would need to control for to close any backdoor paths Determine whether a regression model plausibly eliminates omitted variable bias Predict the direction of omitted variable bias First, let’s consider the two possible goals of regression: Explain the change in an outcome due to a change in a set of explanatory variables Predict the value of an outcome given values for a set of explanatory variables The usefulness of a model with the sole goal of prediction is how well it predicts the outcome. That may sound obvious and cyclical but it is an important point. Which variables we choose to include and their linear or nonlinear relationships is a secondary concern of prediction, if at all. If we don’t care how variables affect the outcome and only care about predicting the outcome with the greatest accuracy and precision possible, then we can throw together whatever model we want to achieve that without much concern for what the model actually means. Models with the goal of prediction are common within the field of forecasting, which will be introduced in Chapter 14. Sometimes, we care about good prediction and how some explanatory variables impact the outcome. Then we are back in the realm of explanation where we have to take special care about which variables are included and excluded from our model as well as how they relate with each other. The focus of this chapter is explanation using regression. Many scenarios within program evaluation or policy analysis involve explaining whether and to what extent one variable impacts another we care about changing. Ultimately, our concern is causality. It is one thing to conclude two variables are associated with each other; it is an entirely different thing to conclude that a change in one variable causes the other to change. If we propose to spend millions of dollars on a program to help people, or decide to cut a program that does not, we should be as certain as about this causal claim as statistics allows us to be. Sufficient understanding of causality or causal inference warrants its own course. It does not involve regression models that much different from what you have learned so far or will learn by the end of this book, but it does involve a broader and deeper understanding of research design and knowing how to identify threats to internal and external validity. Let us revisit the figure of credible analysis first shown in Chapter 3. Figure 9.1: Components of credible analysis Chapter 3 covered measurement validity and measurement reliability. Now, let us define internal validity and external reliability. Internal validity: the credibility of the theoretical assumptions applied to the causal connection established between the explanatory variable(s) and its (their) effect on the outcome. External validity: can results of the analysis be applied beyond the subjects included or context involved? In other words, is there reason to believe our results are critically mistaken and are those affected or the context in which they were affected so unique or limited that we can not generalize to other potential targets or contexts? 9.2 Causality Three conditions must be met to credibly claim a causal relationship. We will address each in turn. The explanatory variable is correlated with the outcome The change in the explanatory variable occurred prior to the change in the outcome No alternative explanation exists to which the change in the outcome could be attributed instead of the explanatory variable Correlation between the explanatory and outcome variables is perhaps the most straightforward condition to satisfy. We have not yet covered inference and how to identify statistically significant results. For now, suffice it to say that if we run a regression and the estimate for our explanatory variable is statistically significant, then we have established correlation between the explanatory and outcome variables that is unlikely to be random. The second condition is succinctly referred to as temporal precedence. In order for something to be a cause, it must occur prior to its alleged effect. Otherwise, perhaps it is our supposed outcome that is having an effect on the cause, similar to what was considered in Chapter 7 with the mandatory jail for drunk driving. This is also known as reverse causality. Reverse causality could still be argued even when it seems clear the cause occurred prior to the effect. For example, the mere availability of a scholarship may cause students to reach higher levels of academic achievement. If we were to claim receipt of the scholarship caused a rise in the likelihood of completing college, which obviously occurred prior to graduation, this may not be accurate. Perhaps by virtue of motivating oneself to perform better in school would result in a higher likelihood of graduation whether the scholarship was received or not. Then again, the student would not have been as motivated if not for the scholarship. Perhaps the availability of the scholarship would meet temporal precedence more convincingly. As may be evident by now, something as seemingly simple as before-and-after can become complex if the causal pathway is considered carefully. Credible causal claims require subject matter expertise as much as quantitative skills. For temporal precedence, do your best to ensure your explanatory variable was measured or occurred prior to when your outcome was measured or occurred. The remainder of this chapter concerns the third and most difficult condition to satisfy: no plausible rival or alternative explanation for our causal claim. Since we do not have the luxury of delving deep into causal modeling, what can MPA students learn that will serve them well when they need to consider whether the possibility of alternative explanations has been reasonably eliminated? My answer to this is the directed acyclical graph. 9.3 Directed acyclical graphs Directed acyclical graphs (DAGs) are visual representations of causal pathways. Constructing a DAG involves theory, existing research, or theory. A DAG requires us to state our assumptions clearly, thus allowing us and others to evaluate the internal validity of our model. Figure 9.2 below shows a basic DAG. This model involves three variables, X, Y, and Z. Y denotes the outcome and X denotes the variable of primary interest for which we intend to estimate a causal effect. Z denotes any other variables in the model. The arrow from X to Y indicates our claim that X causes changes in Y. X also causes Z to change, and Z causes Y to change. In this example, the claim is that X has a direct effect on Y and an indirect (mediated) effect on Y through Z. Figure 9.2: A basic DAG Any DAG follows a few rules and conventions: Only one-directional arrows (directed) No arrow from Y back to X (acyclical) Solid arrows used for relationships between observable variables or variables specifically observed in our data Dashed arrows used for relationships between unobservable variables (e.g. ability, attitudes, propensities for certain behaviors) or variables unobserved in our data Underlying every regression with the goal to explain a causal relationship is a DAG. Consider the following regression model \\[\\begin{equation} BMI = \\beta_0 + \\beta_1Exercise + \\epsilon \\tag{9.1} \\end{equation}\\] Figure 9.3 below shows the DAG that corresponds with Equation (9.1). The central claim is that exercise causes BMI to change. Therefore, there is a solid arrow from exercise to BMI. Also, note that because \\(\\epsilon\\), by definition, represents all other unobserved factors that affect BMI, there is a dashed line from \\(\\epsilon\\) to BMI. Lastly, this DAG makes the assumption that no variables contained in \\(\\epsilon\\) affects exercise, nor does exercise affect any variables contained in \\(\\epsilon\\) Figure 9.3: DAG representation of a regression model With this basic set-up, we can begin to learn how to evaluate a DAG in order to determine if our regression model credibly eliminates alternative explanations of our causal claim. 9.3.1 Evaluationg DAGs We will review two aspects of evaluating DAGs for causal claims: Identifying backdoor paths Adjusting regression models based on the presence of confounding or colliding variables A backdoor path is any indirect path from X to Y no matter the direction of the arrows that connect it. For example, Figure 9.2 has one backdoor path: X –&gt; Z –&gt; Y. Figure 9.3 has no backdoor paths between exercise and BMI. Confounders Identifying backdoor paths allow us to then identify whether confounding or colliding variables are present in our model. Figure 9.4 below shows a variation on the simple DAG with one backdoor path. The backdoor path still runs from X to Z to Y, but now the direction of the arrows connecting this path are different. Specifically, the backdoor path is X &lt;– Z –&gt; Y. Figure 9.4: Example of confounder variable The variable Z in this case is a confounder. Any variable in a backdoor path with arrows directed away from it toward X and Y is a confounder. This representation means Z affects X and Y. If as Z changes, X and Y change, then we may incorrectly attribute the effect of Z on Y to the effect of X on Y because it appears to us that as X changes, Y changes. And it does, but it is really Z that is causing changes. This example of a confounder is sometimes referred to as spurious correlation. For example, ice cream sales and crime are spuriously correlated due to both increasing because of temperature. It would be mistake to claim an increase in ice cream sales causes an increase in crime. Colliders Figure 9.5 below shows another variation of the simple DAG with one backdoor path between X and Y that goes through Z. The direction of the arrows are such that this backdoor path can be written as X –&gt; Z &lt;– Y. In this case, Z is a collider. Any variable on which the arrows connecting X and Y converge is a collider. Figure 9.5: Example of collider variable Z is just some variable that is affected by both X and Y. Changes in Z do not cause changes in X or Y. Therefore, if we estimate how much Y changes in response to a change in X, Z has nothing to do with that causal estimate. 9.3.2 Backdoor criterion For any theoretical model we intend to estimate via regression, we can now identify backdoor paths and whether there are confounding or colliding variables along those backdoor paths. How do we use this information to eliminate plausible alternative explanations and confidently claim a one-unit change in X causes Y to change by \\(\\beta\\)? We need to satisfy the backdoor criterion. The backdoor criterion is satisfied if all backdoor paths between X and Y are closed. We know if a backdoor path is open or closed depending on the presence of confounding or colliding variables. A confounder variable opens a backdoor path A collider variable closes a backdoor path If a backdoor path is open, we can close it by controlling for the confounder variable or any other variable along the backdoor path between X and Y. For instance if were were to identify a backdoor path of the following form X &lt;– Z –&gt; A –&gt; Y where A is some fourth variable in our model, then controlling for Z or A will close this backdoor path. If a set of control variables in our regression model closes all backdoor paths, then we have satisfied the backdoor criterion and we can consider our estimate of X on Y to be a causal estimate. Consider the simple backdoor path again where Z is a collider variable. X –&gt; Z &lt;– Y This backdoor path is already closed. We don’t need to control for anything in our model because of this backdoor path. In fact, controlling for a collider opens a backdoor path. Therefore, we should not control for Z in our model, as doing so opens a backdoor that was already closed. This is a valuable insight provided by the use of DAGs. If we already have the data, it costs us virtually no time or effort to include a variable in our model we think may affect our outcome Y, and it can be quite tempting to throw variables into a regression model for not much more reason than we have it in our data. However, a DAG forces us to consider all of the relationships between the variables in our model. If an explanatory variable is a collider, then including it may threaten our ability to make causal claims. Sometimes, deliberately excluding a variable from a regression model is the right choice and DAGs give us a fairly simple way to make and explain that choice. Consider another backdoor path where Z is a collider and A is a confounder X –&gt; Z &lt;– A –&gt; Y A opens this backdoor path but Z blocks A’s confounding. Controlling for Z would open this backdoor path. We can control for A in our regression without reopening the backdoor path, but it is not necessary. 9.3.3 DAGs and regression Let us relate this new information to making choices about regression models. Referring back to Equation (9.1) and Figure 9.3, recall that \\(\\epsilon\\) represents all other variables we do not observe or cannot include in our regression model but affect our outcome, BMI. Based on the DAG for this regression model, there are no backdoor paths. Therefore, whatever estimate we get for \\(\\beta_1\\) is causal estimate. However, Figure 9.3 is probably incorrect. There are likely variables contained in \\(\\epsilon\\) that affect exercise. If that is the case, then our DAG should be drawn like Figure 9.6 below. Now we have a backdoor path of the form Exercise &lt;– \\(\\epsilon\\) –&gt; BMI where \\(\\epsilon\\) is a confounder. Therefore, this backdoor path is open, and because we do not observe the variables in \\(\\epsilon\\), we do not currently have the means to close it. Figure 9.6: Counfounding error term The issue illustrated by Figure 9.6 is commonly referred to as omitted variable bias (OVB) and it is the bane of analysts’ attempts to estimate causal relationships. To have omitted variable bias means we have failed to satisfy the third criterion of causality. There is a variable out there we have not controlled for which causes our explanatory variable of interest and our outcome to change. Therefore, we cannot trust our estimate of the effect of our explanatory variable on our outcome because it may be due to the omitted variable. In other words, an omitted variable is biasing our estimate, \\(b_1\\) to be systematically above or below the population parameter \\(\\beta_1\\). Our next task then is to identify the set of variables that would eliminate the arrow from \\(\\epsilon\\) to exercise. If we can credibly break the link between \\(\\epsilon\\) and our explanatory variables, then we can credibly claim there is no omitted variable bias in our model. For the sake of this example, suppose healthy eating is the key omitted variable. Healthy nutrition gives us the energy to exercise and obviously affects our BMI. Suppose we collect a variable that measures the extent to which a person’s diet is healthy. Now, we have a new DAG, as depicted in Figure 9.7. Figure 9.7: Eliminating OVB To close the backdoor path in this DAG, we need to control for nutrition in our regression model. Equation (9.2) shows our new regression model. If our theory informing our new DAG is correct, then our estimates of \\(\\beta_1\\) and \\(\\beta_2\\) are unbiased. \\[\\begin{equation} BMI = \\beta_0 + \\beta_1Exercise + \\beta_2Nutrition + \\epsilon \\tag{9.2} \\end{equation}\\] The DAG in Figure 9.7 may still be fail to be sufficiently convincing to those who have expertise in public health or related fields. In that case, we would continue the process of identifying and controlling for variables until we credibly break the link between \\(\\epsilon\\) and all explanatory variables in our model. Isolating causal effects is hard, and careers can be made by successfully doing so. Regardless of whether an unbiased causal estimate can be obtained for a particular question, knowing whether or not threats exist and what could be done about it is valuable, especially for managers or consumers of statistical analyses who have expertise concerning the potential causal pathways involved. 9.4 Direction of OVB Fortunately, we can salvage estimates that suffer from omitted variable bias to make causal conclusions in some cases. Again, doing so requires us to have knowledge about the variables involved and their causal pathways. We will cover this more thoroughly in the following chapters, but a statistically significant result means we can confidently conclude the association between X and Y is not equal to zero. In other words, our regression results provide an estimate so much less or greater than zero that it would be highly unlikely to see these results if the true association were equal to zero. That is the issue with OVB: it causes our estimate to be lower or higher than what it should be. Therefore, OVB may lead us to conclude statistically significant results when otherwise there would not be statistically significant results in the absence of OVB; our estimate would not be sufficiently far from zero to confidently conclude a relationship. However, if we can predict the direction of the OVB–whether it is pushing our estimate below or above what it should be–then we may be able to salvage our results. For instance, suppose we obtain a statistically significant estimate of 10 that we suspect is biased due to an omitted variable. If we can credibly claim the OVB causes our estimate to be lower than what it should be, then we still have useful results; our result would be even greater than 10 if not for OVB. Similarly, if our estimate were -10 and we suspect the OVB causes our estimate to be greater than what is should be, then we would have an even lower estimate in the absence of OVB. The moral of this section is that when you or someone identifies a variable that may cause OVB, all is not lost. If OVB works against the estimate’s value relative to zero, then it actually lowers the likelihood of significant results that you still obtained. However, if OVB works with the estimate’s value relative to zero, then it increases the likelihood of finding the significant results you found when there may not be a significant relationship. How can we postulate the direction of OVB? Figure 9.8 below shows a simple confounding scenario where our estimate of the effect of X on Y is biased due to an omitted variable Z. If X and Y move in the same direction because of a change in Z, then OVB is positive. If X and Y move in opposite direction because of a change if Z, then OVB is negative. Figure 9.8: Predicting direction of OVB 9.5 Usefulness Just because you cannot claim a causal relationship does not mean you do not have useful results. It is worth reiterating the value of prediction. Regardless of whether we have a model that eliminates OVB, if it accurately predicts an outcome we would like to preempt, then it could be useful. Sure, we would like to know the underlying causes of an outcome, but the ability to accurately predict the likelihood of an outcome still allows policy or programs to intervene. If I have a model that is biased but accurately predicts students who will drop out of college, I will use that model to provide assistance to those likely to drop out. In addition to making a difference by helping my target population, perhaps I will gain insights into the underlying causes I have failed to include in my model. 9.6 Key terms and concepts Internal validity External validity Establishing correlation Temporal precedence Reverse causality DAG confounder collider backdoor path backdoor criterion Omitted variable bias "],
["10-sampling.html", " 10 Sampling 10.1 Learning objectives 10.2 Normal distribution 10.3 Sampling Distribution 10.4 Central Limit Theorem 10.5 Confidence intervals 10.6 Conclusion 10.7 Key terms and concepts", " 10 Sampling “This is what I’m learning, at 82 years old: the main thing is to be in love with the search for truth.” —Maya Angelou We now turn our attention to inference, which involves taking a sample from a population to make conclusions about the population with some degree of certainty. At its foundation, inference is about the search for truth. Specifically, when we calculate some estimate using a sample of data, we use inference to say whether that estimate is a good guess of the unobserved population parameter. Rather than focus on sampling techniques (e.g. random, clustered, stratified, convenience), this chapter focuses on the theory that allows us to use samples for inference as well as the potential limitations of doing so. 10.1 Learning objectives Explain the 68-95-99 rule and apply it given the mean and standard deviation of a normal distribution Explain how a sampling distribution is constructed Explain why the Central Limit Theorem is needed to conduct inferential statistics and how it allows us to do so Given an estimate and standard error, construct 95 and 99 percent confidence intervals Interpret confidence intervals Explain the effect of random or biased sampling on the accuracy and precision of a sample estimate and sampling distribution Explain the effect of sample size on the accuracy and precision of a sample estimate and sampling distribution 10.2 Normal distribution When we calculate an estimate, the estimate is highly unlikely to be exactly equal to the population parameter it is intended to represent. But, given a sample and an estimate, we can calculate the range in which the parameter falls with a certain degree of confidence. If that range does not include zero, then we have reason to believe the parameter is positive or negative. A non-zero parameter may be cause for action. Or, depending on the situation, a parameter of zero may be cause for action. We are able to calculate a confidence interval because of the normal distribution. Inference relies on the normal distribution introduced in Chapter 4. The normal distribution has a unique and useful quality such that wherever the mean of a variable’s distribution lies, if the distribution of the variable is normal, then 68% of the values lie within one standard deviation above and below that mean, 95% of the values lie within two standard deviations above and below, and 99% lie within three standard deviation. This quality of the normal distribution is sometimes called the 68-95-99 rule, which is shown in Figure 10.1 below. The Greek symbol \\(\\mu\\) (mû) denotes the mean and the symbol \\(\\sigma\\) (sigma) denotes the standard deviation. Figure 10.1: 68-95-99 rule of normal distribution Suppose we take a sample of 397 professor salaries in order to estimate the average salary of all professors at the university. From our sample, we calculate a mean of 113706 dollars and a standard deviation of 30289 dollars. Figure 10.2 below shows the distribution of this sample of salaries. Figure 10.2: Distribution of professor salaries Note that the distribution of salaries is roughly normal-looking, though it is skewed somewhat to the right. If the distribution of 397 salaries were perfectly normal, then 68% of those salaries fall between 83417 and 143995, 95% percent of the 397 salaries fall within 53128 and 174284, and 99% of the 397 salaries fall within 22839 and 204573. However, since we can observe the entire distribution, we can calculate the range in which 95% of values fall or any percentage of values. The exact 95% range for Figure 10.2 is 70,761 to 181,511. These ranges are merely descriptions of our sample just like the measures used in Chapter 4. We have not yet made any inference about the salaries of all professors at the university. Again, it is highly unlikely that the average salary of all professors at the university equals 113706 dollars exactly. Our estimate is a guess of something we do not directly observe. Naturally, we want to know a range in which we can be reasonably confident the average salary of all professors falls. This range is called a confidence interval. However, unlike the distribution of 397 salaries, we only have one estimate from our one sample of salaries. How can we calculate a confidence interval of something we do not observe? To construct a confidence interval, we assume the sampling distribution of the mean of salaries is normal. 10.3 Sampling Distribution Distributions were covered in Chapter 4. A variable is comprised of multiple values that can be plotted along a number line or axis to form a distribution. This distribution can be described in terms of its center via the mean and its spread via the standard deviation. A sampling distribution is simply a distribution comprised of multiple estimates, each taken from a separate sample, instead of multiple values, each taken from a separate unit of analysis. Imagine if the distribution in Figure 10.2 were made of 397 averages from 397 samples of salaries. If we have no reason to suspect our estimates are systematically above or below the population mean (i.e. unbiased), then we have a distribution of guesses for the population mean, the center of which should approach the population mean given enough sample estimates. Assuming this sampling distribution is normally distributed, then we can construct an interval in which 95% of the estimates fall as a plausible range in which the unobserved population mean falls. This tends to be a large theoretical leap for many to make. To reiterate, we have only one sample, not 397. How do we construct a 95% confidence interval off of a sampling distribution we do not have the data to observe? We do so using theory and assumptions. Most importantly, we use the Central Limit Theorem. 10.4 Central Limit Theorem The Central Limit Theorem may seem like magic more than anything else in statistics, though it is scientifically sound. Given a sufficient sample size, the Central Limit Theorem allows us to assume sampling distributions are normally distributed even though we do not have data to observe the sampling distribution. Without it, we could not construct confidence intervals. Thus, we could not make inferences about a population. Seeing the Central Limit Theorem work is believing, especially when circumstances are set that would seem to work against it. To do this, let us revisit the distribution of a six-sided die discussed in Chapter 4. With each of the six values having an equal probability of occurring, we know each value has about a 17% chance of occurring. If we were to roll the die some number of times divisible by 6, then all values should occur the same number of times, resulting in a distribution like that depicted in Figure 10.3. Figure 10.3: Probability distribution of a six-sided die The distribution in Figure 10.3 is decidedly not normal. Yet, the Central Limit Theorem states that if we took numerous samples from this distribution, each of sufficient sample size, then the sampling distribution will be normal. We typically do not know the distribution of a variable like we do with a six-sided die, thus we do not know where an important measure like the mean is in that distribution. However, if any estimate regarding any variable–no matter the distribution of the variable–is normally distributed, then we do not need to know the distribution of the variable. This is the power and importance of the Central Limit Theorem. To see how the Central Limit Theorem works, we need a population we can observe but would not be able to in usual circumstances. Suppose we had a population comprised of 10,000 observations. Each observation is the result of rolling a six-sided die. This is obviously a play example for the sake of instruction, but one could imagine the six values of the die to be something more interesting and important, such as levels of education. Table 10.1 below shows a preview of our simulated population. Table 10.1: Preview of simulated population from uniform distribution ID education 1 2 2 6 3 4 9998 4 9999 6 10000 4 Since we have the entire population, we can calculate the population mean, which would typically be a population parameter we cannot calculate. The mean “education level” of this population is 3.518. This is almost exactly equal to the mean we should expect from many rolls of a six-sided die. The distribution of the population’s education is shown in Figure 10.4. The solid red line represents the population mean. Figure 10.4: Distribution of simulated population Suppose we were to draw one random sample of 20 from this population. The distribution of this sample is shown in Figure 10.5. The mean of this sample is 3.65, represented by the purple dashed line. The red solid line represents the population mean of 3.518. Figure 10.5: Distribution of sample of 20 from simulated population The sample mean of 3.65 may or may not be a good guess of the population mean of 3.518; such judgments depend on the context of the research question or the decision needing made. Suppose we take a second random sample of 20 from the population, as shown in Figure 10.6. The mean of this second sample is 3.95. Figure 10.6: Distribution of a second sample of 20 from simulated population Note that the distribution of the two samples are different and neither are uniform like the population distribution. This is the manifestation of randomness. Each sample gives us a different estimate of the population mean, both of which are too high. Estimates of other samples would fall below the population mean. With enough samples and sample estimates of the mean, we can construct a sampling distribution. Suppose we take 100 samples of 20 from the population, estimating the mean of each sample to construct a sampling distribution of the mean. This sampling distribution is shown in Figure 10.7. Note that with only 100 samples of only 20 observations each, the sampling distribution roughly resembles the normal distribution. Also, the mean, or center, of the sampling distribution represented by the yellow line is very close to the population mean. Figure 10.7: Sampling distribution of 100 sample means from samples of size 20 The centering of the sampling distribution at the population mean is due to having an unbiased estimate from random sampling. If our estimate is unbiased, then we expect it to equal the population parameter, on average. This applies to any estimate and its potential bias, including the estimates in regression. If our regression model is unbiased, then our estimate comes from a sampling distribution that centers at the population parameter. Now let’s take 10,000 samples with a size of 33 observations each, calculating the mean of each sample. The sampling distribution of the 10,000 sample means is shown in Figure 10.8 below. Note that it looks very similar to the normal distribution with a mean equal to 3.157. From a variable that is not normally distributed, we obtain a sampling distribution that is normal. No matter the distribution of the variable, the Central Limit Theorem assures us its sampling distribution will be normal, provided we have a large enough sample. Figure 10.8: Sampling distribution of 10,000 sample means from samples of size 33 In reality, we cannot draw 10,000 samples from the population. If we could, and calculated the mean of the sampling distribution, then we could be extremely confident that we have estimated the population parameter with virtually perfect accuracy. Alas, we have only one sample and no sampling distribution to observe. Though we can safely assume our one estimate comes from a normal sampling distribution that, if there is no bias, is centered at the population mean, we do not know where our sample falls within that distribution. Our one sample could be one with an estimate in or near the left or right tails of the sampling distribution in Figure 10.8. Therefore, we need a range of plausible values for the population parameter. For this range we need to measure the spread of the sampling distribution in terms of its standard deviation. 10.5 Confidence intervals The standard error is used to construct confidence intervals. The standard error is essentially the same as the standard deviation. It is the name given to the standard deviation of a sampling distribution instead of a variable’s distribution. Now that we can safely assume our sample estimate comes from a normal sampling distribution, and given that we need to account for the randomness of any one sample, we can calculate the range of values within which 95% or 99% of the estimates in the sampling distribution falls. In short, we have returned to applying the 68-95-99 rule, only this time we use it to construct a confidence interval. The 95% confidence interval for any estimate is 2 standard errors (1.96, technically) below and above the estimate. The 99% confidence interval is about 3 standard errors below and above the estimate. Referring back to the sampling distribution in Figure 10.8, the standard deviation is equal to 0.26. Suppose we drew one sample that gave us an estimate of 4. Suppose the standard error of that estimate happened to equal 0.26. In that case, the 95% confidence interval is approximately 3.48 to 4.52. Since we know the population parameter equals 3.518, we know our confidence interval captures it, which is what we hope to be the case but cannot confirm in typical circumstances. How do we calculate the standard error to construct the 95% confidence interval or any confidence interval without observing the sampling distribution? Again, theory and assumptions. Throughout this running example, we have been trying to estimate the mean of the population. The standard error (SE) of a sample mean is calculated using the following equation \\[\\begin{equation} SE = \\frac{s}{\\sqrt{n}} \\tag{10.1} \\end{equation}\\] where \\(s\\) is the standard deviation of the variable in our sample data and \\(n\\) is the number of observations in our sample. Equation (10.1) highlights one of the reasons sample size is a point of interest in analysis. In addition to needing at least 33 observations for the Central Limit Theorem to work reliably, sample size affects the precision of our confidence interval. As \\(n\\) increases, the denominator in Equation (10.1) increases. Given a standard deviation, greater denominator results in a smaller SE than a lesser denominator. That is, as our sample size increases, the range of our confidence interval decreases. To demonstrate the effect of sample size on precision, suppose we drew 10,000 samples of size 1,000 instead of 33, as was done for the sampling distribution in Figure 10.8. Figure 10.9 depicts the sampling distribution of this hypothetical scenario. Not that the distribution is virtually identical to normal. Of most importance is the spread of the sampling distribution. The standard deviation of the sampling distribution (or standard error) in Figure 10.8 is 0.26. The standard error of the sampling distribution in Figure 10.9 is equal to 0.05. Figure 10.9: Sampling distribution of 10,000 sample means from samples of size 100 From a sample of size 1,000 it is much less likely that we would obtain a sample estimate as far from the parameter of 3.518 as 4. Moreover, whatever our estimate, we can construct a confidence interval with the same level of confidence that will be much smaller. A more precise confidence interval may allow for more confident decision-making. 10.6 Conclusion Though in reality we obtain only one estimate from one sample, we assume that estimate is one value of a normally distributed sampling distribution that is centered at the population parameter we intended to estimate. Our one sample estimate is highly unlikely to equal the population parameter because of randomness. Therefore, in addition to our specific estimate of the parameter, we construct a range of plausible values that captures that population parameter. To walk through the full process of estimation using one sample in this simulated data example, one of the 10,000 samples of size 1,000 (sample 379) had a mean education level of 3.552. The standard deviation of education in this sample was 1.53. Given 1,000 observations, then the standard error is equal to 1.53/sqrt(1000) ## [1] 0.04838285 Therefore, assuming a normal sampling distribution and absence of bias, the 95% confidence interval for our estimate of the population mean of education is 3.552-(1.96*0.048) ## [1] 3.45792 3.552+(1.96*0.048) ## [1] 3.64608 The 95% confidence interval of this particular sample captures the population parameter of 3.518. A common interpretation of, say, a 95% confidence interval is that our population parameter has a 95% probability of falling within our confidence interval. This is incorrect. A confidence interval either contains the population parameter or it does not. There is no 95% probability to speak of; only 0% or 100%. What a confidence interval conveys is that if we were to draw numerous samples from this population rather than just the one, then we would expect 95% of the confidence intervals constructed from all of our samples to capture the population parameter and 5% of the confidence intervals to fail. Our sample could be one of those 5% of samples for which the confidence intervals fail to capture the population parameter. One out of 20 samples are expected to fail to capture the population parameter that it was intended to estimate. This is why many have concerns regarding a crisis of replication in science. If only one study is published, and replications of a study are difficult to have published, then we do not know if the one that was published is the anomaly or not. 10.7 Key terms and concepts Normal distribution 68-95-99 rule Sampling distribution Sample estimate Central Limit Theorem Confidence intervals Standard error Accuracy of sampling distribution and estimate Precision of sampling distribution and estimate "],
["11-surveys-and-evaluations.html", " 11 Surveys and Evaluations 11.1 Learning objectives 11.2 Sample size 11.3 Survey weights 11.4 Hypothesis testing 11.5 Chi-square test 11.6 T-test 11.7 Key terms and concepts", " 11 Surveys and Evaluations “A census taker once tried to test me. I ate his liver with some fava beans and a nice chianti.” —Hannibal Lecter Before considering inference with respect to regression, this chapter covers two common concerns in sampling beyond question design or sampling method: sample power and weighting. Then, hypothesis testing is introduced, which applies to any sample estimate from which we wish to make inference. Lastly, using hypothesis testing for evaluations in their simplest forms is considered. 11.1 Learning objectives Apply the margin or error or the confidence interval of an estimate when making conclusions Compute the sample size required to achieve a desired margin of error or precision around an estimate given the necessary information for such a computation Weight survey results given sufficient information Identify or construct the null and alternative hypotheses of a research question Interpret a p-value and apply it to determine the outcome of a hypothesis test Distinguish between types I and II error in a research scenario Explain when a chi-square test or t-test is appropriate 11.2 Sample size Now that we understand how a sample of relatively small size allows us to make inferences about the population with a reasonable degree of confidence, let us next consider how to determine the size of sample we need to achieve a confidence interval with a specific degree of precision. When President Trump’s impeachment inquiry was in progress, numerous polls were conducted to gauge the sentiment of the U.S. electorate as to whether Trump should be impeached. One poll conducted by Fox News, using a sample of 1,000 voters, reported that 51% of voters support impeaching Trump with a margin of error of 3 percentage points. These results garnered national attention as the first time a majority of U.S. voters supported the impeachment of Trump. Regardless of one’s opinion on the matter or whether a national majority persuades elected officials, the claim that a majority of voters supported impeachment based on this poll is dubious. A sample of U.S. voters was taken. From this sample, the proportion of voters in support of impeachment was calculated to serve as an estimate of the population parameter. This sample produced an estimate of 51 percent. The margin or error in surveys or polls, unless noted otherwise, refers to one-half of the 95% confidence interval, or roughly two standard errors. Therefore, the 95% confidence interval of the polling results was 48 to 54 percent. The confidence interval used to capture the unobserved population parameter includes a minority of voters supporting impeachment. A common mistake made when interpreting estimates and confidence intervals is that the estimate is the most likely value within the confidence interval for the population parameter. The population parameter is no more likely to equal the estimate than any other value within the confidence interval. A confidence interval either captures the population parameter or it does not. Therefore, it was just as likely that 48 percent of voters supported the impeachment as it was 54 percent did or any percentage in between. As long as our estimate is unbiased, we cannot influence its value. Any attempt to do so would be bias by definition. Thus, the pollsters were stuck with an estimate of 51 percent. The estimate of 51 percent was not the issue for making the conclusion that a majority of voters supported impeachment. The issue was the critical lack of precision around the estimate. Unlike the estimate, we can influence the precision of the confidence interval. How many voters would the pollsters have had to survey in order to achieve a margin of error of 1 percentage point and conclude a majority of voters support impeachment? If the outcome is dichotomous or binary, such as whether or not a respondent supports impeachment, then the equation for determining the desired sample size is as follows \\[\\begin{equation} n=p(1-p)({\\frac{Z}{E}})^2 \\tag{11.1} \\end{equation}\\] n is the sample size p is the proportion of yes/true/success Z is the number of standard deviations we set according to what confidence interval is desired E is the desired margin of error It is important to point out that the calculation in Equation (11.1) occurs prior to the poll. Therefore, we do not know the value of \\(p\\). Unless there is reason to expect \\(p\\) to equal a particular proportion, it is customary to input 0.50. If we want to use a 95% confidence interval then we input 2 for \\(Z\\). Lastly, we replace \\(E\\) with how far we want each side of our chosen confidence interval to be below and above our estimate. Suppose the primary purpose of the Fox News poll was to conclude a majority opinion. Then, a margin of error equal to 1 percentage point would allow a valid conclusion that a majority of voters support or oppose impeachment if the result of the poll were slightly below 49% or above 51% in favor. Choosing to use a 95% confidence interval, then the sample size necessary to achieve a margin of error equal to 1 percentage point (0.01) is \\[\\begin{equation} n=0.5(1-0.5)({\\frac{1.96}{0.01}})^2\\\\ n=9,604 \\end{equation}\\] which is likely impractical for the kind of polling that news organizations tend to conduct. If we wish to determine the sample size needed for estimating a 95% confidence interval within a certain distance from a continuous estimate, such as the mean, we can use the following equation \\[\\begin{equation} n=(\\frac{sZ}{E})^2 \\tag{11.2} \\end{equation}\\] where \\(s\\) is the sample standard deviation. Again, we do not have the sample standard deviation prior to obtaining the sample. We must rely on past analyses of the variable in question or a pilot study with a small sample in order to input the sample standard deviation. 11.3 Survey weights Ideally, the demographic composition of survey respondents should match the demographic composition of the survey’s intended population. Thanks to Census data, we have a reasonably accurate understanding of population demographics such as age, sex, race, and ethnicity for multiple geographic areas and government jurisdictions. Other organizations like Pew Research Center and Gallup provide population proportions of other various ways to form groups, such as political party or religious affiliation. Unfortunately, it is unlikely for the demographic composition of survey respondents to match the population. Recipients choose not to respond and surveys tend to reach some demographics disproportionately more than others. This results in over- or under-representation of certain demographic groups in our survey, which limits our ability to generalize survey results and threatens the internal validity of any estimate. Weighting is a way to correct for a demographic mismatch between the composition of respondents and the intended population. There are multiple methods of weighting, some of which are complex, but the basic method described below can work for most cases where maximum correction is not necessary or feasible. Suppose a poll targeted to the general U.S. public asked if workers who have illegally entered the U.S. should be 1) allowed to keep their jobs and apply for citizenship, 2) allowed to keep their jobs as temporary guest workers but not allowed to apply for citizenship, and 3) lose their jobs and have to leave the country. The poll also asked for political party affiliation. A total of 890 responses were collected, generating the following results. Table 11.1: Illegal immigration poll results response party Apply for citizenship:278 Republican :357 Guest worker :262 Democrat :174 Leave the country :350 Independent:359 Based on the results, a plurality of 39% of the U.S. public believes illegal immigrants should leave the country and 31% believe they should be allowed to apply for citizenship. The question these estimates are biased by the composition of political party affiliation. About 40% of the respondents are Republican and Independent, while about 20% are Democrat. Suppose we find a national survey reporting that the U.S. is 30% Republican, 36% Independent, and 31% Democrat. Therefore, Republicans and Independents are over-represented in our survey, while Democrats are under-represented. We need to correct for this using weights. To calculate weights, we can use the following formula. \\[\\begin{equation} Weight = \\frac{Population}{Sample} \\tag{11.3} \\end{equation}\\] Using Equation (11.3), we obtain the following weights for our survey Republican: 30/40 = 0.75 Independent: 36/40 = 0.9 Democrats: 31/20 = 1.55 These weights mean that each Republican response counts as only three-quarters of a response and each Democrat response counts as about 1.5 responses. Next, we need to tabulate how many of each response was made by the three parties. Table 11.2: Response by political party Republican Democrat Independent Apply for citizenship 57 101 120 Guest worker 121 28 113 Leave the country 179 45 126 Then, we multiply the values by their corresponding weight. For example, applying the weight for Republicans results in 134 responses for “Leave the country” (\\(179 \\times 0.75\\)). This process gives us the following counts Table 11.3: Weighted survey counts party response total weight w.total Republican Apply for citizenship 57 0.75 43 Republican Guest worker 121 0.75 91 Republican Leave the country 179 0.75 134 Democrat Apply for citizenship 101 1.55 157 Democrat Guest worker 28 1.55 43 Democrat Leave the country 45 1.55 70 Independent Apply for citizenship 120 0.90 108 Independent Guest worker 113 0.90 102 Independent Leave the country 126 0.90 113 According to our weighted counts, 36% of the U.S. believes illegal immigrants should leave the country, while 35% believe they should be allowed to apply for citizenship. Weighting has changed a 8 percentage point gap in these two responses to a 1 point gap. In case it was not obvious in the example, we have to ask survey recipients to provide the demographic information upon which we plan to weight. Survey administrators must consider what variables might bias the response(s) of interest if there is a mismatch between the sample and population. Wisely, the designers of the survey above suspected if a disproportionate number of Republicans or any other political party responded, this would bias their estimates. Presumably, race and ethnicity are correlated with this response, but we do not have this information to construct a weight. 11.4 Hypothesis testing At its foundation, hypothesis testing is a simple procedural process. It does involve some application of statistical theory, the most fascinating and misunderstood aspect of which is the p-value. This section covers how to set up and use a hypothesis test to determine if an estimate is statistically significant. 11.4.1 Null and alternative hypos Setting up a hypothesis test first involves establishing two mutually exclusive, competing statements: Null hypothesis: the condition I intend to test for is not true, not the case Alternative hypothesis: the condition I intend to test for is true, is the case The condition is based on our research question that warranted the analysis in the first place. For example, is the average age of MPA students less than the average age of all graduate students? Are females more likely to enroll in an MPA program than males? Does obtaining an MPA increase earnings? Based on our question, we make a hypothesis before computing an estimate that would answer the question. For example, the average age of MPA students is less than the average age of all graduate students. Females are more likely to enroll in an MPA program than males. The effect of attaining an MPA increases earnings. These examples are alternative hypotheses; the affirmative of the condition we set out to test. The null hypothesis is the negative of the condition. For example, average age of MPA students is equal to graduate students. The likelihood of enrolling in an MPA program are equal between males and females. An MPA degree does not increase earnings. Note that the examples of alternative hypotheses were all directional. They used words like less/more than or increase/decrease. An alternative hypothesis need not be directional even though we may expect one direction over the other. For example, our alternative hypotheses could be that average age differs between MPA students and other grad students, the likelihood of enrolling differs between males and females, and attaining an MPA effects income. I encourage students to stick with non-directional hypotheses. In additional to simplifying the analysis, doing so reduces the likelihood that we report a statistically significant result when in fact there is not one (i.e. false positive). A strong case can be and is made that the statistical analyses allow too high of a likelihood for false positives. Claiming a direction means we have theoretically ruled out half of the possible results of our analysis before we even conduct the test, thereby increasing the likelihood we get the results we expected. If we are able to do this, one might wonder why conduct the test in the first place. Translating the above into more mathematical concepts, most research questions involve differences: the difference in mean age between MPA students and other grad students, the difference in the proportions of female and male MPA graduates, the difference in the slopes of regression lines drawn through data points for income and education level or degree type. Thus, the null hypothesis states that any of these differences equals zero. The alternative hypothesis states that any of these differences does not equal zero. The null hypothesis is denoted by \\(H_0\\) and the alternative hypothesis is denoted by \\(H_A\\). \\(H_0: \\mu_{MPA}-\\mu_{grad} = 0\\) or \\(H_A: \\mu_{MPA}-\\mu_{grad} \\neq 0\\) \\(H_0: \\rho_{female}-\\rho_{male} = 0\\) or \\(H_A: \\rho_{female}-\\rho_{male} \\neq 0\\) \\(H_0: \\beta_{MPA} = 0\\) or \\(H_A: \\beta_{MPA} \\neq 0\\) Note the use of population parameters above. Again, inference computes a sample estimate to make inferences about a population. Therefore, our hypotheses include the unobserved population parameter. Our estimate and confidence interval represents our best guess of that population parameter. The purpose of the hypothesis test is to establish a threshold at which we are sufficiently confident in our results to make a conclusion concerning our hypotheses prior to viewing the results. 11.4.2 Conclusion and error type There are two possible outcomes of a hypothesis test. We either reject the null hypothesis, or fail to reject the null hypothesis. We never accept the null hypothesis or reject the alternative hypothesis. This may seem like semantics, but it actually has important implications for our conclusions. Suppose our results do not meet the threshold to reject the hypothesis, thus leading us to fail to reject the null hypothesis. This does not mean the null hypothesis is true. Our null hypothesis states whatever parameter of interest in our study equals zero. We do not observe the population parameter. Therefore, we cannot say that our null hypothesis is true. Instead, we say that we do not have sufficient evidence to reject the null. It may be true or false, but we cannot determine which based on our particular estimate from our particular sample. Conversely, if we reject the null, then our conclusion is that the null hypothesis is false. We can rule out with reasonable confidence that the population parameter does not equal zero because doing so does not require us to claim a particular value for the population parameter; just that it is not equal to zero. A popular example of hypothesis testing is a jury decision in a court case. A defendant is accused of committing a crime. In truth, the defendant is either innocent or guilty of that crime. Ideally, the defendant is presumed innocent until proven guilty according to a jury of their peers. Therefore, the null hypothesis is that the defendant is innocent and the alternative hypothesis is that the defendant is guilty. The jury decides either that the defendant is guilty or not guilty. If guilty, then the jury has rejected the null hypothesis of innocence. If not guilty, then the jury has failed to reject the null hypothesis. Note that the jury does not decide the defendant is innocent, which would be equivalent to accepting the null or rejecting the alternative. Despite our best efforts, there always remains some chance that we have reached the wrong conclusion with our hypothesis test. We can make one of two possible errors: Type I error or false positive: rejecting the null when the null is actually true Type II error or false negative: failing to reject the null when the null is actually false For instance, Type I error is finding an innocent defendant guilty, a healthy patient sick, or an ineffective program effective. Type II error is finding a guilty defendant not guilty, no evidence of illness in a sick patient, or no evidence of efficacy in an effective program. Again, the null hypothesis involves the population parameter, so we do not know if it is actually true or not. The null must be true or false, and the outcome of our hypothesis test claims whether the null does not appear to be false or is false. Therefore, there is a probability that we have reached the incorrect conclusion. It is impossible to eliminate the chance of Types I and II errors, though it is possible to increase or decrease their likelihoods. However, the two share an inverse relationship; as we reduce the chance of one type of error, we increase the chance of the other type of error. Depending on the context of our research question, we may be more or less concerned about Type II error, but the focus of a hypothesis test is placed on Type I error, which serves as the threshold for our decision. 11.4.3 Decision rule Before testing our hypothesis, we ask ourselves the following question: “What is the maximum probability of Type I error that I or others should be willing to tolerate?” Actually, this question has been answered for us in most disciplines. The common threshold for this tolerance is 5% or 1% probability of rejecting the null hypothesis when the null is actually true. Social sciences typically use 5%. With our threshold set, we can now test our hypothesis. We calculate the sample estimate and the standard error of our estimate, which are used to calculate the confidence interval. The confidence level of our confidence interval depends on our chosen threshold for Type I error. If our threshold for Type I error is 5%, then we calculate the 95% confidence interval. If our threshold is 1%, we use a 99% confidence interval. Our confidence interval is our best guess of the plausible range of values for the population parameter. We have decided to tolerate the chance that our confidence interval is one of the five out of 100 confidence intervals–or 1 out of 100–expected to fail to capture the parameter. Our null hypothesis states that the parameter equals zero. Therefore, if our confidence interval does not contain zero, we reject the null hypothesis. If our confidence interval does contain zero, then we have failed to reject the null hypothesis because the parameter might equal zero with a higher probability than we decided to tolerate. In most cases, we do not need more information than the estimate, standard error, and confidence interval to make a decision regarding our hypothesis test. However, an analysis provides us an additional piece of information that allows us to arrive at the same conclusion but from a different perspective called the p-value. The p-value tells us the probability of obtaining the estimate we did, or an estimate further away from the null hypothesis, if the null hypothesis were actually true. The p-value provides us a concise decision rule. The tolerance threshold we set is often referred to as the significance level and denoted by the Greek letter \\(\\alpha\\) (alpha). If \\(p&lt;\\alpha\\), reject the null hypothesis. If \\(p\\geq \\alpha\\), fail to reject the null hypothesis. Again, a typical value for \\(\\alpha\\) is 5%, or 0.05. Having chosen a 5% significance level, if our results generate a p-value of 0.04, for instance, then the likelihood of obtaining our result in a world where the null is true is 4%. Therefore, we reject the null hypothesis. If our p-value were equal to 0.06, then the likelihood of obtaining our result in a world where the null is true is 6%. This exceeds our maximum tolerance, thus we fail to reject the null hypothesis. 11.5 Chi-square test The Chi-square (\\(\\chi^2\\)) test is a common choice for introducing the application of hypothesis testing. Chi-square is used to test whether two nominal variables are associated to a statistically significant extent. A nominal variable, such as race, sex, or political party affiliation, has two or more of levels. If one wanted to test if, given the level for a unit of analysis in one nominal variable (e.g. male), there is a higher likelihood for a particular level in another nominal variable to occur (e.g. Republican), a Chi-square test is an appropriate choice. For an example, let us return to the immigration survey. We saw in 11.2 that 179 out of 357 Republicans (50%) responded that illegal immigrants should be forced to leave the country, while 101 out of 174 Democrats (58%) responded that illegal immigrants should be allowed to apply for citizenship. Is there a statistically significant pattern in responses conditional on political party affiliation, or are these differences due to random noise? The null hypothesis for this question is that there is no association between the opinion on illegal immigration and political party affiliation. That is to say, if we chose any of the three political party levels in our data, the probability of an individual providing one of the three opinions is equal the other opinions, or \\(H_0: P_{leave} = P_{guest} = P_{citizen}\\) The alternative hypothesis is that there is an association between opinion on illegal immigration and political party affiliation, or \\(H_A\\): at least one \\(P\\) is not equal to the others Next, suppose we chose to use the customary 5% statistical significance level, or \\(\\alpha=0.05\\). Now we are ready to test our hypothesis using a Chi-square test. Doing so generates the following results. ## ## Pearson&#39;s Chi-squared test ## ## data: immigration_poll ## X-squared = 100.95, df = 4, p-value &lt; 0.00000000000000022 This is one case where there are no confidence intervals to compute since our variables are not numerical. Instead, we rely on the p-value. Our p-value is less than 0.00000000000000022. More concisely, \\(p&lt;0.05\\). Therefore, we reject the null hypothesis that there is no association between the three illegal immigration opinions in the survey and political party affiliation. Furthermore, the probability for us to get the values in Table 11.2 or more extreme in a world where the null hypothesis is actually true is equal to an infinitesimal percent, not to suggest that such a small p-value is required to make inferences. Our results allow us to make inferences such as Republicans are more likely to believe illegal immigrants should lose their jobs and have to leave the country, while Democrats are more likely to believe they should be allowed to keep their jobs and apply for citizenship. Not a particularly surprising inference, but perhaps that is because many such inferences in the past have been made using similar techniques and reported many times. 11.6 T-test The t-test is another common introductory application of hypothesis testing. A t-test is used to test the association between a nominal variable with two levels and a numerical variable. It is frequently used in simple program evaluations with a pre/post or treatment/control design. Both involve a nominal variable with two levels. If one want to test if a numerical outcome is different between the two levels, then a t-test is an appropriation choice. There are two varieties of the t-test. To test if an average of a numerical outcome is different between two groups, such as a treatment and control group, then we use an independent t-test. To test if an average numerical outcome is different before and after a treatment for the same units of analysis, we use a dependent t-test. The difference between the two t-tests concerns how we approximate the sampling distributions and confidence intervals, but their use for hypothesis testing is essentially the same. Suppose we work for a nonprofit that provides job training workshops and want to evaluate their effectiveness. One way to go about such a task is to compare the earnings of participants (i.e. treatment group) to the earnings of non-participants (i.e. control group). We have earnings data for 185 participants and 128 non-participants, some of which is previewed in the table below. Table 11.4: Preview of job training data treatment earnings 1 66493.964 1 0.000 0 46651.829 1 10070.227 1 0.000 0 1211.736 Before constructing the null and alternative hypotheses, a note about the population in this example. In program evaluations or generally any analysis aimed toward testing whether some event caused an effect on an outcome of interest, there are two populations. There is the entire population of units of analysis (e.g. all people, all nations, all dogs) and the subset of that population for whom/which the program, policy, or intervention is intended. The choice of population leads to two slightly different questions. If the entire population is our research population, then our intent is to estimate the average effect of the program on a randomly chosen unit from the entire population. This is referred to as the average treatment effect (ATE). If the subset that the program targets is our research population, then our intent is to report the average effect of a randomly chosen targeted unit. This is referred to as the average treatment on the treated (ATT). The detailed differences between the two are beyond the scope of this book and more appropriate for a class in program evaluation or causal inference, but the existence of this difference is worth being aware of. Given that job training programs are not intended for all people, the presumption is that we want to estimate the average effect on those the program targets. Of course, we could choose as our population only those who participated in the program. In that case, we need not bother with hypothesis tests, as we would be calculating the population parameters directly from the observed data. However, we would not be able to generalize the results. With the population in mind, our null hypothesis is that the average earnings of participants is equal to the average earnings of non-participants, or \\(H_0: \\mu_{treated} = \\mu_{untreated}\\) Our alternative hypothesis is that the average earnings of participants is not equal to the average earnings of non-participants, or \\(H_A: \\mu_{treated} \\neq \\mu_{untreated}\\) Choosing a significance level of 5%, we are ready to test our hypothesis. A simple computation of the average earnings between the two groups provides the following information. Table 11.5: Comparison of means between treated and untreated treatment Average Earnings 0 21645.10 1 26031.49 Participants have a higher average earnings than non-participants, but is this difference statistically significant? For that, we should use an independent t-test because participants and non-participants are two different groups. Running the t-test provides the following results ## ## Welch Two Sample t-test ## ## data: earnings by treatment ## t = -1.1921, df = 275.58, p-value = 0.2342 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -11629.708 2856.939 ## sample estimates: ## mean in group 0 mean in group 1 ## 21645.10 26031.49 Our p-value is greater than our significance level, \\(0.23&gt;0.05\\). Therefore, we fail to reject the null hypothesis and conclude that there is not statistically significant evidence that participants earn more than non-participants, on average. Note that our 95% confidence interval ranges between -11,630 and 2,856. It includes zero, thus we cannot claim the difference between the means is not equal to zero with reasonable confidence. To learn how to conduct chi-square and t-tests in R, proceed to Chapter 24. 11.7 Key terms and concepts Margin of error Survey weight Null and alternative hypotheses Rejecting the null Failing to reject the null Types I and II error Confidence level Significance level P-value Chi-square test T-test "],
["12-regression-inference.html", " 12 Regression Inference 12.1 Learning objectives 12.2 Regression table 12.3 Other hypotheses 12.4 Practical significance 12.5 Key terms and concepts", " 12 Regression Inference “One out of every four people is suffering from some form of mental illness. Check three friends. If they’re OK, then it’s you.” —Rita Mae Brown We can now apply our knowledge of inference to fully understand all of our regression results and extend our results to other questions. First, this chapter explains each column in a regression table as well as how to test additional hypotheses that standard regression results do not answer by default. Then, while inference is used to identify statistical significance, that does not necessarily mean our results are practically significant. This chapter ends with how to determine the latter. 12.1 Learning objectives Explain and interpret the standard components in a table of regression results Construct the null and alternative hypotheses of a variable in a regression model Determine the outcome of the hypothesis test based on the regression results Explain the consequence of choosing a significance level for a hypothesis test Distinguish between statistical and practical significance Determine whether results are practically significant 12.2 Regression table Chapters 6, 7, and 8 presented numerous regression tables. These tables included the standard set of results that statistical programs provide by default. Below is one of the tables from Chapter 7 for a regression to explain traffic fatalities as a function of miles driven and U.S. region. Table 12.1: Parallel slopes for regions term estimate std_error statistic p_value lower_ci upper_ci intercept 0.260 0.167 1.553 0.121 -0.069 0.589 vmiles 0.188 0.021 8.895 0.000 0.147 0.230 regionN. East -0.076 0.065 -1.166 0.245 -0.204 0.052 regionSouth 0.519 0.056 9.283 0.000 0.409 0.630 regionWest 0.641 0.062 10.264 0.000 0.518 0.763 We have covered how to interpret the estimate column at length. This value is the sample estimate of our unobserved population parameter. Provided our model is unbiased, and because of the Central Limit Theorem, we assume that the value of our estimate was drawn from a sampling distribution that is approximately normal and a mean equal to the population parameter. We assume our estimate is the mean of that sampling distribution. For example, in Table 12.1, it is assumed the estimate for vmiles of 0.188 represents the mean of an unobserved sampling distribution comprised of numerous estimates for vmiles that would be obtained if we repeated the regression using numerous samples. The std-error column is the standard error of the regression estimate (aka coefficient) in the same row. This value is an approximation of the standard deviation of the sampling distribution from which the estimate was drawn. For example, the standard error for vmiles of 0.021 represents the standard deviation of its sampling distribution. We know the true population parameter is highly unlikely to exactly equal our estimate but is expected to fall somewhere within our sampling distribution. With our estimate assumed to be the center of the normal sampling distribution and the standard error its standard deviation, we can apply the 68-95-99 rule to construct a range of values that represent a percentage of the estimates within the sampling distribution. The common choices are 95% and 99%, with 95% being the default in statistical programs. The lower_ci and upper_ci columns provide the 95% confidence interval. This range represents our best guess of the plausible values for the unobserved population parameter. The population parameter either falls within our confidence interval or it does not. There is not a 95% probability that the parameter falls within our confidence interval. Rather, it is one range that if we were to repeat the analysis many times using different samples to construct many confidence intervals, we expect 95% of those ranges to successfully capture the population parameter. Therefore, the population parameter is no more likely to equal our estimate as it is to equal any value within our confidence interval. The values for the confidence interval are obtained by subtracting and adding 1.96 standard errors from the estimate. In Table 12.1, the 95% confidence interval for vmiles is 0.147 (\\(0.188-2\\times 0.021\\)) to 0.230 (\\(0.188+2\\times 0.021\\)). If we were to repeat this regression 20 or 100 times, we would expect 19 or 95 of the resulting confidence intervals to capture the population parameter for vmiles. Understanding this chosen rate of success/failure, 0.147-0.230 is our best guess of the range of plausible values for the true response in traffic fatalities to the average number of miles driven. It is just as likely that this true population parameter equals 0.147 or 0.230 as it is to equal 0.188. The statistic and p_value columns concern hypothesis testing. As a reminder, the regression model that produced Table 12.1 was \\[\\begin{equation} mrall = \\beta_0 + \\beta_1vmiles + \\beta_2region + \\epsilon \\tag{12.1} \\end{equation}\\] where mrall is the number of traffic fatalities in a state per 10,000 population. If the purpose of our regression is to explain traffic fatalities, then the inclusion of vmiles and region implies a research question along the lines of “Do distances driven and region in a state affect traffic fatalities?” Therefore, our regression model sets out to test whether vmiles and region has a statistically significant effect on mrall. The standard null hypothesis for a regression model is no effect, or \\(H_0: \\beta=0\\) and the alternative hypothesis is \\(H_A: \\beta \\neq 0\\) for each of the explanatory variables we include in the model. As we now know, our results will lead us to either reject the null hypothesis or fail to reject the null hypothesis for each explanatory variable. If the null hypothesis were actually true, \\(\\beta=0\\), for any of our explanatory variables, then its sampling distribution should be centered at 0, not centered at the value of our estimate. For example, if \\(\\beta_1=0\\) for vmiles, then its sampling distribution should have a mean of 0, not 0.188. This alternative distribution if the null were true is referred to as the null distribution. Just like the sampling distribution, we assume the null distribution is approximately normal. The statistic and p_value columns answer the following question: “If the null for my explanatory variable were true, thus my estimate having a null distribution with a mean equal to zero and a standard deviation equal to the standard error of my estimate, how likely is it that I got the estimate I got?” Specifically, the statistic column equals how many standard deviations or standard errors our estimate is away from the center of the null distribution, zero. The value is equal to the estimate divided by the std_error. For example, the estimate for vmiles is 8.895 standard errors away from 0 (\\(\\frac {0.188}{0.021}\\)). Assuming the null distribution is normal, how likely is it for us to get this estimate or one further away from 0 if the null were true? This is what the p_value provides us. If only 5% of the values in a normal distribution lie 3 standard deviations from the center, then an extremely small percentage of values must lie almost 9 standard deviations from the center. This is why the p-value for vmiles rounds to zero. Supposing we chose a 5% significance level prior to running the regression, our p-value for vmiles is statistically significant, meaning we reject the null hypothesis that \\(\\beta_1=0\\). In other words, there is statistically significant evidence that the average number of miles driven by each driver in a state is associated (perhaps causes) with an increase in the state’s traffic fatality rate. Since region is a nominal variable with four categories, it results in three estimates as if each level was a separate dummy variable equal to 1 if a state is in that region and 0 otherwise. The three regions in Table 12.1 are compared to the excluded region, Midwest. The null hypothesis is that there is no difference between the Midwest and another region of focus. Let us first focus on states in the West. On average, the traffic fatality rate for states in the West is 0.64 higher than states in the Midwest. The p-value is below 0.05, thus we can reject the null hypothesis and report this result as statistically significant. By contrast, the traffic fatality rate for states in the Northeast is 0.08 less than states in the Midwest. However, the p-value is greater than 0.05. Therefore, we fail to reject the null hypothesis, meaning we cannot conclude with reasonable confidence that \\(\\beta_{Northeast} \\neq 0\\). Note that every estimate for which the p-value is less than 0.05, its confidence interval does not contain zero. These two parts of the table will always agree because they answer the same question from slightly different angles. If we choose a 95% confidence interval as our best guess of the plausible ranges for the population parameter, and that interval does not contain 0, then we must have obtained an estimate that, if the null were true, is so far away from 0 that the likelihood of getting it is less than 5%. Had we chosen a 99% confidence interval, or 1% significance level, then for any interval that does not contain 0 the corresponding p-value is less than 0.01. 12.3 Other hypotheses By default, the standard regression table addresses hypothesis tests of the form \\(H_0: \\beta=0\\) and \\(H_A: \\beta \\neq 0\\). If our null hypothesis is \\(\\beta\\) equals something other than 0, then we need to be careful because the statistic and p_value columns do not apply. However, the confidence interval can still be used. If the confidence interval does not contain the value used for our null hypothesis, then we can conclude with our chosen level of confidence that the population parameter does not equal that value. Comparing different levels within a categorical variable is slightly more complicated. In Table 12.1, we can conclude that states in the South and West are different than states in the Midwest, and we cannot conclude states in the Northeast are different than states in the Midwest. But, what if we wanted to compare states in the South to states in the West, or Northeast to South? Our regression and our results were not set-up to make such comparisons. A quick and dirty way to make various comparisons across levels is to examine their confidence intervals. If the confidence intervals do not overlap or do not come close to overlapping, then we can be reasonably certain the population parameters for the two levels are not equal to each other. For example, the upper_ci for Northeast is 0.052 and the lower_ci for South is 0.4. The two intervals are separated by multiple standard errors. Therefore, it is probably safe to conclude they are different. By contrast, the confidence intervals for South and West overlap substantially. Therefore, it is not safe to conclude that South and West are different. Alternatively, we can tell our statistical software to exclude a specific level, thereby allowing us to test our hypothesis without the guesswork. Setting the reference level is covered in this chapter’s corresponding R chapter. Finally, every regression result includes a global hypothesis test of the form \\(H_0: \\beta_0 = \\beta_1 = \\cdots = \\beta_k = 0\\) \\(H_A\\): at least one \\(\\beta \\neq 0\\). Keep in mind that all of our conclusions our probabilistic. There is always a chance of Type I and Type II error. Since the hypothesis test assumes a sampling distribution for each explanatory variable, each explanatory variable we add is like taking an additional sample from some underlying population relevant to our regression. At 95% confidence, we expect 1 out of every 20 intervals to fail at capturing the parameter, such as not including zero when the parameter is truly zero. The global hypothesis test above is a way of testing whether we got a significant result due to the random chance that 1 out of every 20 explanatory variables can be expected to be significant even if the null were true. This global hypothesis test is commonly referred to as an F-test. Its use and interpretation is covered in the R Chapter. 12.4 Practical significance It is easy to lose sight of the forest for the trees when focusing on statistical significance. Just because we find a statistically significant relationship does not mean that relationship is practically significant or economically meaningful. Also, obtaining insignificant results does not necessarily mean you have results that are not important or worth reporting. Such distinctions between statistical and practical significance require an analyst or manager to have a broader sense of the underlying data and the context of the results. After obtaining our results, asking ourselves three questions can help determine if our results are practically significant: What is the typical change in the explanatory variable associated with the statistically significant estimate? Is the predicted change in the outcome due to a typical change in the explanatory variable negligible or meaningful? If the explanatory variable is statistically significant, is its confidence interval so close to zero that using the upper or lower bound instead of the midpoint estimate would make the predicted change in the outcome negligible? If the explanatory variable is statistically insignificant, is its confidence interval so closely around zero that the entire range of plausible values of the parameter would lead to a negligible change in the outcome? The estimate in our regression table conveys the predicted change in the outcome given a 1-unit or percent change in the explanatory variable. Referring back to Table 12.1, as the average number of miles driven per driver increases by one mile, traffic fatality rate increases 0.188 per 10,000. Is a one-mile increase a realistic change in the average distances driven per driver? Is one mile representative of its typical variation? How do we get a sense of what is a typical change in the explanatory variable? The standard deviation of a variable tells us the average deviation from the variable’s mean. For example, the standard deviation of vmiles is 1.1, so a typical change in vmiles is quite close to one unit. Based on the estimate for vmiles, a 1.1 unit change is predicted to change the traffic fatality rate by 0.21 (\\(0.188 \\times 1.1\\)). Next, is a predicted change of 0.21 in the traffic fatality rate negligible or meaningful? Again, we can use descriptive measures to answer this question. The mean traffic fatality rate is 2.0 and its standard deviation is 0.6. Thus, the predicted change in mrall from a typical change in vmiles is about 10% of the mean and about one-third a standard deviation. Given the typical variation in traffic fatality rate is 0.6, is a change of 0.2 negligible or meaningful? This is where professional judgment and context plays a role, as there is no universal rule to determine what is a meaningful effect. Since the context is something as consequential as fatalities, perhaps any change is practically significant. Lastly, since the population parameter is just as likely to equal any value in the confidence interval as it is the estimate, we should check if the lower or upper bound of the confidence interval changes our answer regarding practical significance. Since the result for vmiles is positive, we should focus on how close the lower bound is to zero. The lower bound for vmiles equals 0.147. Repeating the calculations above using the lower bound indicates that a typical change of 1.1 in vmiles predicts a change in mrall of 0.16, which is about 8% of the mean fatality rate and one-fourth its standard deviation. Does this represent a negligible or meaningful change? Again, professional judgment and context is required. Students of statistics are taught to focus so much on the estimate and statistical significance that they understandably get the impression that insignificance implies the results are useless. This is not necessarily the case. Once again, the confidence interval is helpful to determine whether statistically insignificant results are still practically significant. Suppose the p-value for vmiles was equal to or greater than 0.05, thus leading us to fail to reject the null hypothesis. This would also mean that our 95% confidence interval contains 0. Whether the results are still useful depends on the precision of the confidence interval around 0 relative to what we consider a meaningful change in the fatality rate given a typical change in vmiles. For instance, if the confidence interval ranged between -10 and 10, then our best guess for the effect ranges between substantially negative to positive or possibly no effect. This sort of imprecision is useless. However, what if the confidence interval was -0.01 to 0.01? Then, assuming a change of 0.01 in the fatality rate is negligible, we could conclude the effect of vmiles is negligible with a reasonable level of confidence despite failing to reject the null hypothesis. 12.5 Key terms and concepts Regression results estimate standard error statistic or t-statistic p-value lower and upper confidence intervals Null distribution Practical significance "],
["13-regression-diagnostics.html", " 13 Regression Diagnostics 13.1 Learning objectives 13.2 Classical assumptions 13.3 Multicollinearity 13.4 Influential Data 13.5 Key terms and concepts", " 13 Regression Diagnostics “The hardest assumption to challenge is the one you don’t even know you are making.” —Douglas Adams As previous chapters explained, the regression model we choose to use for explaining or predicting an outcome and the inferences we make involve several assumptions based on sound statistical theory. However, this is not to suggest that those assumptions cannot be violated. Bad choices regarding the inclusion or exclusion of explanatory variables, small sample size, and statistical oddities in our data can cause necessary assumptions to break down. If so, we may make or accept invalid conclusions. Recall the credible analysis figure depicted below. Whether one’s role is a producer or consumer of a quantitative analysis, expertise on the subject in question can make significant contributions to every level of Figure 13.1. Understanding how variables are measured helps us evaluate measurement validity and reliability. Understanding the causal pathways between variables helps us evaluate internal and external validity. Understanding inference, probabilities of error, and the context of the results can help us make valid statistical conclusions like whether we have statistical and or practical significance. Analysts and managers alike can involve themselves in this process and work together to ensure an analysis is as credible as possible. Figure 13.1: Components of credible analysis This chapter covers some remaining assumptions and diagnostics that a credible quantitative analysis should include. While running diagnostics may primarily fall within the role of an analyst, those managing an analysis can ask good questions or identify potential issues if they at least know what else can go wrong. 13.1 Learning objectives Determine whether and which classical regression assumptions may be violated based on a residual versus fitted plot (RVFP) Explain why and when multicollinearity may be a problem and propose potential solutions Distinguish between outlier, high-leverage, and high-influence observations in regression Identify influential observations using a residual vs. fitted plot (RVLP) 13.2 Classical assumptions The estimates we obtain from regression are the best linear unbiased estimates possible if certain assumptions hold. If they do not, then our estimates could be biased or they could render our hypothesis tests invalid, creating a higher chance of Types I and II error than we chose that our significance level establishes. Fortunately, these assumptions can be remembered with an apt acronym: LINE. For the assumptions of regression to hold, the relationship between the outcome and explanatory variables must be Linear (or modeled correctly as nonlinear), the observations must be Independent of each other, the data points must be Normally distributed around the regression line, and the data points should have Equal variation around the regression line. A key tool used to evaluate these assumptions is a Residual vs. Fitted Plot (RVFP). An RVFP is a simple transformation of the regression line plot. Figure 13.2 below shows a generic regression line fit to data with the outcome and predicted outcome on the y axis. An RVFP rotates the predicted outcome to the x axis, resulting in a horizontal line. This allows the distance between the observed and the fitted outcome to be vertical. Thus, the residuals of the regression are plotted on the y axis. Figure 13.3 shows a generic RVFP. Figure 13.2: Generic regression line through data Figure 13.3: Generic residual vs. fitted plot Note that the residuals in the RVFP above appear to be randomly positioned; there is no discernible pattern in the scatter plot. No pattern in the RVFP is a visual indication that the classic regression assumptions are not violated. Certain patterns in the RVFP signal violations of certain assumptions. For example, Figure 13.4 below shows a clear case that the linear assumptions is violated due to age and wage sharing a quadratic relationship. Figure 13.4: Fitting linear model to quadratic data The RVFP can also be used to check whether residuals are normally distributed around the regression line and whether the residuals have equal variance. Figures 13.5 and 13.6 below show examples where each is clearly violated. Figure 13.5: Violation of normally distributed risiduals Figure 13.6: Violation of equal variation There is not a direct visual check for the assumption that observations are independent of each other. However, signs that normality or linearity have been violated could be due to violation of independence. Independent observations is a very strong assumption. It states that the units in our data share absolutely no relationship with each other; the information pertaining to one unit has absolutely no bearing on the information gathered for another. Consider all the scenarios in which this assumption is likely violated: individuals in the same household or community, governments in the same state/province, states/provinces in the same country. Random sampling ensures independence, but random sampling is often unfeasible or not applicable to a research question. Other than controlling for the variable(s) by which observations are related covered in Chapter 9, there are statistical methods to account for dependence across observations, but they are beyond the scope of this book. A competent analyst should know at least a few such methods. As with any of the above assumptions, a manager who is knowledgeable in statistics knows to ask questions regarding independence. 13.3 Multicollinearity Multicollinearity involves whether two or more explanatory variables in our regression are strongly correlated. If the correlation between two or more explanatory variables is strong enough, it can result in Type II error (i.e. false negative) for one or more of the variables sharing the strong correlation. Recall that multiple regression isolates the effect of one variable on the outcome by holding all other explanatory variables constant at their mean. This requires variables to vary while holding others constant. If the values of two variables move in near perfect tandem, then regression will find it difficult to isolate the effect of one while another is held constant. It is as if regression creates a traffic intersection with each variable having its own lane and stoplight. To investigate the isolated effect of one variable, regression turns the stoplight for that variable green and sets the stoplights for the other variables to red, letting them idle at their mean. But suppose two variables have decided not to move unless the other is allowed to move. Thus, when one gets the green light to go, it does not move, and regression estimates an effect that is less likely to be statistically significant than should be the case. Calculating the correlation coefficient covered in Chapter 4 can give us a sense of whether multicollinearity may be an issue. As a general rule of thumb, if two variables have a correlation coefficient greater than 0.8 or less than -0.8, then multicollinearity could be a problem. Once a regression is run, if one or more variables that you thought should reject the null fail to do so, this could be due to multicollinearity with another explanatory variable in the model. The solution to multicollinearity is somewhat subjective. If one variable is integral to the original purpose of your analysis, then consider dropping the other variable causing the problem. However, dropping a variable from your model should not be done lightly. The inclusion of a variable implies a theoretical claim that it affects the outcome. By dropping that variable because it is correlated with another explanatory variable, you may be introducing omitted variable bias because the dropped variable may be a confounder as discussed in Chapter 9. Instead, you could combine the collinear variables into a single index variable, which were discussed in Chapter 2. For instance, if the collinear variables are numerical, you calculate the average between them as a more holistic measure of the construct they both represent and include that in your regression model instead. 13.4 Influential Data Regression is an extension of correlation, which is fundamentally based on the mean. As is any measure based on the mean, regression estimates are sensitive to extreme values in our data. Depending on our sample size, one or a few extreme values can substantially impact our regression estimates. We should be aware of influential observations and consider whether our conclusions or recommendations should differ depending on whether influential observations are included. One must be more specific when communicating extreme values in regression, as there are three varieties: Regression Outlier: an observation with a extreme residual High-leverage observation: an observation with an extreme value with respect to a particular variable; an outlier in the distribution of the explanatory variable High-influence: a regression outlier with high leverage Figure 13.7 below provides a visual example of an influential observation in regression. Note the plot point in the far upper-right corner in the left panel. This plot point has an extreme positive residual and it imposes high positive leverage because it is positioned far from the center of the poverty distribution. As a result, this plot point pulls the slope of the regression line upward. The right panel visualizes the same regression with the influential observation removed. The regression line is noticeably flatter and fits the data better. Figure 13.7: Regression with and without a high-influence observation Figure 13.7 provides an obvious case. The primary question is how do we decide an observation is high-influence? As is the case when identifying outliers of a single distribution, there is no definitive rule for identifying high-influence observations in regression. Furthermore, whether to exclude a high-influence observation is subjective and depends on the context. Either way, influential observations should be noted in a report. A key tool used to investigate possible high-influence observations is a residual vs. leverage plot (RVLP). This is similar to an RVFP in that it is a simple transformation of the standard regression scatter plot that allows us to identify outliers, high-leverage, and high-influence observations more effectively. Figure 13.8 below shows an RVLP for the regression of poverty and murder rates. Figure 13.8: Residual vs. leverage plot The software used to produce this RVLP also adds something called Cook’s distance to the plot, denoted by the red dashed line. Cook’s distance is a measure commonly used to identify influential observations. One rule of thumb is that any observation with a Cook’s distance greater than 1 should be investigated. Here, we see that observation 51 in our data has a Cook’s distance greater than 1. To learn how to run regression diagnostics in R, proceed to Chapter 25. 13.5 Key terms and concepts Violations of regression assumptions Multicollinearity Regression outlier High-leverage observation High-influence observation Excluding observations from a regression model "],
["14-forecasting.html", " 14 Forecasting 14.1 What is forecasting 14.2 Patterns 14.3 Forecasting basics 14.4 Recap", " 14 Forecasting “Forecasting is the art of saying what will happen, and then explaining why it didn’t!” —Anonymous; Balaji Rajagopalan Previous chapters primarily used cross-sectional data to demonstrate various applications. Those applications fundamentally apply to time series and panel data as well. However, time series and panel data contain additional information, opening a vast array of additional methods that go far beyond the scope of this book. This and the next chapter offer narrow coverage of two common, yet potentially advanced data applications in public administration: forecasting with time series data and fixed effects analysis with panel data. The intent is to provide the readers a few skills to conduct or understand basic analyses in each scenario. 14.1 What is forecasting Recall in Chapter 2 that time series measures one or more characteristics pertaining to the same subject over time. Therefore, the unit of analysis is the unit of time over which those characteristics are measured. Table 14.1: Time series example country continent year lifeExp pop gdpPercap United States Americas 1987 75.020 242803533 29884.35 United States Americas 1992 76.090 256894189 32003.93 United States Americas 1997 76.810 272911760 35767.43 United States Americas 2002 77.310 287675526 39097.10 United States Americas 2007 78.242 301139947 42951.65 Forecasting involves making out-of-sample predictions for a measure within a time series. Throughout the chapters on regression, we made out-of-sample predictions each time we computed the predicted value of the outcome in our regression, \\(\\hat{y}\\), for a scenario not observed in our sample. Forecasting is no different in this regard. It is specific to predictions with time series data. Since the unit of analysis in time series data is a unit of time, an out-of-sample prediction involves a time period unobserved in our sample (i.e. the future). Analyses can seek to predict, to explain, or both. Keep in mind that forecasting is typically focused on prediction rather than explanation. Would it be helpful to know why an outcome is the value that it is in most cases? Certainly, but good decisions can be made by knowing what to expect regardless of why. Moreover, the benefits of modeling a valid explanatory model may not exceed the costs of delaying accurate predictions. If the focus is solely prediction, then we do not need to concern ourselves with internal validity or omitted variable bias. Frankly, we do not care if our model makes theoretical sense as long as its predictions are accurate. While this frees us from many constraints, it makes goodness-of-fit even more important. Therefore, the primary focus of this chapter is how to identify a good forecast model and how to choose the best model among multiple good models. Lastly, keep in mind that a forecast involves confidence intervals. Whereas explanatory regression produces confidence intervals around the estimated effect of an explanatory variable on an outcome, forecasts produce confidence intervals around the predicted value of a future outcome. These confidence intervals convey the range of values that our forecast model expects the future outcome to fall within some percentage of simulated futures. Figure 14.1 shows one forecast model of Australian tourism with 10 simulated futures based on resampling. This is not a typical way to show a forecast. Instead, forecasts are usually shown with a shaded confidence interval as in Figure 14.2. The darker region represents an 80% confidence interval and the lighter region represents a 95% confidence interval. These confidence intervals are based on the simulated futures. Figure 14.1: Simulated futures of a forecast Figure 14.2: Confidence intervals based on simultated futures 14.2 Patterns We rely on patterns to make good forecasts. A time series that exhibits no patterns offers no information for predicting the future. Time series can exhibit the following three types of patterns: Trend: a long-term increase or decrease Seasonal: a repeated pattern according to a calendar interval usually shorter than a year Cyclic: irregular increases or decreases over unfixed periods of multiple years With a time series of U.S. GDP in Figure 14.3, we can see two of the aforementioned patterns. First, there is an obvious upward trend. Secondly, there appear to be irregularly spaced plateaus or dips, most of which represent economic recessions. Recessions exhibit a cyclical pattern. Phenomena related to weather or holidays, such as energy production, consumption, and travel, are likely to exhibit seasonal patterns like the sales data shown in Figure 14.4 below. Figure 14.3: U.S. GDP 1975-2019 Figure 14.4: Sales data 14.2.1 Autocorrelation Again, it is useful for forecasts if a time series exhibits a pattern. Another way to think of a pattern is that past values provide some information for predicting future values. Whereas correlation measures the linear association between two variables, autocorrelation measures the linear association between an outcome and past values of that outcome. We can use an autocorrelation plot to examine if past values appear to predict future values. Figure 14.5 below is an autocorrelation plot of U.S. GDP. For all measurements along the time series of GDP, the autocorrelation plot quantifies the correlation between a chosen “current” GDP and past measurements of GDP called lags. Figure 14.5 goes as far as 22 lagged measures. The blue dashed line denotes the threshold at which the correlations are statistically significant at the 95% confidence level. We can see that the first lag of GDP is almost perfectly correlated with current GDP. In other words, last quarter’s GDP is a very strong predictor of current GDP. The strength of the correlation decreases over time but remains statistically significant. This gradual decrease in autocorrelation is indicative of time series with a trend pattern. Figure 14.5: Autocorrelation of U.S. GDP Figure 14.5 below shows the autocorrelation from the quarterly sales time series that exhibited a seasonal pattern. The autocorrelation plot suggests that each even-numbered lag is correlated with the current sales measure, switching between negative and positive each time. This peak and valley pattern is common in seasonal data. Figure 14.6: Autocorrelation of sales In each of the examples above, we can use information from the past to predict the future. A time series that shows no autocorrelation is called white noise. White noise provides us no significant information about predicting the future. Figures 14.7 and 14.8 below is an example of white noise. Note there is no discernible pattern in the time series plot and no autocorrelations are statistically significant. Figure 14.7: White noise time series Figure 14.8: Autocorrelation of white noise 14.3 Forecasting basics Forecasts use past observed data to predict future unobserved data. If time series exhibits a pattern such that autocorrelation is present, we can use the past to improve predictions of the future. 14.3.1 Evaluation The central goal of a forecast is to provide the most accurate prediction. How can we evaluate the accuracy of our predictions if the future events have not occurred? As was the case in previous chapters on regression, a forecast essentially draws a line through data. We can get a sense of how accurate our forecast model is by comparing its predictions to observed values. That is, we can use the residuals of a forecast model to evaluate its goodness-of-fit. A better fitting model is expected to generate more accurate predictions, on average. Residuals Figure ?? shows a forecast model denoted by the red line that simply uses the previous GDP measure to predict current GDP, compared to observed GDP denoted by the blue line. Recall how strongly lagged GDP was correlated with current GDP. This results in a forecast that appears to fit the trend fairly well. Nevertheless, there is error for almost every year, and since GDP in this time window exhibits a consistent upward trend, using last year’s GDP causes a consistent underestimation. Figure ?? below plots the residuals between observed and predicted GDP–the vertical distance between blue and red lines–in the top panel. The bottom-left panel is a autocorrelation plot for the residuals–computing the correlation between current residuals and lagged residuals–and the bottom-right panel shows the histogram of the residuals. Figure ?? provides a lot of useful information related to the central goal of forecasting. In order for us to conclude we have a good forecast, two goals must be met: The time series of residuals should be white noise, and the residuals should have a mean approximately equal to zero. It is difficult to tell from the top panel of Figure ?? whether these goals are met. However, notice that the residuals are almost always positive, which we would expect since we know our forecast almost always underestimates GDP. Therefore, the mean is certainly greater than zero, as can be seen in the histogram. The autocorrelation plot of the residuals suggests that residuals lagged up to six time periods is significantly correlated with current residuals. This is further evidence that the time series of our residuals is not white noise. A good forecast extracts as much information from past data as possible to predict the future. If it fails to do so, then lagged residuals will be correlated with current residuals. Therefore, our simple forecast for GDP has not extracted all the information from the past that could inform future predictions, resulting in a sub-par forecast. Root Mean Squared Error Multiple models could achieve residuals that are white noise and have a mean equal to zero. We can further evaluate forecast models by comparing their root mean squared errors (RMSE). Recall from Chapter 6 that the RMSE quantifies the typical deviation of the observed data points from the regression line and is analogous to the standard deviation or standard error measures. In fact, the 95% confidence interval around a forecast is based on two RMSEs above and below the point forecast, just as two standard errors are used to construct a 95% confidence interval around a point estimate in regression. Table 14.2 shows a set of standard goodness-of-fit measures for our simple forecast of GDP. We will only concern ourselves with RMSE. According to the results, the point forecast of our model is off by plus-or-minus 137 billion dollars, on average. If we developed a model with a smaller RMSE, we would prefer it to this model, provided its residuals behave no worse. Table 14.2: Forecast goodness-of-fit measures ME RMSE MAE MPE MAPE MASE ACF1 Training set 110.1394 137.2821 118.1553 1.421887 1.475215 0.2562912 0.4933519 14.3.2 Models There are four basic forecasting models: Mean: future outcomes predicted to equal the average of the outcome over the entire time series Naive: future outcomes predicted to equal the last observed outcome Drift: draws a straight line connecting the first and last observed outcome and extrapolates it into the future Seasonal naive: same as naive but predicts each future season to equal its last observed season Figure ?? below applies the mean, naive, and drift forecast models to U.S. GDP. It should be obvious that using the mean is a poor choice and will be for any time series with a strong trend pattern. Under normal circumstances absent of an impending economic shutdown, we would likely conclude that the drift model provides a more accurate forecast than the naive model. Figure 14.9: Comparison of forecast models to trend data Figure 14.10: Comparison of forecast models to trend data Figure 14.11: Comparison of forecast models to trend data According to the drift model, predicted GDP for the next ten time periods is shown in Table ??. Again, this is not a sophisticated model, and some may be alarmed by making predictions based on simply connecting the first and last observations, then extending the line into the future. It is important to keep in mind that the utility of a forecast is not the exact point forecasts in Table ??. In fact, it would be misleading to report GDP in Q2 of 2020 is predicted to be 21.65 trillion dollars. The utility of a forecast is the corresponding confidence interval. If this is our best model, then we can report that GDP in Q2 of 2020 is predicted to be between 21.48 and 21.81 trillion dollars with 95% confidence. Table 14.3: Forecast values Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 2020 Q2 21645.05 21539.73 21750.36 21483.98 21806.11 2020 Q3 21755.19 21605.84 21904.53 21526.78 21983.59 2020 Q4 21865.33 21681.91 22048.74 21584.82 22145.83 2021 Q1 21975.46 21763.10 22187.83 21650.68 22300.25 2021 Q2 22085.60 21847.53 22323.68 21721.50 22449.71 2021 Q3 22195.74 21934.24 22457.25 21795.81 22595.68 2021 Q4 22305.88 22022.67 22589.10 21872.74 22739.02 2022 Q1 22416.02 22112.44 22719.60 21951.74 22880.30 2022 Q2 22526.16 22203.31 22849.01 22032.41 23019.91 2022 Q3 22636.30 22295.09 22977.51 22114.46 23158.14 Perhaps more sophisticated methods would provide a better forecast model. If so, then the model will fit observed data better, resulting in more precise confidence intervals. Greater precision could indeed be valuable depending on the context, as many decisions can be aided by considering best- and worst-case scenarios. Nevertheless, as long as our model achieves residuals that look like white noise with a mean approximately equal to zero, we can be fairly confident that our model is not wildly inaccurate though it may be less precise than an alternative model. Let us check the residuals for our drift model. As can be seen in Figure 14.12, the mean of the residuals is approximately zero, but it appears that there is still information in past measures not extracted by our simple drift model. These results suggest we should try to improve our model. Figure 14.12: GDP drift residuals The figures below compare mean, naive, and seasonal naive models using the seasonal sales data from earlier. Because this time series does not exhibit a clear trend, the mean model is not as obviously bad as it was with GDP, though it is highly imprecise. The same applies to the naive model. If we care about predicting specific seasons (i.e. quarters), then clearly the seasonal naive model is the preferred choice. Figure 14.13: Comparison of forecast models to seasonal data Figure 14.14: Comparison of forecast models to seasonal data Figure 14.15: Comparison of forecast models to seasonal data Let us check the residuals of the seasonal naive model. The residuals have a mean of zero, and with the exception of one significantly correlated residual for lag 4, it appears we have mostly white noise. This model may be sufficient in many cases. The fact that sales from a year ago still provide information for current sales suggests there may be an annual trend component to this time series that our seasonal naive model does not extract. Therefore, a better model is achievable. Figure 14.16: Residual check 14.4 Recap We have only scratched the surface of forecasting. The corresponding R Chapter covers how to implement the models and plots above as well as incorporating explanatory variables into a forecast model. Here are the key takeaways from this chapter: Prediction does not care about the theory of a model. Patterns in time series contain information that can be used to predict the future. A good forecast model extracts all useful information from the past to predict the future. If this is achieved, the residuals from our forecast will look like white noise and have a mean equal to zero. The best model among competing good models is the model with the smallest RMSE. "],
["15-panel-analysis.html", " 15 Panel Analysis 15.1 Panel data 15.2 Fixed effects", " 15 Panel Analysis “The more things change, the more they are the same.” —Jean-Baptiste Alphonse Karr 15.1 Panel data Recall from Chapter 2 that panel data measures the same units over multiple time periods. Table 15.1 below provides an example of panel data. Panels for geographic or political areas such as counties, school or voting districts, states, and countries are common and easy to obtain. Due to privacy protections and challenges of following people over time, panels of individual people are somewhat more difficult to obtain but are quite common. Table 15.1: Panel example country continent year lifeExp pop gdpPercap Argentina Americas 1997 73.275 36203463 10967.282 Argentina Americas 2002 74.340 38331121 8797.641 Argentina Americas 2007 75.320 40301927 12779.380 Bolivia Americas 1997 62.050 7693188 3326.143 Bolivia Americas 2002 63.883 8445134 3413.263 Bolivia Americas 2007 65.554 9119152 3822.137 Cross-sectional data is like having a single picture for each unit among multiple units. We use variation across those units with respect to some variable (e.g. income, unemployment rates) to explain or predict outcomes of interest. Time series is like having a video of one subject, following that subject over multiple time periods. We use variation over time to explain or predict outcomes of interest for that subject. Panel data is like having videos for multiple subjects. Therefore, we have variation across units and over time to use for explaining or predicting outcomes of interest. 15.2 Fixed effects The additional information contained within panel data affords us a wide array of new analytic techniques that go far beyond the scope of this book. There is one technique or model, however, that is probably the most common and very easy to use: the fixed effects model. Recall the standard multiple regression model shown below. This model is slightly different from what you have seen before because it uses indexing to indicate that we have panel data. This indexing is done with the i and t subscripts, which represent subject and time, respectively. It is simply used to convey that we have multiple subjects i over multiple time periods t. \\[\\begin{equation} y_{it}=\\beta_0+\\beta_1x_{1it}+\\beta_2x_{2it}+\\cdots+\\beta_kx_{kit}+\\epsilon_{it} \\tag{15.1} \\end{equation}\\] A fixed effects model is a slightly modified version of Equation (15.1) that represents an important conceptual leap. Recall that the \\(\\epsilon_{it}\\) term represents all the factors that are associated with or affect the outcome \\(y_{it}\\) that we cannot include in our model for various reasons. This error term is inevitable and not a problem as long as there are no factors that also affect any of our explanatory variables. Otherwise, we have omitted variable bias in our model and may not be able to use our results. In most cases, someone can probably think of an omitted variable that is related to one or more of the explanatory variables. In other words, it is really difficult to convincingly guard against claims of omitted variable bias. However, having panel data allows us to guard against an important source of potential OVB by using a fixed effects model. Using a fixed effects model allows us to control for all of the omitted factors that do not change over time. The fixed effects model is represented in Equation (15.2) below. Note the new term (the Greek letter alpha) immediately to the right of the equal sign has replaced the usual y-intercept, \\(\\beta_0\\), term. Also, note the index for this new term only includes i. Because our data contains a time series for each subject i, we can model a unique y-intercept for each subject. The unique y-intercepts represent all of the stuff that makes the subjects inherently different from each other and do not change over time, or at do not meaningfully change over the time span of our data. \\[\\begin{equation} y_{it}=\\alpha_{i}+\\beta_1x_{1it}+\\beta_2x_{2it}+\\cdots+\\beta_kx_{kit}+\\epsilon_{it} \\tag{15.2} \\end{equation}\\] The fixed effect model is essentially identical to controlling for a categorical variable like we saw in Chapter 7. Recall the graph below where we controlled for the region each state is in when modeling the relationship between miles driven and traffic fatalities. We controlled for region not because we thought being in the West literally causes drivers to have more fatal accidents, but rather because regions might capture unobserved geographic or infrastructure characteristics that affect traffic fatalities. Figure 15.1: Parallel slopes for 4 groups The fixed effect model takes this idea a little further, controlling for the unobserved characteristics of each unit–the state in this case–rather than some aggregated level like region. This produces a separate regression line for each unit, as seen in Figure 15.2. For unobserved reasons, states seem to have inherent differences with respect to miles driven and fatalities. Note how flat the common regression slope is for all of the states compared to the slope of the regression without fixed effects in Figure 15.3. Ignoring the inherent differences between states leads to the conclusion that distance driven has a larger effect on the fatality rate than what the data actually suggests once we control for those differences. This is the primary reason for using a fixed effects model. Figure 15.2: Visualizing fixed effects Figure 15.3: Ignoring fixed effects There is one trade-off of using fixed effects that casual users should be aware of: using a fixed effect absorbs all constant variables. Variables that tend not to change over time such as race, sex, geography, membership to some higher-level unit (e.g. employee within an agency or union) all collapse into the fixed effect. This means that we will not obtain an estimate for these variables in a fixed effects model because they are also fixed. If we really care about getting an estimate from a time-invariant variable, then we cannot use a fixed effects model. For example, recall the regression results we obtained in Chapter 7 for the following parallel slopes model. \\[\\begin{equation} mrall = \\beta_0 + \\beta_1vmiles + \\beta_2region + \\epsilon \\tag{15.3} \\end{equation}\\] Table 15.2: Parallel slopes for regions term estimate std_error statistic p_value lower_ci upper_ci intercept 0.260 0.167 1.553 0.121 -0.069 0.589 vmiles 0.188 0.021 8.895 0.000 0.147 0.230 regionN. East -0.076 0.065 -1.166 0.245 -0.204 0.052 regionSouth 0.519 0.056 9.283 0.000 0.409 0.630 regionWest 0.641 0.062 10.264 0.000 0.518 0.763 We got estimates for three of the regions, providing us average differences in the traffic fatality rate relative to the fourth excluded region. Running a fixed effects model for Equation (15.3) generates the following results. Note the absence of a single intercept because there is a separate intercept for each state that typically is not included in a table. More importantly, there is no estimate for the regions because a state’s region is absorbed by the state’s fixed effect. We are left with an estimate for the only variable in our model that differs across time within each state: vmiles. The estimate is statistically significant at the standard 5% significance level and is substantially less than the estimate in the model ignoring fixed effects. Here, as the average miles driven per driver within a state increases by 1 mile, the fatality rate increases by 0.057 deaths, all else equal. Table 15.3: Fixed effects results term estimate std.error statistic p.value vmiles 0.057 0.019 2.956 0.003 "],
["16-r-introduction.html", " 16 R Introduction 16.1 R chapter structure 16.2 R chapter feedback 16.3 What is R and RStudio 16.4 Installing R and RStudio 16.5 RStudio orientation 16.6 Submit 16.7 Additional Resources", " 16 R Introduction This part of the book contains what are referred to as R chapters that correspond to chapters in previous parts of the book. Though previous chapters use R to present information, they focus on concepts that are applicable regardless of statistical software. R chapters take those concepts and present ways to practically apply them via a short series of exercises using R. 16.1 R chapter structure Each R chapter begins with a list of learning objectives followed by a what you need to set up in terms of packages and datasets to complete the chapter. Each chapter then guides you through a few exercises that require you to operate R. Periodically, they will ask you to interpret your results or connect what you have done to the concept it was meant to help you understand. By the end of each chapter, you will have at least one document to save and submit. That document will contain code and answers to questions. Some chapters may ask you to generate an output document such as PDF, Word, or HTML. 16.2 R chapter feedback Once you submit your work to the course site on eLC, a document will become available that contains answers prepared by your instructor. This document is meant to provide nearly immediate feedback. You should compare your work to that of your instructor, making note of any differences and attempting to make sense of them. At the beginning of each class, we will review the corresponding R chapter if students have questions. 16.3 What is R and RStudio R is a programming language for statistical computing. RStudio is a user interface for R. These two programs are analogous to a smart phone. Your phone has base code you never interact with directly but is what allows your phone to work. You interact with this code, doing all the cool things it allows you to do through what you see on the screen. R is like the base code for your phone. RStudio is like the screen. 16.4 Installing R and RStudio First, download and install R here. Windows user: click on “Download R for Windows”, then click on “base”, then click on “Download R #.#.# for Windows.” MacOS user:, click on “Download R for (Mac) OS X.” What you click on next depends on what version of macOS you are using. Under “Latest release,” you will see a link such as “R-#.#.#.pkg” with a description to the right that indicates which versions of macOS it is compatible with, such as macOS 10.13 (High Sierra) and higher. If you are using an older version of macOS, scroll down to the header “Binaries for legacy OS X systems” where you can find the link that will work with your version. If you do not know which version of macOS you are using, click on the apple symbol in the top-left of your screen, then click on “About This Mac.” The resulting window will display your version of macOS. Second, download and install RStudio here. Click on the download link beneath the “RStudio Desktop” version that is “FREE.” The website should automatically provide a link under step 2 to download the version of RStudio recommended for your computer. 16.5 RStudio orientation Exercise 1 Launch RStudio Upon launch, you should see three panes: Console pane (left) is where you can tell R what to do. It also displays the results of commands. Only use the console for installing packages. Environment pane (top right) displays all the data in your current R session. A session is the time between launching and closing R. Files pane (bottom right) allows you to navigate your files, displays plots, provides a list of installed packages, allows you to search for help, and displays file exports. You will usually see a fourth pane while working in RStudio – the source editor pane. Exercise 2 In the RStudio menu bar at the top of your screen, go to File -&gt; New File -&gt; R Script. A new pane will open. This is the pane where you will tell R what to do 99% of the time because it allows you to do so while creating a document you can save. 16.5.1 R Packages Many tasks in R require you to install R packages that augment its functionality. Extending the smartphone analogy from above, your phone comes with base programs (e.g. calendar, weather), but others create third-party applications to augment the functionality of your phone. This is also the case with R, which has an active user community that develops useful third-party apps called packages. Install Just like your phone, you have to first install a R package to use it. Install packages by typing the following code into the console pane. install.packages(&quot;name_of_package&quot;) You only need to install a package once. The package is saved on your computer where R can find it. Exercise 3 We will almost always need to use a package called tidyverse. In your console pane of RStudio, type install.packages(“tidyverse”), then click Enter on your keyboard. This will begin the installation. Monitor the console while the package installs. RStudio may ask you some Yes/No questions during the process. Answer all questions in the affirmative by typing Yes then clicking Enter. Load When an app is installed on our phone, you still have to launch it to use it. The same goes for using a R package. Each time you launch RStudio, you need to load the package(s) whose functions you plan to use. Therefore, loading packages should be done in the source editor. The following generic code is used to load a package. library(name_of_package) Note that to load a package, we do not use quotation marks around the name like we do when installing. Exercise 4 You should have a script open in the source editor (upper-left) pane. Type library(tidyverse) in the script. Click Run or Cmd+Enter to execute this line of code. The console pane will provide information about the loading. You must load a package before using any functions included in that package or else you will receive the following error message, Error: could not find function. Remember this crucial fact. I will tell you what packages are needed for certain tasks, but remember that you need to install and load the package to use it. 16.6 Submit Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download. 16.7 Additional Resources There are many resources that provide orientations to R. Below are a few that offer a thorough and accessible introduction. Chapters 1-3 of Getting Used to R, RStudio, and R Markdown BasicBasics of RYouWithMe by R-Ladies Sydney "],
["17-r-data.html", " 17 R Data 17.1 Learning Objectives 17.2 Set-up 17.3 Viewing datasets 17.4 Glimpsing Datasets 17.5 Preview first or last few rows 17.6 Submit", " 17 R Data 17.1 Learning Objectives In this chapter, you will learn how to: Examine datasets to determine their units of analysis and structures Examine variables to determine their type Print a preview of the first or last few rows of a dataset 17.2 Set-up You should by now know how to start a script and a notebook. For this chapter, use a script. As a reminder, everything you type in a script R will interpret as code to execute. You can add comments not meant as code by adding a hashtag # before the line of text. Use comments to answer any questions. To complete this R chapter, you need to load the following packages. Add this code to your script and run it. library(tidyverse) library(carData) #if this fails, you need to install Before we begin, go to the Packages tab in the bottom-right pane. Find carData in the list and click on the name. This should take you to the Help tab, which will contain the documentation for carData. This page serves as a directory to all of the datasets that come loaded with the package. We will be examining some of these datasets. If you want or need to learn more about a particular dataset, you can click on its name in this list. 17.3 Viewing datasets It is assumed most readers are familiar with spreadsheets. Perhaps one of the first obstacles to using R is that you do not constantly stare at a spreadsheet, creating somewhat of a disconnect between what you do to data and seeing it done. You can examine a dataset in spreadsheet form using the following command: View(dataset) where you replace dataset with the actual name of the dataset. Exercise 1: In your script, use the View command on the Arrests dataset that should be available in your current R session. A new tab should have appeared in the top-left pane containing the spreadsheet version of Arrests. This dataset contains information on arrests for possession of small amounts of marijuana in the city of Toronto. Exercise 2 Based on what you see, what is the structure of this dataset? What is the unit of analysis? Let’s examine a new dataset called Florida which contains county voting results for the 2000 presidential election. Exercise 3 What is the structure and unit of analysis of the Florida dataset? Exercise 4 Do the same thing one more time with the USPop dataset. What is its structure and unit of analysis? 17.3.1 Warning about View View is useful but should not be included in a notebook that you plan to export to another document. This is because R will attempt to print the entire dataset to the export document. This is almost always a mistake. To avoid making this mistake, I suggest you not use the View function as code, but rather use a point-and-click alternative. Run the following code to save Arrests to your environment pane in the top-right. arrests &lt;- Arrests Now click on arrests. This should do the same thing as running View. 17.4 Glimpsing Datasets If your dataset is moderately large, View is an inefficient way to get a sense of your data. The glimpse function generates a compact printout providing key information about a dataset. glimpse(dataset) Exercise 5 Use glimpse on Arrests. Notice that the results show you the dimensions of the dataset–the number of rows (observation) and columns (variables). Next, it provides a vertical list of variables, with several of their values listed horizontally, that can export to documents more easily. Exercise 6 Having now examined Arrests using View and glimpse, what type are the following variables based on the taxonomy used in the chapter on data. year age sex The column immediately to the right of the variable name is also informative. It tells you how each variable is stored in R. A variable can be stored in several ways: Integers: commonly used for discrete variables Doubles/numerics: commonly used for continuous variabls but can also store discrete variables Logicals: commonly used for categorical variables that are binary (i.e. 1 or 0). In R, logicals are assigned TRUE, if equal to 1, or FALSE, if equal to 0. Factors: commonly used for categorical variables. Factors can store categorical variables with any number of levels. Therefore, a binary variable can be stored as a factor instead of a logical if you want the variable to be assigned different values like “Yes” or “No.” Characters: commonly used for strings of text that don’t fit the other storage types well, such as open-ended responses in a survey. However, any variable can be stored as a character. A numerical variable can be stored as a character and R will not recognize its values as numbers. Character variables are often denoted with quotation marks \"\" around them. Exercise 7 Are the variables in exercise 6 stored in a way that makes sense given your answers? Variables will not always be stored the way they should. Sometimes we have to tell R how to store a variable based on our own understanding of their type. This skill will be covered later. 17.5 Preview first or last few rows Sometimes we want to show people what the data looks like in spreadsheet form, or we want to examine the first or last few rows in our dataset. As previously, mentioned the View function prints the entire dataset, which is almost always too much. We can preview the top few rows of a dataset using the head function like so head(dataset, n = number_of_rows_you_want_to_print) or the bottom few rows of a dataset with the tail function tail(dataset, n = number_of_rows_you_want_to_print) Exercise 8 Print the first and last 4 rows of the USPop dataset. Note that R prints the rows in descending order according to how the dataset is ordered. For tail R went up 4 rows from the bottom and printed the 4 rows in descending order. 17.6 Submit Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download. "],
["18-r-missing-data.html", " 18 R Missing Data 18.1 Learning Objectives 18.2 Set-up 18.3 Data 18.4 Checking for missing data 18.5 Counting missing values 18.6 Bypassing missing values 18.7 Replacing missing values 18.8 Submit", " 18 R Missing Data 18.1 Learning Objectives In this chapter, you will learn how to: Determine if a dataset has missing values Determine which variables in a dataset have missing values and how many values are missing Run functions on variables that have missing values Replace all missing values with a non-missing value, such as 0, if doing so is advisable 18.2 Set-up To complete this chapter, you need to Open a script Load the following packages library(tidyverse) library(carData) 18.3 Data We will be using the SLID data from the carData package to learn how to deal with missing data. Per its documentation, “The SLID data frame has 7425 rows and 5 columns. The data are from the 1994 wave of the Canadian Survey of Labour and Income Dynamics, for the province of Ontario. There are missing data, particularly for wages.” As is always the case when we begin working with new data, we want to get a sense of what it contains. Exercise 1: Use glimpse to examine SLID. This is a moderately large dataset with 7,425 observations. Obviously, it would be crazy to look for missing values by scrolling through a spreadsheet. We can see from the glimpse results that wages definitely has missing values. 18.4 Checking for missing data We can tell R to check if an entire dataset has any missing data using the following function anyNA(dataset) where we dataset with the name of the dataset. If the dataset has at least one missing value, then anyNA will return TRUE. Exercise 2: Use anyNA to confirm SLID has missing values. The anyNA hasn’t told us anything we didn’t already know given the obvious NAs present in wages. Next, we may want to know which variables have missing values. To determine which variables have missing values, we want to run anyNA repeatedly for each variable in our dataset. To run any function repeatedly on each row or column of a dataset, we can use the following function: apply(dataset, 1 (for rows, or) 2 (for columns), function) where we replace dataset with the name of our dataset, include either 1 or 2, and replace function with the name of the function we want to repeat. Exercise 3: Use apply to run the anyNA function repeatedly on each column. Your results should tell you that wages, education, and language contain missing values. 18.5 Counting missing values Once we know a variable has missing values, we typically want to know how many values are missing or what percentage of total observations are missing for that variable. The is.na function tests every value of a variable for whether it is missing. If a value is NA, is.na returns TRUE. To illustrate, the below code assigns a series of ten values to v, five of which are missing. This v object is no different from a variable in a dataset. Then, using the is.na function on v will return a list of values accordingly. v &lt;- c(NA, 5, NA, 4, 10, 11, NA, NA, 1, NA) is.na(v) ## [1] TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE TRUE Recall in Chapter 17 that the logical value of TRUE equals 1 in R, while FALSE equals 0. This means we can do math on TRUE/FALSE values just like we would if they were coded as 1/0. If is.na gives us TRUE for every NA, then adding all the TRUEs will give us the total count of missing values. To sum all the values of any variable, we can use the sum function sum(is.na(v)) ## [1] 5 The result tells us 5 of the 10 values in v are missing. We can easily determine that 50% of the data for v is missing. But what if we have some denominator that is not as easy as 10? We can quickly to determine the percent of missing values by taking the average of TRUEs and FALSEs from the is.na function because the average sums the values of the variable and divides by the number of values. We take the average of the is.na function using the mean function mean(is.na(v)) ## [1] 0.5 As expected, we get 0.5 or 50%. Building from this example, we can quantify the total and percent of missing values for wages like so sum(is.na(SLID$wages)) ## [1] 3278 mean(is.na(SLID$wages)) ## [1] 0.4414815 Wages is missing 3,278 observations, or about 44% of all observations. Exercise 4: Use the sum and mean function to determine the count and percent of missing values for the education and language variables. If we had, say, 10 variables with missing values, the process above would be rather tedious. Like before, we can tell R to repeatedly quantify missing values for each variable using a slightly different function: sapply(SLID, function(x) sum(is.na(x))) ## wages education age sex language ## 3278 249 0 0 121 sapply(SLID, function(x) mean(is.na(x))) ## wages education age sex language ## 0.44148148 0.03353535 0.00000000 0.00000000 0.01629630 18.6 Bypassing missing values Many functions that execute some kind of computation (e.g. sum, average) do not work if you execute them on variables that contain missing values. This is deliberate so users are notified of missing values. For instance, let’s try to calculate average years of education. mean(SLID$education) ## [1] NA In order to have functions bypass missing values, we have to include the na.rm=TRUE option that tells R to skip NAs. mean(SLID$education, na.rm = TRUE) ## [1] 12.49608 Since education is missing only 3% of its values, this is probably a good approximation of what the average would be if there were no missing values. Exercise 5: Compute the average for wages. It is unclear what to do with average wages since almost half of its values is missing. At the very least, we can report something like, “Only 56% of respondents reported a wage. Of those who reported a wage, the average equals $15.55.” 18.7 Replacing missing values There are several reasons wages may be missing. The respondent could be unemployed, a student, or a stay-at-home parent. We don’t know exactly why wages is missing values. For the sake of this example, let’s assume all missing wages means the respondent is unemployed (a bad assumption since this would suggest an unemployment rate of about 44%). Suppose we wanted to know the average wages for our entire sample, regardless of whether they are employed. In that case, we want to replace the missing values with 0. To replace NAs with some other value, we can use the replace_na function like so dataset_nomissing &lt;- replace_na(dataset, list(variable1 = 0, variable2 = 0,...)) where we create a new dataset indicating we’ve replaced the missing values (we don’t want to overwrite the original data). Inside the replace_na function, we include the name of the dataset then list the names of the variables and the value we want to use to replace their missing values. Exercise 6: Create a new dataset SLID_nomissing that replaces all missing values for wages with 0. Then, calculate the average wage for this new data. 18.8 Submit Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download. "],
["19-r-description.html", " 19 R Description 19.1 Learning Objectives 19.2 Set-up 19.3 Introduction 19.4 Summary Table 19.5 Using Arsenal 19.6 Submit", " 19 R Description 19.1 Learning Objectives In this chapter, you will learn how to: Make a professional quality table of descriptive statistics 19.2 Set-up To complete this chapter, you need to Start a notebook Load the following packages library(tidyverse) library(arsenal) library(knitr) library(carData) 19.3 Introduction Summary statistics tables are ubiquitous in reports and studies. Usually a project involves numerous variables that would require too many visualizations to summarize, though we should still consider visualizations for the most important variables. A standard summary stats table provides readers a reference for key measures pertaining to all our variables in a fairly compact form. They can even provide new insights in lieu of a visualization. In this chapter, we set out to summarize variables within the States dataset of the carData package. Exercise 1: Use the glimpse function to examine the States dataset. States is a cross-section of the 50 states and D.C. containing education and related statistics. Be sure to read the help page for States to understand each variable. You can do that by typing States in the search bar of the bottom-right pane of RStudio or going to the carData package under the Packages tab and clicking on the States link. 19.4 Summary Table Summary tables come in many styles, so there is no way to cover everything. In most cases, a summary table includes the following descriptive measures depending on the type of variable: Numerical variables Mean Standard deviation Minimum Maximum Categorical variables Counts for each level, and/or Percentages for each level If a variable is skewed, then it may be wise to replace the mean and standard deviation with the median, first quartile, and third quartile. We will learn how to do this. 19.5 Using Arsenal Due to the many styles of summary tables, there are numerous R packages designed to produce summary tables. The best R package in terms of quickly getting the information to a nicely formatted table of which I am aware is Arsenal. Therefore, we will learn how to use Arsenal. I will demonstrate Arsenal using the gapminder data with which we are all familiar. Then, I will ask you to replicate those demonstrations using the States data. Producing a summary table with Arsenal involves at least two, probably three, steps. Create a new object containing the summary statistics we want to include in a table Relabel the variables to something appropriate for our audience Generating the summary table based on the new object we just created Here is an example using gapminder data without altering any of Aresenal’s default options that we will want to know how to alter in most cases. sum.gapminder &lt;- tableby(~ continent + gdpPercap + lifeExp + pop, data = gapminder) labels(sum.gapminder) &lt;- c(continent = &quot;Continent&quot;, gdpPercap = &quot;GDP Per Capita&quot;, lifeExp = &quot;Life Expectancy&quot;, pop = &quot;Population&quot;) summary(sum.gapminder, title = &quot;Summary Stats for Gapminder Data&quot;) Summary Stats for Gapminder Data Overall (N=1704) Continent Africa 624 (36.6%) Americas 300 (17.6%) Asia 396 (23.2%) Europe 360 (21.1%) Oceania 24 (1.4%) GDP Per Capita Mean (SD) 7215.327 (9857.455) Range 241.166 - 113523.133 Life Expectancy Mean (SD) 59.474 (12.917) Range 23.599 - 82.603 Population Mean (SD) 29601212.325 (106157896.744) Range 60011.000 - 1318683096.000 The second code chunk above requires a specific code chunk option in order for the table to print correctly. The top line of a code chunk includes {r} by default. To print the summary table from the Arsenal package correctly using the summary() function, we need to add the code chunk option results='asis'. Therefore, the top line of the code chunk should be {r, results='asis'}. Exercise 2: Replicate the code shown above to create a default summary table for the States data using the Arsenal package. Be sure to relabel the variables to something relatively understandable and brief. Labeling is tedious but you only need to do it once. You can run the code individually or knit your notebook to preview the table. In three relatively short bits of code, we already have a decent summary table that would have taken excruciatingly long to input manually. But it can be made better. 19.5.1 Adjustments Decimal digits The biggest aesthetic issue with this table is including so many decimals. None of these variables have such a small range that rounding to integers masks useful information. Obviously, if a variable only ranges between 0 and 1, we would not want to round to an integer. Specifying the number of decimals is quite easy with Arsenal. Because arsenal tries to be as flexible as possible, we have to specify the number of decimals separately for numerical and percentage measures. The following code sets the number of decimals to zero for the gapminder data. sum.gapminder2 &lt;- tableby(~ continent + gdpPercap + lifeExp + pop, data = gapminder, digits = 0, digits.pct = 0) labels(sum.gapminder2) &lt;- c(continent = &quot;Continent&quot;, gdpPercap = &quot;GDP Per Capita&quot;, lifeExp = &quot;Life Expectancy&quot;, pop = &quot;Population&quot;) summary(sum.gapminder2, title = &quot;Summary Stats for Gapminder Data&quot;) Summary Stats for Gapminder Data Overall (N=1704) Continent Africa 624 (37%) Americas 300 (18%) Asia 396 (23%) Europe 360 (21%) Oceania 24 (1%) GDP Per Capita Mean (SD) 7215 (9857) Range 241 - 113523 Life Expectancy Mean (SD) 59 (13) Range 24 - 83 Population Mean (SD) 29601212 (106157897) Range 60011 - 1318683096 Exercise 3: Replicate the code shown above to create a summary table for the States data with no decimals. You will need to copy-and-paste the labels code. Reporting median and IQR Instead of the mean and standard deviation, we may want to report the median, first quartile, and third quartile for our numerical variables. We can control the descriptive measures using the following code. sum.gapminder3 &lt;- tableby(~ continent + gdpPercap + lifeExp + pop, data = gapminder, digits = 0, digits.pct = 0, numeric.stats = c(&quot;median&quot;, &quot;q1q3&quot;, &quot;range&quot;)) labels(sum.gapminder3) &lt;- c(continent = &quot;Continent&quot;, gdpPercap = &quot;GDP Per Capita&quot;, lifeExp = &quot;Life Expectancy&quot;, pop = &quot;Population&quot;) summary(sum.gapminder3, title = &quot;Summary Stats for Gapminder Data&quot;) Summary Stats for Gapminder Data Overall (N=1704) Continent Africa 624 (37%) Americas 300 (18%) Asia 396 (23%) Europe 360 (21%) Oceania 24 (1%) GDP Per Capita Median 3532 Q1, Q3 1202, 9325 Range 241 - 113523 Life Expectancy Median 61 Q1, Q3 48, 71 Range 24 - 83 Population Median 7023596 Q1, Q3 2793664, 19585222 Range 60011 - 1318683096 Exercise 4: Replicate the code shown above to create a summary table for the States data that reports median and the first and third quartiles. Across groups Finally, instead of reporting summary statistics for the entire sample, we may want to report them separately for each level of a categorical variable. This is a common way to make comparisons. We can have Arsenal report across groups by adding the categorical variable to the left side of the formula in the tableby code. The code below reports the gapminder data across continents. By default, Arsenal tests for correlations across groups and reports a p-value. This is not a common part of a summary table, so I turn this feature off with the test = FALSE within the code below. sum.gapminder4 &lt;- tableby(continent ~ gdpPercap + lifeExp + pop, data = gapminder, digits = 0, digits.pct = 0, test = FALSE) labels(sum.gapminder4) &lt;- c(continent = &quot;Continent&quot;, gdpPercap = &quot;GDP Per Capita&quot;, lifeExp = &quot;Life Expectancy&quot;, pop = &quot;Population&quot;) summary(sum.gapminder4, title = &quot;Summary Stats for Gapminder Data&quot;) Summary Stats for Gapminder Data Africa (N=624) Americas (N=300) Asia (N=396) Europe (N=360) Oceania (N=24) Total (N=1704) GDP Per Capita Mean (SD) 2194 (2828) 7136 (6397) 7902 (14045) 14469 (9355) 18622 (6359) 7215 (9857) Range 241 - 21951 1202 - 42952 331 - 113523 974 - 49357 10040 - 34435 241 - 113523 Life Expectancy Mean (SD) 49 (9) 65 (9) 60 (12) 72 (5) 74 (4) 59 (13) Range 24 - 76 38 - 81 29 - 83 44 - 82 69 - 81 24 - 83 Population Mean (SD) 9916003 (15490923) 24504795 (50979430) 77038722 (206885205) 17169765 (20519438) 8874672 (6506342) 29601212 (106157897) Range 60011 - 135031164 662850 - 301139947 120447 - 1318683096 147962 - 82400996 1994794 - 20434176 60011 - 1318683096 Exercise 5: Replicate the code shown above to create a summary table for the States data that reports across regions. 19.5.2 Export to CSV Knitting your notebook to HTML, Word, or PDF should produce a summary table in the appropriate format. Depending on our or others’ workflow, we may want to export our summary table to CSV in order to easily open in Excel or other spreadsheet software. Arsenal can easily handle this. To export my gapminder summary to CSV, I need to create a new object that contains the actual summary table. Below, I save the last summary to the object named sum.table. sum.table &lt;- summary(sum.gapminder4, title = &quot;Summary Stats for Gapminder Data&quot;) Next, I need to convert this table into a data frame using the as.data.frame() function like so. sum.table &lt;- as.data.frame(sum.table) Lastly, I just need to save this data frame as a CSV file using the write.csv() function like so. write.csv(sum.table, file = &quot;sumtable.csv&quot;) R will save the CSV file to my project folder. Otherwise, R will save the file to my current working directory. 19.6 Submit Arsenal has many more options. The help page and links to tutorials can help you generate various types of summary tables. Submit your notebook to eLC. "],
["20-r-visualization.html", " 20 R Visualization 20.1 Learning Objectives 20.2 Set-up 20.3 Grammar of graphics 20.4 Histogram 20.5 Box plot 20.6 Bar chart 20.7 Scatter plot 20.8 Line graph 20.9 Submit", " 20 R Visualization 20.1 Learning Objectives In this chapter you will learn how to make the following visualizations: Histogram Box plot Bar chart Line graph Scatter plot The code used to make the above visualizations in 5 will be provided and explained. Then, you will be asked to replicate the visualization using different data. 20.2 Set-up To complete this chapter, you need to Start a notebook Load the following packages and data library(tidyverse) library(data.table) # contains fread function to import from URL library(fpp2) countyComplete &lt;- fread(&#39;https://www.openintro.org/data/R/county_complete.R&#39;) For everything but the line graph, we will use the countyComplete data within the openintro package. This dataset contains 3,143 counties and 53 variables. For the line graph, we will use the prisonLF data within the fpp2 package. This dataset is a quarterly time series of prisoner numbers in Australia from 2005 to 2016, split by sex, state, and legal status. 20.3 Grammar of graphics R uses the grammar of graphics to make visualizations. You need to define three essential elements to produce a graph: data: defines the dataset containing our variable(s) of interest aes: defines the variables used to generate the plot and how they are used geom: defines the kind of plot We plot variables from data to aesthetic attributes of geometric objects. The function we use to do this is called ggplot. The generic code below shows the essential elements that will produce a default plot. We replace data with the name of the dataset. Within the aes parentheses, we tell R to assign one or more variables to a variety of attributes, such as x or y or color. What we include within aes depends on the type of plot we want to make, which is determined by the second line which always includes geom_ followed by the type of plot. ggplot(data, aes(x = variable, ...)) + geom_type() For example, the below code takes the variable gdpPercap from the dataset gapminder and maps it to the horizontal x axis of a chosen geometric object called a histogram. ggplot(gapminder, aes(x = gdpPercap)) + geom_histogram() Data, aesthetics, and geometries are the essential elements needed to generate a graph. If you tell R these three elements correctly, it will produce a graph. There are additional elements that can be added to make graphs be more effective or look better that will be covered in class. Aesthetics take variables in your data and assign them to attributes that correspond to the geometric object you intend to use. That is, aes and geom work together and must be compatible. For example, you can’t generate a scatter plot–geom_point–if you only define an x aesthetic. You must define an x and y aesthetic for a scatter plot. The figure below lists various aesthetic attributes. The type of your variable also informs which aesthetic(s) to use. The following work well with visualizing continuous variables: x and y size The following aesthetics work well with visualizing categorical variables: labels color and fill shape linetype size 20.4 Histogram Here is the code used to make the histogram from Chapter 5. The dataset is named college_grad_students and the variable assigned to the x axis aesthetic is named grad_median, which is the median earnings of full-time employees with various graduate school majors. Then, a geom_histogram is added. The rest of the code inside the histogram parentheses, the labs parentheses (stands for labels), and the theme_classic is optional and used to make the histogram look more polished. ggplot(college_grad_students, aes(x = grad_median)) + geom_histogram(bins = 30, fill = &#39;steelblue&#39;, color = &#39;white&#39;) + labs(x = &#39;Median earnings&#39;, y = &#39;Count of graduate majors&#39;) + theme_classic() Figure 20.1: Histogram of full-time median earnings for different graduate school majors Here is the code for the same histogram without the optional code. This is all that is needed to generate a histogram. In general, all plots require very little code if we do not care what they look like. ggplot(college_grad_students, aes(x = grad_median)) + geom_histogram() Exercise 1: In the countyComplete dataset, there is a variable named bachelors_2010 that measures the percent of the county population with a bachelor’s degree between 2006-2010. Suppose we want to visualize the distribution of bachelors with a histogram. Generate a simple, default histogram (no optional code unless you want to add it). 20.5 Box plot Below is the code used for the box plot in Chapter 5. Like the histogram, a box plot visualizes the distribution of one variable but uses descriptive measures median, first and third quartile, and identifies extreme values. Therefore, we only need to tell R which variable to assign to the either the x or y axis. Note that I assign grad_median to the y axis so that the box plot is vertical, which is merely a stylistic choice. Assigning grad_median to the x axis would make the box plot horizontal. Then, geom_boxplot is used to tell R to make a boxplot from grad_median. Again, all the code after geom_boxplot is optional and was used to make the box plot look more polished. The fill = 'steelblue' changes the color of the box, labs is used to help the reader understand what grad_median measures, theme_classic is one of several themes built within R that changes the look of a plot, and the code inside the theme function removes all of the ink related to the x axis due to it being unnecessary. The theme function allows you to control every element of a plot. Unique themes can be created and saved for replication, saving time and avoiding errors. ggplot(college_grad_students, aes(y = grad_median)) + geom_boxplot(fill = &#39;steelblue&#39;) + labs(y = &#39;Median earnings&#39;) + theme_classic() + theme(axis.line.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank()) Figure 20.2: Box plot of full-time median earnings for different graduate school majors Again, if we do not care how the box plot looks, all we need to make the plot is shown in the code below. ggplot(college_grad_students, aes(y = grad_median)) + geom_boxplot() Exercise 2: Generate a simple, default box plot for bachelors_2010 (no optional code unless you want to add it). 20.6 Bar chart There are two functions that make bar graphs: geom_bar and geom_col. Recall that a bar chart is used to show counts or proportions of levels within a categorical variable. In Chapter 5, a bar chart was used to show the counts and proportions of graduate majors defined as having either high or low unemployment. The table below shows a few rows and variables from the data. Note that these data are disaggregated with respect to the count of majors belonging to high or low unemployment. That is, we need our bar chart to count the number of rows with “high” and “low” in the unemp_cat column. In this case, we should use geom_bar. major grad_total grad_unemployment_rate unemp_cat grad_median Public Administration 42661 0.059 high 75000 Political Science And Government 695725 0.039 low 92000 International Relations 69355 0.045 low 86000 Public Policy 15284 0.031 low 89000 Below is the code used to generate the side-by-side or dodged bar chart from Chapter 5. Note the use of geom_bar, which requires either an x or y aesthetic to be defined. Here, I assign the categorical variable unemp_cat to the x aesthetic, making the bar chart vertical. Assigning it to the y aesthetic would make the bar chart horizontal. Again, all the code past geom_bar is optional. ggplot(gradschool, aes(x = unemp_cat)) + geom_bar(fill = &#39;steelblue&#39;) + labs(y = &#39;Count of degrees&#39;) + theme_classic() + theme(axis.line.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank()) Here is what the bar chart looks like without the optional code. ggplot(gradschool, aes(x = unemp_cat)) + geom_bar() Below is the code used to generate the stacked bar chart showing counts, Figure 5.6. The code within aes is less intuitive. Since geom_bar requires an x or y aesthetic to be defined, I have to assign x to nothing via the blank quotation marks. The fill = unemp_cat tell R to stack the bar chart, filling the bar with the counts of high and low. ggplot(gradschool, aes(x = &quot;&quot;, fill = unemp_cat)) + geom_bar() + labs(y = &#39;Count of degrees&#39;, fill = &#39;Unemployment&#39;) + theme_classic() + theme(axis.line.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank()) Below is what the stacked bar chart looks like by default. ggplot(gradschool, aes(x = &quot;&quot;, fill = unemp_cat)) + geom_bar() Lastly, the code below is used to show proportions rather than counts. The only substantive difference between this code and the code above is the use of position='fill' within the geom_bar function. This tells R to show proportions. ggplot(gradschool, aes(x = &quot;&quot;, fill = unemp_cat)) + geom_bar(position = &#39;fill&#39;) + labs(y = &#39;Proportion of degrees&#39;, fill = &#39;Unemployment&#39;) + theme_classic() + theme(axis.line.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank()) Exercise 3: Suppose we want to visualize how many counties each state has. That is, we want to count how many rows belong to each state in the countyComplete data using a bar chart. Generate a bar chart that achieves this. Choose the type of bar chart you deem best. When should we use geom_col instead? When our counts are already aggregated in our data. Refer back to the table above. Note that grad_total and grad_median contain the count of graduates within each major and their median pay, respectively. Therefore, we do not need R to count the number of rows in our data, but rather report each number already included in the data. The geom_col function takes these counts and visualizes them using a bar (or column) chart. The below code shows how to generate Figure 5.9, which visualized the median pay for the four majors in the data related to those offered by SPIA. Median pay is simply a number in the data that does not need counting, thus the code uses geom_col, which requires both an x and y aesthetic to be defined. To allow room for the long names of each major, I made the bar chart horizontal by assigning major to the y aesthetic. Each bar visually represents the numbers for grad_median in the above table. There is a new piece of code in the below chunk that can be used to reorder bars in ascending or descending order, which is generally preferred over random order of peaks and valleys. The reorder(major, -grad_median) code tells R to reorder the majors in the plot in ascending because of the minus sign; removing it would reverse the order to descending. ggplot(gradschool_spia, aes(y = reorder(major, -grad_median), x = grad_median)) + geom_col(fill = &#39;steelblue&#39;) + theme_classic() + labs(x = &#39;Median pay&#39;) + theme( axis.title.y = element_blank(), axis.line.y = element_blank(), axis.ticks.y = element_blank() ) Below is what the bar chart looks like without the optional code. ggplot(gradschool_spia, aes(y = major, x = grad_median)) + geom_col() 20.7 Scatter plot The code below shows how the scatter plot in Chapter 5 was generated. This scatter plot actually contains two geometric objects. The geom_point function generates the scatter plot, and the geom_smooth function generates the regression/trend line. Note the assignment of x and y aesthetics that every scatter plot requires. Everything beyond geom_point is optional. ggplot(gradschool, aes(x = grad_median, y = grad_total)) + geom_point(color = &#39;steelblue&#39;, size = 2) + geom_smooth(method = &#39;lm&#39;, se = FALSE, linetype = &#39;dashed&#39;, color = &#39;black&#39;) + scale_y_log10(label=scales::comma_format()) + labs(y = &#39;Total degrees&#39;, x = &#39;Median pay&#39;) + theme_minimal() Below is the scatter plot without optional code. ggplot(gradschool, aes(x = grad_median, y = grad_total)) + geom_point() Exercise 4: Pick two variables in the countyComplete data and plot their relationship using a simple scatter plot. 20.8 Line graph Line graphs are best for visualizing variables over time (i.e. time series). The prisonLF data separate prisoner counts by male vs. female, remanded vs. sentenced, and state. Therefore, there are four times series for each state. The code below generates a line graph for the time series of female prisoners who were sentenced in each Australia state. Note how this code is different from the code before because I need to manipulate it before creating the plot. Specifically, I pipe the prisonLF data into the filter verb, which keeps only the rows with Female and Sentenced. Then, I pipe that result into the typical ggplot. However, I do not need to specify the dataset because it is already being piped into ggplot. Therefore, ggplot only needs aes and geom to be defined. Time t is assigned to the x aesthetic, count is assigned to the y aesthetic, and state is assigned to the color aesthetic. The color aesthetic is a common way to plot multiple groups. It also provides a legend by default. The geom_line function generates a line graph. prisonLF %&gt;% filter(gender == &#39;Female&#39; &amp; legal == &#39;Sentenced&#39;) %&gt;% ggplot(aes(x = t, y = count, color = state)) + geom_line() + labs(color = &#39;State&#39;, y = &#39;Female Sentenced Prisoners&#39;, x = &#39;&#39;) + theme_minimal() Exercise 5: Create a line graph for male prisoners who were sentenced by state. 20.9 Submit Save your notebook and submit to eLC. Once you submit, answers will become available for download. "],
["21-r-regression.html", " 21 R Regression 21.1 Learning Objectives 21.2 Set-up 21.3 Running Regression 21.4 Reporting Results 21.5 Submit", " 21 R Regression 21.1 Learning Objectives In this chapter, you will learn how to: Run a regression model Have R report key results 21.2 Set-up To complete this chapter, you need to Open a script Load the following packages library(tidyverse) library(moderndive) library(Stat2Data) We will use the TeenPregnancy dataset within the Stat2Data package. We need to manually load specific datasets from Stat2Data in order to use them. Therefore, run the following code, and the dataset should show up in your Environment pane in the top-right. data(&quot;TeenPregnancy&quot;) Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar. 21.3 Running Regression Chapters 6 and 7 cover the following regression models: Simple linear regression with two numerical variables Multiple linear regression with all numerical variables Including a categorical explanatory variable (parallel slopes) Regression with a categorical explanatory interacted with a numerical variable Regression with a binary categorical outcome (linear probability model) While our interpretation of results changes depending on the kind of regression model we use, the code to run a standard linear regression is the same regardless of the number and type of explanatory variables and whether the outcome variable is numerical or binary. With the exception of including an interaction, running the regression models listed above can be done with the same code structure shown below. new_object_name &lt;- lm(outcome ~ exp_1 + exp_2 + ... + exp_k, data = name_of_dataset) We name a new object that will hold the results of our regression lm is the function for linear regression (acronym for linear model) We replace outcome with the name of our outcome variable that should be either numerical or binary The tilde ~ separates the outcome from the explanatory variables We replace the exp_1 to exp_k with the names of how ever many explanatory variables we wish to include, each separated by a plus sign + We replace name_of_dataset with the name of the dataset that contains the variables for the regression model. Recall the following multiple regression model from Chapter 6. \\[\\begin{equation} FedSpend = \\beta_0 + \\beta_1Poverty + \\beta_2HomeOwn + \\beta_3Income + \\epsilon \\tag{21.1} \\end{equation}\\] I ran this regression using the following code: fedpov2 &lt;- lm(fed_spend ~ poverty + homeownership + income, data = selcounty) That’s all there is to it. I named the model fedpov2 to remind myself it was the second model I ran to examine the relationship between federal spending and poverty rate. Note that the code within the lm function mimics Equation (21.1). No matter if the explanatory variables happen to be numerical or categorical, the regression works the same in R. Lastly, I did some behind-the-scenes cleaning of the original county data discussed in Chapter 6 and named it selcounty. Therefore, I told R to use that dataset when running the regression. Exercise 1: Suppose we want to use the TeenPregnancy dataset to examine whether state teen pregnancy rates are associated with church attendance and a state’s role in the Civil War (admittedly an odd variable to include but let’s think of it as a proxy for region). The model would be represented using the following formula \\[\\begin{equation} Teen = \\beta_0 + \\beta_1Church + \\beta_2CivilWar + \\epsilon \\end{equation}\\] Run this regression model. 21.4 Reporting Results This section presents two ways to obtain results after running a regression. The first uses functions that load with the moderndive package and the second uses functions that load with R by default (i.e. Base R). The moderndive functions are somewhat more intuitive and produce results that look nicer, but the base R functions are more robust to any variety of regression model. 21.4.1 Moderndive To get a standard table of regression results from using moderndive, we can use the get_regression_table function on our saved regression model results like so get_regression_table(fedpov2) term estimate std_error statistic p_value lower_ci upper_ci intercept 23.519 1.333 17.645 0.000 20.905 26.132 poverty -0.056 0.021 -2.674 0.008 -0.097 -0.015 homeownership -0.126 0.012 -10.736 0.000 -0.149 -0.103 income -0.086 0.011 -7.723 0.000 -0.108 -0.064 And to get goodness-of-fit measures, we can use the get_regression_summaries function like so get_regression_summaries(fedpov2) r_squared adj_r_squared mse rmse sigma statistic p_value df nobs 0.064 0.063 20.72216 4.55216 4.555 71.055 0 3 3123 and if I only want the R-squared, Adjusted R-squared, and RMSE from this table, I can add the select function to the above code chunk like so get_regression_summaries(fedpov2) %&gt;% select(r_squared, adj_r_squared, rmse) r_squared adj_r_squared rmse 0.064 0.063 4.55216 Exercise 2: Produce a standard table of regression results and goodness-of-fit measures for your regression model for teen pregnancy rates using the moderndive functions. 21.4.2 Base R A comprehensive set of regression results can be obtained using the summary function on our regression model like so options(scipen = 999) # turns off scientific notation if necessary summary(fedpov2) ## ## Call: ## lm(formula = fed_spend ~ poverty + homeownership + income, data = selcounty) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.463 -2.502 -1.007 1.015 39.327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.51860 1.33290 17.645 &lt; 0.0000000000000002 *** ## poverty -0.05597 0.02093 -2.674 0.00752 ** ## homeownership -0.12582 0.01172 -10.736 &lt; 0.0000000000000002 *** ## income -0.08593 0.01113 -7.723 0.0000000000000152 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.555 on 3119 degrees of freedom ## Multiple R-squared: 0.06397, Adjusted R-squared: 0.06307 ## F-statistic: 71.06 on 3 and 3119 DF, p-value: &lt; 0.00000000000000022 which gives us most of the information from get_regression_table except for the confidence intervals as well as R-squared and Adjusted R-squared. The Residual standard error is not exactly the same as the RMSE above–it is actually equal to sigma in the full table from get_regression_summaries–but you can treat them the same. The difference between the two is statistically technical and obtaining the same RMSE as the table above requires too much new code. To get the confidence intervals, we can use the confint function like so, confint(fedpov2) ## 2.5 % 97.5 % ## (Intercept) 20.90515522 26.13203853 ## poverty -0.09700107 -0.01493693 ## homeownership -0.14880274 -0.10284564 ## income -0.10774450 -0.06411382 which uses the 95% confidence level by default. Exercise 3: Generate the key regression results and goodness-of-fit measures for your regression model for teen pregnancy rates using the Base R functions. 21.5 Submit Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download. "],
["22-r-interactions.html", " 22 R Interactions 22.1 Learning Objectives 22.2 Set-up 22.3 Including an interaction 22.4 Submit", " 22 R Interactions 22.1 Learning Objectives In this chapter, you will learn how to: Include an interaction of two explanatory variables in a regression model 22.2 Set-up To complete this chapter, you need to Open a script Load the following packages library(tidyverse) library(moderndive) library(carData) We will use the Salaries dataset within the carData package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar. 22.3 Including an interaction Though we only cover interacting a numerical variable with a categorical variable in this course, we can interact two variables of any type using the same code. In theory, we can interact more than two variables. In any case, an interaction requires us to multiply the variables within the lm function. Recall the regression model from Chapter 7 where mrall is traffic fatality rate, vmiles is the average miles driven, and jaild is whether a state imposes mandatory jail for drunk driving. \\[\\begin{equation} mrall = \\beta_0 + \\beta_1 vmiles + \\beta_2 jaild + \\beta_3 vmiles \\times jaild + \\epsilon \\end{equation}\\] We can run this regression using the following code interactmod &lt;- lm(mrall ~ vmiles + jaild + vmiles*jaild, data = trdeath) Note that the only difference from the code in the previous chapter is vmiles*jaild, which tells R to interact the two variables. Once again, the code reflects the equation. We can obtain the results of this regression using the functions we learned in the previous chapter. get_regression_table(interactmod) term estimate std_error statistic p_value lower_ci upper_ci intercept -0.384 0.221 -1.741 0.083 -0.819 0.050 vmiles 0.300 0.028 10.634 0.000 0.245 0.356 jaildyes 0.731 0.399 1.831 0.068 -0.054 1.516 vmiles:jaildyes -0.058 0.050 -1.178 0.240 -0.156 0.039 Exercise 1: Suppose we want to use the Salaries dataset to examine whether professor salary is associated with their sex and how long they have worked at the institution. Furthermore, suppose we theorize that the association between salary and how long they have worked at the insitution differs by sex, thus requiring an interaction. Therefore, we have the following model. \\[\\begin{equation} salary = \\beta_0 + \\beta_1sex + \\beta_2yrs.service + \\beta_3 sex \\times yrs.service + \\epsilon \\end{equation}\\] Run this regression model and obtain the results. 22.4 Submit Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download. "],
["23-r-nonlinear-regression.html", " 23 R Nonlinear Regression 23.1 Learning Objectives 23.2 Set-up 23.3 Quadratic term 23.4 Log Transformation 23.5 Submit", " 23 R Nonlinear Regression 23.1 Learning Objectives In this chapter, you will learn how to: Run a regression with a quadratic term Run a regression with log transformations 23.2 Set-up To complete this chapter, you need to Open a script Load the following packages library(tidyverse) library(moderndive) library(carData) We will use the Mroz dataset within the carData package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar. 23.3 Quadratic term Recall the below regression model from Chapter 8 that includes a squared term for Age, which allows our regression line to change directions once as Age changes. We included this term because Figure 8.1 suggested wages initially increase with age, then decreases. \\[\\begin{equation} Wage = \\beta_0 + \\beta_1Age + \\beta_2Age^2 + \\beta_3Educ + \\epsilon \\end{equation}\\] The below code demonstrates how to include a quadratic term within the lm function. quad_mod &lt;- lm(Wage ~ Age + I(Age^2) + Educ, data = wages) In this case, the code reflects the equation only somewhat; the I() is necessary to tell R that Age^2 is the squared version of Age. Otherwise, R would not recognize Age^2 in the data, thus excluding it from the regression. Now we can obtain results in the usual manner. get_regression_table(quad_mod) term estimate std_error statistic p_value lower_ci upper_ci intercept -22.722 3.023 -7.517 0 -28.742 -16.701 Age 1.350 0.134 10.077 0 1.083 1.617 I(Age^2) -0.013 0.001 -9.840 0 -0.016 -0.011 Educ 1.254 0.090 13.990 0 1.075 1.432 We need to alter the Mroz data slightly before running a regression. Run the following code that creates a new variable that equals 1 if lfp equals “yes” and 0 if lfp equals “no.” This is necessary because our outcome variable–even though categorical–must be represented numerically in order for the regression to work. my_Mroz &lt;- Mroz %&gt;% mutate(lfp_numeric = if_else(lfp == &quot;yes&quot;, 1, 0)) Exercise 1: Suppose we want to examine factors that explain whether married women participate in the labor force, which is a binary outcome. We use the following model: \\[\\begin{equation} lfp = \\beta_0 + \\beta_1k5 + \\beta_2age + \\beta_3age^2 + \\beta_4wc + \\beta_5lwg + \\beta_6inc + \\epsilon \\end{equation}\\] Run this regression model and obtain the results. 23.4 Log Transformation In Chapter 8, the following log-log regression model was run. \\[\\begin{equation} ln(LifeExp)=\\beta_0 + \\beta_1ln(GDPpercap) + \\beta_2Continent + \\epsilon \\end{equation}\\] The below code demonstrates how to transform a variable into its natural log within the lm function. loglog &lt;- lm(log(lifeExp) ~ log(gdpPercap) + continent, data = gapminder) Note that all we need to do is place the appropriate variables within the log() function, which R interprets as the natural log. This temporarily transforms the variables; it does not create new variables in the dataset equal to the natural log of the variables. Now we can obtain results in the usual manner. get_regression_table(loglog) term estimate std_error statistic p_value lower_ci upper_ci intercept 3.062 0.026 117.692 0 3.011 3.113 log(gdpPercap) 0.112 0.004 31.843 0 0.105 0.119 continentAmericas 0.133 0.011 12.519 0 0.112 0.154 continentAsia 0.110 0.009 12.037 0 0.092 0.128 continentEurope 0.166 0.012 14.357 0 0.143 0.189 continentOceania 0.152 0.029 5.187 0 0.095 0.210 Exercise 2: Suppose we decide we want to use the natural log of family income exclusive of wife’s income, inc, resulting in the following model \\[\\begin{equation} lfp = \\beta_0 + \\beta_1k5 + \\beta_2age + \\beta_3age^2 + \\beta_4wc + \\beta_5lwg + \\beta_6ln(inc) + \\epsilon \\end{equation}\\] Run this regression model and obtain the results. 23.5 Submit Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download. "],
["24-r-evaluations.html", " 24 R Evaluations 24.1 Learning Outcomes 24.2 Set-up 24.3 Chi-square test 24.4 T-tests 24.5 Submit", " 24 R Evaluations 24.1 Learning Outcomes In this chapter, you will learn how to: Conduct a chi-square test Conduct an independent t-test 24.2 Set-up To complete this chapter, you need to Open a script Load the following packages library(tidyverse) library(carData) library(MASS) For the chi-square test, we will use the MplsStops dataset within the carData package. For the t-tests, we will use the UScrime dataset within the MASS package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar. 24.3 Chi-square test A chi-square test, like the one demonstrated in Chapter 11, requires two steps: Create a cross-tabulation table using the table function Run the chi-square on the cross-tabulation using the chisq.test function 24.3.1 Cross-tab Below is the code used to produce the cross-tab from Chapter 11. I save the new table as polltable. Using the table function, I tell R which two variables from the poll dataset to cross-tabulate. The $ is how we identify a specific variable within a dataset. The levels of the first variable, response, will be tabulated by row, while the frequency of the levels of the second variable, party, will be tabulated by column. polltable &lt;- table(poll$response, poll$party) Table 24.1: Response by political party Republican Democrat Independent Apply for citizenship 57 101 120 Guest worker 121 28 113 Leave the country 179 45 126 24.3.2 Run chi-square Now that we have a cross-tabulation table, we can run the chi-square test. The code below demonstrates how. chisq.test(immigration_poll) ## ## Pearson&#39;s Chi-squared test ## ## data: immigration_poll ## X-squared = 100.95, df = 4, p-value &lt; 0.00000000000000022 Then, it is simply a matter of interpreting the results. Exercise 1: Using the MplsStops data, suppose we wanted to test whether receiving a citation after being stopped by the police, citationIssued, is independent of race. Both are nominal variables, so a chi-square test can be used. Run this chi-square test. Exercise 2: Are the two variables independent? Why? 24.4 T-tests To reiterate, if the two groups in a t-test are comprised of different subjects, we use an independent t-test. If they are comprised of the same subjects, then we use a dependent t-test. 24.4.1 Independent t-test The code below demonstrates how the independent t-test from Chapter 11 was conducted. The t.test function works a lot like the lm function in that the outcome is entered first, then we input the variable that identifies the groups, which is essentially an explanatory variable. The two variables are separated by ~. Then, we tell R which dataset to use, which is called jobtrain in this case. t.test(earnings ~ treatment, data = jobtrain) ## ## Welch Two Sample t-test ## ## data: earnings by treatment ## t = -1.1921, df = 275.58, p-value = 0.2342 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -11629.708 2856.939 ## sample estimates: ## mean in group 0 mean in group 1 ## 21645.10 26031.49 Exercise 3: Using the UScrimes data, suppose we wanted to test whether the probability of imprisonment, Prob, is independent of between Southern and non-Southern states, So. The outcome is numerical and the explanatory is nominal. Therefore, a t-test can be used. Run this t-test. Exercise 4: Is there an association between the two variables? Why? 24.4.2 Dependent t-test To conduct a dependent t-test, add the option paired=TRUE inside the t.test code like so t.test(earnings ~ treatment, data = jobtrain, paired = TRUE) However, this code will not work because the number of observations in the treatment and control groups are not equal. If we truly had a paired sample where the same subjects measured twice, then we should have the same number of observations in both groups. 24.5 Submit Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download. "],
["25-r-regression-diagnostics.html", " 25 R Regression Diagnostics 25.1 Learning Outcomes 25.2 Set-up 25.3 Diagnostic Plots 25.4 Variance Inflation Factor 25.5 Statistical test on assumption 25.6 Excluding observations 25.7 Submit", " 25 R Regression Diagnostics 25.1 Learning Outcomes In this chapter, you will learn how to: Produce residual vs. fitted (RVFP) and residual vs. leverage plots (RVLP) Check for multicollinearity using variance inflation factor (VIF) Exclude observations from a regression model 25.2 Set-up To complete this chapter, you need to Open a script Load the following packages library(car) library(gvlma) library(carData) We will use the States dataset within the carData package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar. 25.3 Diagnostic Plots Diagnostic plots provide us suggestive visual evidence that one or more regression assumptions have failed to hold in our model. Producing diagnostic plots is very easy. Chapters 6 and 21, the following regression was run. fedpov2 &lt;- lm(fed_spend ~ poverty + homeownership + income, data = selcounty) get_regression_table(fedpov2) ## # A tibble: 4 x 7 ## term estimate std_error statistic p_value lower_ci upper_ci ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 intercept 23.5 1.33 17.6 0 20.9 26.1 ## 2 poverty -0.056 0.021 -2.67 0.008 -0.097 -0.015 ## 3 homeownership -0.126 0.012 -10.7 0 -0.149 -0.103 ## 4 income -0.086 0.011 -7.72 0 -0.108 -0.064 term estimate std_error statistic p_value lower_ci upper_ci intercept 23.519 1.333 17.645 0.000 20.905 26.132 poverty -0.056 0.021 -2.674 0.008 -0.097 -0.015 homeownership -0.126 0.012 -10.736 0.000 -0.149 -0.103 income -0.086 0.011 -7.723 0.000 -0.108 -0.064 Once we have our regression results saved to an object, all we need to do is use the plot function, as shown in the code below. The plot function produces four plots. The first plot is the RVFP and the fourth plot is the RVLP. These two plots alone can be used to investigate your regression model’s LINE assumptions and influential data, but the second and third plots are useful too. plot(fedpov2) We want to see no obvious pattern in the RVF plot and a relatively straight line running along the 0 reference line. For this RVF plot, we can see an obvious pattern where the positive residuals are much greater than the negative residuals. This is a classic sign that the regression model violates assumption N. Whether the red line trends in one direction or another away from 0 tells us whether assumption L is violated. This assumption appears to be relatively OK. The second plot is the Normal Q-Q plot. As the name suggests, it is especially useful for checking assumption N, which was already a concern based on the RVF plot. We want the points of a Normal Q-Q plot to track along the straight, dotted line. We see clear evidence now that assumption N has failed. We will need to address this in our model in order to have valid estimates. The RVF plot did not exhibit an obvious fanning out that would indicate a violation of assumption E (i.e. heteroskedasticity). The third plot is the Scale-Location plot. It is especially useful for checking assumption E. We want to see a straight line. Here, we see some indication that the variance in our residuals is not equal as our fitted/predicted values increase. Finally, the RVL plot tells us whether any observations impose problematic influence on our regression results. As with all of the diagnostic plots produced by plot, the three most problematic observations are identified by their row number in the data. We can see that observation 2907 is the largest outlier (i.e. has the largest residual), but has only a moderate amount of leverage. The other two observations are not as much of outliers as other observations, but their leverage combined with their residual makes them more influential than the other outliers. It does not appear as though any observations have a Cook’s distance high enough to warrant removal. Exercise 1: Using the States data, run a regression model where either SATV or SATM is the outcome. Once you have the model, produce its diagnostic plots. Do any assumptions appear to be a concern? Do any particular observations appear problematic? 25.4 Variance Inflation Factor VIF is a common way to check for excessive multicollinearity. There is no strict rule for identifying multicollinearity, but a VIF between 5 and 10 signals a potential problem with multicollinearity. A VIF greater than 10 is a strong indicator of multicollinearity. To obtain the VIF, we can use the vif function from the car package like so. vif(fedpov2) ## poverty homeownership income ## 2.6846 1.2016 2.4083 None of the VIF values for the explanatory variables come close to 5. Therefore, we can be confident that multicollinearity is not an issue. Exercise 2: Obtain VIF values for your regression model. Is multicollinearity a concern? 25.5 Statistical test on assumption Visuals may be all we want or need, but we can actually conduct hypothesis testing on the critical regression assumptions for linearity, normality, and equal variance. This is also quite easy to do with the gvlma function from the gvlma package (gvlma stands for global violation of linear model assumptions). gvlma(fedpov2) ## ## Call: ## lm(formula = fed_spend ~ poverty + homeownership + income, data = selcounty) ## ## Coefficients: ## (Intercept) poverty homeownership income ## 23.51860 -0.05597 -0.12582 -0.08593 ## ## ## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS ## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM: ## Level of Significance = 0.05 ## ## Call: ## gvlma(x = fedpov2) ## ## Value p-value Decision ## Global Stat 32128.558 0.0000000000 Assumptions NOT satisfied! ## Skewness 4772.305 0.0000000000 Assumptions NOT satisfied! ## Kurtosis 27323.830 0.0000000000 Assumptions NOT satisfied! ## Link Function 4.702 0.0301309727 Assumptions NOT satisfied! ## Heteroscedasticity 27.721 0.0000001401 Assumptions NOT satisfied! Clearly, there are some serious issues with this model. That first Global Stat line is like a holistic judgment of the model based on the lower four tests. Sometimes, a model might slightly violate one assumption but not the others, resulting in a satisfied global stat. The Skewness and Kurtosis tests pertain to the normality of the residuals. As we already knew from the plots, our residuals are not normal. The Link Function pertains to the linearity of the model. This is a sign that our model is simply misspecified, perhaps requiring some nonlinear transformations or a more complicated model outside the scope of this chapter. Lastly, Heteroscedasiticity tests the assumption of equal variance in the residuals. This assumption wasn’t quite as clear from the plot. Here we receive a clear message that this too is a problem. 25.6 Excluding observations We should be careful and transparent when deciding to exclude observations from an analysis. When in doubt, do not exclude observations. In this running example, I would not exclude any observations. The problems with the model are not due to one or a few observations. Sometimes, the diagnostic plots will provide clear evidence that removing a few observations will solve the problems. First, I may want to know which counties were identified in my diagnostic plots. The code below does this via subsetting. selcounty[c(2907, 2874, 1991, 2898, 2831),] name state fed_spend poverty homeownership income 2907 Fairfax city Virginia 45.08154 5.0 72.1 97.900 2874 Prince George County Virginia 45.70920 6.7 75.4 64.171 1991 Foster County North Dakota 45.84445 7.3 75.8 41.066 2898 Alexandria city Virginia 33.05986 7.8 45.7 80.847 2831 Fairfax County Virginia 26.64889 5.1 71.9 105.416 Interesting that most of the most problematic counties come from Virginia. Perhaps something deeper is going on with Virginia, or perhaps this is a meaningless coincidence. If we decide an exclusion of observations is defensible, then we can exclude observations directly within the lm function to avoid the need to create a new dataset. In the code below, I exclude the observations in the above table from the regression model. fedpov3 &lt;- lm(fed_spend ~ poverty + homeownership + income, data = selcounty[-c(2907, 2874, 1991, 2898, 2831),]) This data has over 3,000 observations, so it is unlikely that removing 5 will have any notable impact on the results, but let’s check. get_regression_table(fedpov3) term estimate std_error statistic p_value lower_ci upper_ci intercept 24.159 1.284 18.817 0.000 21.642 26.676 poverty -0.066 0.020 -3.250 0.001 -0.105 -0.026 homeownership -0.123 0.011 -10.940 0.000 -0.145 -0.101 income -0.103 0.011 -9.490 0.000 -0.124 -0.081 The point estimates have changes a little, but the hypothesis tests are the same. Has this changed whether assumptions are violated? gvlma(fedpov3) ## ## Call: ## lm(formula = fed_spend ~ poverty + homeownership + income, data = selcounty[-c(2907, ## 2874, 1991, 2898, 2831), ]) ## ## Coefficients: ## (Intercept) poverty homeownership income ## 24.1589 -0.0655 -0.1232 -0.1026 ## ## ## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS ## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM: ## Level of Significance = 0.05 ## ## Call: ## gvlma(x = fedpov3) ## ## Value p-value Decision ## Global Stat 23865.871 0.00000 Assumptions NOT satisfied! ## Skewness 4106.412 0.00000 Assumptions NOT satisfied! ## Kurtosis 19751.553 0.00000 Assumptions NOT satisfied! ## Link Function 5.449 0.01958 Assumptions NOT satisfied! ## Heteroscedasticity 2.457 0.11699 Assumptions acceptable. Globally, no, although heteroskedasticity appears to no longer be a problem. The other tests could be due to a variety of complicated issues. Perhaps the theoretical relationships implied by the model are totally wrong. Perhaps the residuals among counties within each state are strongly correlated, thus violating assumption I. The most straightforward potential solution in this case is to try a log transformation the outcome at least and perhaps one or more explanatory variables. In the below model, I log-transform federal spending and income. fedpov4 &lt;- lm(log(fed_spend) ~ poverty + homeownership + log(income), data = selcounty[-c(2907, 2874, 1991, 2898, 2831),]) Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : NA/NaN/Inf in 'y' It appears some counties have federal spending that is 0 or negative, which cannot cannot be log-transformed. Let’s see what those are. selcounty[-c(2907, 2874, 1991, 2898, 2831),] %&gt;% filter(fed_spend &lt;= 0) name state fed_spend poverty homeownership income Skagway Alaska 0 10.8 59.1 73.500 Wrangell Alaska 0 8.3 78.7 50.389 Fortunately, only two counties were the problem. Now I’ll go ahead create a separate dataset to keep things clear. selcounty2 &lt;- selcounty[-c(2907, 2874, 1991, 2898, 2831),] %&gt;% filter(fed_spend &gt; 0) And rerun the regression. fedpov4 &lt;- lm(log(fed_spend) ~ poverty + homeownership + log(income), data = selcounty2) Does this fix our assumptions? gvlma(fedpov4) ## ## Call: ## lm(formula = log(fed_spend) ~ poverty + homeownership + log(income), ## data = selcounty2) ## ## Coefficients: ## (Intercept) poverty homeownership log(income) ## 6.72532 -0.01675 -0.01254 -0.89687 ## ## ## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS ## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM: ## Level of Significance = 0.05 ## ## Call: ## gvlma(x = fedpov4) ## ## Value p-value Decision ## Global Stat 1210.5140072 0.0000 Assumptions NOT satisfied! ## Skewness 452.0579572 0.0000 Assumptions NOT satisfied! ## Kurtosis 758.3481943 0.0000 Assumptions NOT satisfied! ## Link Function 0.1076608 0.7428 Assumptions acceptable. ## Heteroscedasticity 0.0001948 0.9889 Assumptions acceptable. This appears to have fixed the issue with linearity, but normality of the residuals is still an issue. Let’s produce a new set of diagnostic plots to visualize the difference all this has made. plot(fedpov4) With the exception of the Normal Q-Q plot, all of the plots look much better. Unfortunately, we have exhausted the options at our disposal for fixing our regression (insofar as this course is covers). As you can see from this example, regression diagnostics can take you down some interesting paths of investigation. Sometimes the solution is obvious. Other times the solution still eludes you after several iterations. Exercise 3: Try to correct your regression model based on your diagnostic results. Maybe exclude one or more observations from your regression model. 25.7 Submit Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download. "],
["A-wrangle-and-tidy-reference.html", "A Wrangle and Tidy Reference A.1 Cheatsheets A.2 Common functions", " A Wrangle and Tidy Reference Unless data are already perfectly prepared, the most time consuming part of data analysis is wrangling and tidying data. It is impossible to cover all scenarios one may encounter when preparing raw data for an analysis. Even for advanced users of R, it is not uncommon to search for an unknown solution to a new problem via the web, texts, or manuals. Attempting to memorize the plethora of functions in R that could serve as solutions would quickly result in diminishing returns. Instead, it is more realistic to obtain enough familiarity with basic wrangle and tidy problems and solutions that one knows where how to effectively search for the solution. A.1 Cheatsheets RStudio provides numerous cheatsheets to help R users reference commonly used and helpful functions. Below is a list of cheatsheets that pertain to wrangling and tidying and mostly accessible for new users. This is the most relevant cheatsheet for what you will encounter in the course: Data transformation Here are some more that are less relevant: Factors Working with string variables Dates and times A.2 Common functions Knowing just a handful of functions can help you make considerable progress in many situations. The remainder of this chapter serves as a sort of cheatsheet that distills some of the most common and relevant functions useful for problems you may encounter during the course. Functions are demonstrated using the friendly gapminder data. The tidyverse package is actually a collection of several packages designed to make the wrangle, tidy, and data exploration process as intuitive and consistent as possible. You should almost always load tidyverse, as it contains every function you may need to wrangle and tidy data. library(tidyverse) A.2.1 Wrangle Verbs filter: extract rows/cases select: extract columns/variables mutate: alter existing variables or create new variables arrange: reorder rows in ascending or descending order of one or more variables head/tail: extract the top/bottom number of rows summarize: collapses data into a table of summary statistics group_by: tells R to apply functions to each group separately; common to use with summarize Filter Use filter to extract rows from a dataset. Inversely, one can think of filter as a way to remove rows. However, remember that filter keeps the rows that meet the condition on which you filter. Therefore, you want to use a condition that keeps the rows you want. Note there are 1,704 rows in the gapminder dataset. glimpse(gapminder) ## Rows: 1,704 ## Columns: 6 ## $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… ## $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 1… ## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134,… Suppose I want to keep only countries in Asia. Then: gapminder %&gt;% filter(continent == &#39;Asia&#39;) %&gt;% glimpse() ## Rows: 396 ## Columns: 6 ## $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… ## $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 1… ## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134,… The result is a new dataset with 396 rows. Note the use of double equal signs == to tell R it is a conditional (“if equal to”) rather than setting something equal to something else, which would not make sense in this case. Suppose I want countries in Asia AND in the year 1952. Then: gapminder %&gt;% filter(continent == &#39;Asia&#39; &amp; year == 1952) %&gt;% glimpse() ## Rows: 33 ## Columns: 6 ## $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Bahrain&quot;, &quot;Bangladesh&quot;, &quot;Cambodia&quot;, &quot;China&quot;… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… ## $ year &lt;int&gt; 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952,… ## $ lifeExp &lt;dbl&gt; 28.801, 50.939, 37.484, 39.417, 44.000, 60.960, 37.373, 37.… ## $ pop &lt;int&gt; 8425333, 120447, 46886859, 4693836, 556263527, 2125900, 372… ## $ gdpPercap &lt;dbl&gt; 779.4453, 9867.0848, 684.2442, 368.4693, 400.4486, 3054.421… This results in a new dataset with 33 rows. Note the use of the ampersand &amp; to code the “and” conditional. Suppose I want countries in Asia with a life expectancy less than or equal to 40 in 1952. Then: gapminder %&gt;% filter(continent == &#39;Asia&#39; &amp; year == 1952 &amp; lifeExp &lt;= 40) %&gt;% glimpse() ## Rows: 10 ## Columns: 6 ## $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Bangladesh&quot;, &quot;Cambodia&quot;, &quot;India&quot;, &quot;Indonesi… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia ## $ year &lt;int&gt; 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952 ## $ lifeExp &lt;dbl&gt; 28.801, 37.484, 39.417, 37.373, 37.468, 36.319, 36.157, 37.… ## $ pop &lt;int&gt; 8425333, 46886859, 4693836, 372000000, 82052000, 20092996, … ## $ gdpPercap &lt;dbl&gt; 779.4453, 684.2442, 368.4693, 546.5657, 749.6817, 331.0000,… Suppose I all countries in 1952 except those in Asia. There are a few options to do this. Which option is most efficient depends on the specific case. In this case: Option 1: Using the “or” conditional | (least efficient) gapminder %&gt;% filter(continent == &#39;Africa&#39; | continent == &#39;Americas&#39; | continent == &#39;Europe&#39; | continent == &#39;Oceania&#39;) %&gt;% glimpse() ## Rows: 1,308 ## Columns: 6 ## $ country &lt;fct&gt; Albania, Albania, Albania, Albania, Albania, Albania, Alban… ## $ continent &lt;fct&gt; Europe, Europe, Europe, Europe, Europe, Europe, Europe, Eur… ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… ## $ lifeExp &lt;dbl&gt; 55.230, 59.280, 64.820, 66.220, 67.690, 68.930, 70.420, 72.… ## $ pop &lt;int&gt; 1282697, 1476505, 1728137, 1984060, 2263554, 2509048, 27800… ## $ gdpPercap &lt;dbl&gt; 1601.056, 1942.284, 2312.889, 2760.197, 3313.422, 3533.004,… Option 2: Using the shortcut %in% for multiple “or” conditionals (moderately efficient) gapminder %&gt;% filter(continent %in% c(&#39;Africa&#39;, &#39;Americas&#39;, &#39;Europe&#39;, &#39;Oceania&#39;)) %&gt;% glimpse() ## Rows: 1,308 ## Columns: 6 ## $ country &lt;fct&gt; Albania, Albania, Albania, Albania, Albania, Albania, Alban… ## $ continent &lt;fct&gt; Europe, Europe, Europe, Europe, Europe, Europe, Europe, Eur… ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… ## $ lifeExp &lt;dbl&gt; 55.230, 59.280, 64.820, 66.220, 67.690, 68.930, 70.420, 72.… ## $ pop &lt;int&gt; 1282697, 1476505, 1728137, 1984060, 2263554, 2509048, 27800… ## $ gdpPercap &lt;dbl&gt; 1601.056, 1942.284, 2312.889, 2760.197, 3313.422, 3533.004,… Option 3: Use the “not equal to” conditional != (most efficient) gapminder %&gt;% filter(continent != &#39;Asia&#39;) %&gt;% glimpse() ## Rows: 1,308 ## Columns: 6 ## $ country &lt;fct&gt; Albania, Albania, Albania, Albania, Albania, Albania, Alban… ## $ continent &lt;fct&gt; Europe, Europe, Europe, Europe, Europe, Europe, Europe, Eur… ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… ## $ lifeExp &lt;dbl&gt; 55.230, 59.280, 64.820, 66.220, 67.690, 68.930, 70.420, 72.… ## $ pop &lt;int&gt; 1282697, 1476505, 1728137, 1984060, 2263554, 2509048, 27800… ## $ gdpPercap &lt;dbl&gt; 1601.056, 1942.284, 2312.889, 2760.197, 3313.422, 3533.004,… Select Suppose I want a dataset that contains only country, continent, year, and life expectancy. There are multiple options. Which is more efficient depends on the specific case. In this case: Option 1: List the variables I want to keep (least efficient) gapminder %&gt;% select(country, continent, year, lifeExp) %&gt;% glimpse() ## Rows: 1,704 ## Columns: 4 ## $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… Option 2: List the variables I don’t want to keep (moderately efficient) gapminder %&gt;% select(-pop, -gdpPercap) %&gt;% glimpse() ## Rows: 1,704 ## Columns: 4 ## $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… Option 3: Use : to specify the range of variables, which only works because the variables I want happen to be stored next to each other (most efficient) gapminder %&gt;% select(country:lifeExp) %&gt;% glimpse() ## Rows: 1,704 ## Columns: 4 ## $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… Mutate The mutate function allows you to mutate your dataset by either changing an existing variable or creating a new one. Suppose I wanted to change GDP per capita so that it is expressed in thousands of dollars instead of dollars. Then: gapminder %&gt;% mutate(gdpPercap = gdpPercap/1000) %&gt;% glimpse() ## Rows: 1,704 ## Columns: 6 ## $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… ## $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 1… ## $ gdpPercap &lt;dbl&gt; 0.7794453, 0.8208530, 0.8531007, 0.8361971, 0.7399811, 0.78… Note that I use the name of an existing variable on the left-hand side of the equation. This overwrites the data according to the function I have specified. You can scroll up to previous glimpses to confirm that gdpPercap has indeed been divided by 1,000. Suppose I wanted a new variable that measures total GDP to have in addition to GDP per capita expressed in thousands. Since GDP per capita equals GDP divided by population, I can simply use the inverse of this calculation. Thus: gapminder %&gt;% mutate(gdpPercap = gdpPercap/1000, gdp = gdpPercap*pop) %&gt;% glimpse() ## Rows: 1,704 ## Columns: 7 ## $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… ## $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 1… ## $ gdpPercap &lt;dbl&gt; 0.7794453, 0.8208530, 0.8531007, 0.8361971, 0.7399811, 0.78… ## $ gdp &lt;dbl&gt; 6567086, 7585449, 8758856, 9648014, 9678553, 11697659, 1259… Since mutate applies mathematical functions, there are way too many possible uses to cover here. The second page of the Data transformation cheatsheet lists numerous common functions used with mutate under the “Vector Functions” header. Also, the first page of the cheatsheet lists a few different versions of mutate that can come in handy. One particularly helpful variation is mutate_at. Suppose there are multiple variables you want to mutate using the same formula. A common example is when a bunch of variables are expressed as proportions between 0 and 1 when you want them all to be expressed as percentages between 0 and 100. You could list each mutate individually, but this quickly becomes tedious. Instead, you can use mutate_at to list the variables you want to mutate, then define the function you want applied to them. For example, suppose I wanted to multiply all of the numerical variables in gapminder by 100 (doesn’t make sense but just go with it). Then: gapminder %&gt;% mutate_at(vars(year, lifeExp, pop, gdpPercap), funs(.*100)) %&gt;% glimpse() ## Rows: 1,704 ## Columns: 6 ## $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… ## $ year &lt;dbl&gt; 195200, 195700, 196200, 196700, 197200, 197700, 198200, 198… ## $ lifeExp &lt;dbl&gt; 2880.1, 3033.2, 3199.7, 3402.0, 3608.8, 3843.8, 3985.4, 408… ## $ pop &lt;dbl&gt; 842533300, 924093400, 1026708300, 1153796600, 1307946000, 1… ## $ gdpPercap &lt;dbl&gt; 77944.53, 82085.30, 85310.07, 83619.71, 73998.11, 78611.34,… The period . inside funs is a generic placeholder, telling R to multiply each of the variables inside vars by 100. Combining filter, select, and mutate You can do some serious wrangling efficiently with filter, select, and mutate. Suppose I wanted a new dataset of GDP (in billions) for European countries in 2007. euro_gdp07 &lt;- gapminder %&gt;% filter(continent == &#39;Europe&#39; &amp; year == 2007) %&gt;% mutate(gdp = (gdpPercap*pop)/1000000000) %&gt;% select(country, year, gdp) glimpse(euro_gdp07) ## Rows: 30 ## Columns: 3 ## $ country &lt;fct&gt; Albania, Austria, Belgium, Bosnia and Herzegovina, Bulgaria, … ## $ year &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2… ## $ gdp &lt;dbl&gt; 21.376411, 296.229401, 350.141167, 33.897027, 78.213929, 65.6… Arrange The arrange verb is useful if you want to identify cases that have the highest or lowest values for one or more variables. By default, arrange reorders rows in ascending order (i.e. lowest to highest). In the previous glimpse, countries are arranged in alphabetical order. Suppose I wanted them arranged based on GDP. euro_gdp07 %&gt;% arrange(gdp) %&gt;% glimpse() ## Rows: 30 ## Columns: 3 ## $ country &lt;fct&gt; Montenegro, Iceland, Albania, Bosnia and Herzegovina, Sloveni… ## $ year &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2… ## $ gdp &lt;dbl&gt; 6.336476, 10.924102, 21.376411, 33.897027, 51.774743, 65.6887… Now we see a few countries with the lowest GDP. If instead I wanted GDP arranged from highest to lowest, then: euro_gdp07 %&gt;% arrange(desc(gdp)) %&gt;% glimpse() ## Rows: 30 ## Columns: 3 ## $ country &lt;fct&gt; Germany, United Kingdom, France, Italy, Spain, Netherlands, T… ## $ year &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2… ## $ gdp &lt;dbl&gt; 2650.87089, 2017.96931, 1861.22794, 1661.26443, 1165.75989, 6… Now we see some of the wealthiest European countries. Head/Tail By default, the head and tail verbs extract the top and bottom 6 rows of a dataset, respectively. These verbs are useful if we want to show a reader a sample of the data in a familiar spreadsheet form, which can be useful. Though the output from glimpse is very useful, it does not look good in a report. The head and tail verbs allow us to provide similar information in a much more presentable format. Suppose we wanted to show a reader the three wealthiest and poorest European countries (in absolute terms). We can specify the number of rows head or tail extract using n=#. Thus: euro_gdp07 %&gt;% arrange(desc(gdp)) %&gt;% head(n=3) %&gt;% kable(format = &#39;html&#39;) country year gdp Germany 2007 2650.871 United Kingdom 2007 2017.969 France 2007 1861.228 euro_gdp07 %&gt;% arrange(gdp) %&gt;% head(n=3) %&gt;% kable(format = &#39;html&#39;) country year gdp Montenegro 2007 6.336476 Iceland 2007 10.924102 Albania 2007 21.376411 Summarize Summarize creates a new dataset by collapsing all of the cases of a dataset into one or more summary statistics. It is useful for providing quick summary stat calculations in a somewhat presentable format. I do not recommend using summarize to produce the kind of summary stats table commonly found in reports because it can become tedious and the formatting is not good enough. I recommend using the arsenal package instead. Suppose I wanted to report the average gdpPercap and lifeExp for 2007 in a rough and ready table. Then: gapminder %&gt;% filter(year == 2007) %&gt;% summarize(&#39;Average GDP per capita&#39; = mean(gdpPercap), &#39;Average life expectance&#39; = mean(lifeExp)) %&gt;% kable(format = &#39;html&#39;) Average GDP per capita Average life expectance 11680.07 67.00742 The summarize verb works with numerous summary functions listed on the second page of the Data transformation cheatsheet under the heading “Summary Functions.” Group_By The group_by verb is most commonly used in tandem with summarize. If instead of calculating a summary stat for the entire dataset, you wanted to calculate the summary stat for each group of a categorical variable separately, use group_by before using summarize. Suppose I wanted average GDP per capita and life expectancy in 2007 for each continent. Then: gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarize(&#39;Average GDP per capita&#39; = mean(gdpPercap), &#39;Average life expectance&#39; = mean(lifeExp)) %&gt;% kable(format = &#39;html&#39;) ## `summarise()` ungrouping output (override with `.groups` argument) continent Average GDP per capita Average life expectance Africa 3089.033 54.80604 Americas 11003.032 73.60812 Asia 12473.027 70.72848 Europe 25054.482 77.64860 Oceania 29810.188 80.71950 Pretty powerful! You can also use multiple grouping variables. Suppose I wanted these summary stats for each continent each year since 1997. Then: gapminder %&gt;% filter(year &gt;= 1997) %&gt;% group_by(continent, year) %&gt;% summarize(&#39;Average GDP per capita&#39; = mean(gdpPercap), &#39;Average life expectance&#39; = mean(lifeExp)) %&gt;% kable(format = &#39;html&#39;) ## `summarise()` regrouping output by &#39;continent&#39; (override with `.groups` argument) continent year Average GDP per capita Average life expectance Africa 1997 2378.760 53.59827 Africa 2002 2599.385 53.32523 Africa 2007 3089.033 54.80604 Americas 1997 8889.301 71.15048 Americas 2002 9287.677 72.42204 Americas 2007 11003.032 73.60812 Asia 1997 9834.093 68.02052 Asia 2002 10174.090 69.23388 Asia 2007 12473.027 70.72848 Europe 1997 19076.782 75.50517 Europe 2002 21711.732 76.70060 Europe 2007 25054.482 77.64860 Oceania 1997 24024.175 78.19000 Oceania 2002 26938.778 79.74000 Oceania 2007 29810.188 80.71950 A.2.2 Tidy Verbs As with wrangling, one can encounter numerous different tidying scenarios. However, most of the time tidying involves converting a wide dataset to a long dataset. The most common untidy data one encounters is a time series or panel data where each time period is stored across columns (i.e. wide) rather than down rows (i.e. long). Let’s begin with a simple time series of population taken from the gapminder data. Suppose we downloaded a dataset named uspop for U.S. population. country 1997 2002 2007 United States 272911760 287675526 301139947 We don’t want each year to be a variable. Rather, we want year to be one variable with separate levels/rows for each period. We can achieve this with pivot_longer. uspop %&gt;% pivot_longer(cols = &#39;1997&#39;:&#39;2007&#39;, names_to = &#39;year&#39;, values_to = &#39;pop&#39;) %&gt;% kable(format = &#39;html&#39;) country year pop United States 1997 272911760 United States 2002 287675526 United States 2007 301139947 Note that pivot_longer tries to make the code as intuitive as possible using natural language. First, we tell R which columns to pivot, then we tell R to name the new column ‘year’, then we tell R to name the new column with the values for population ‘pop’. Suppose we encountered a more difficult wide version of the gapminder data named gap_wide shown below. This one has multiple variables listed wide for each year. Tidying gap_wide will take two steps. First, we can separate the variable names pop/lifeExp/gdpPercap from the numeric year into two columns using pivot_longer. This will result in a column that contains all three variables that precede the year and a column that contains year. We will also need to name a third column that will contain the values that the pivoted columns contained. In the code below, I tell R which columns to pivot using cols and to name the two new columns ‘var’ and ‘year’. I use names_sep to tell that each of the columns should be separated using the underscore. Then, I give the new column that will contain the values the generic name ‘value’ since this is an temporary column. gap_long1 &lt;- gap_wide %&gt;% pivot_longer(cols = pop_1997:gdpPercap_2007, names_to = c(&#39;var&#39;, &#39;year&#39;), names_sep = &#39;_&#39;, values_to = &#39;value&#39;) DT::datatable(gap_long1, rownames = FALSE, options = list(pageLength = 5, scrollX=T)) Now we need to convert the var column to wide using pivot_wider. This will create new columns for each of the unique values contained in the ‘var’ column. Since there are three unique values, the result will be three new columns. We also need to specify which column contains the values that will be transferred over to the three new columns. In the code below, I tell R to pivot the ‘var’ column wide and take the values from the ‘value’ column. And voila; we are back to having our original, tidy data. gap_long2 &lt;- gap_long1 %&gt;% pivot_wider(names_from = var, values_from = value) DT::datatable(gap_long2, rownames = FALSE, options = list(pageLength = 5, scrollX=T)) "]
]
