# Surveys and Evaluations

> *"A census taker once tried to test me. I ate his liver with some fava beans and a nice chianti."*
>
>---Hannibal Lecter

<br>

Before considering inference with respect to regression, this chapter covers two common concerns in sampling beyond question design or sampling method: sample power and weighting. Then, hypothesis testing is introduced, which applies to any sample estimate from which we wish to make inference. Lastly, using hypothesis testing for evaluations in their simplest forms is considered.

## Learning objectives {#lo11}

```{block, type='learncheck', purl=FALSE}
- Apply the margin or error or the confidence interval of an estimate when making conclusions
- Compute the sample size required to achieve a desired margin of error or precision around an estimate given the necessary information for such a computation
- Weight survey results given sufficient information
- Identify or construct the null and alternative hypotheses of a research question
- Interpret a p-value and apply it to determine the outcome of a hypothesis test
- Distinguish between types I and II error in a research scenario
- Explain when a chi-square test or t-test is appropriate
```

## Sample size

Now that we understand how a sample of relatively small size allows us to make inferences about the population with a reasonable degree of confidence, let us next consider how to determine the size of sample we need to achieve a confidence interval with a specific degree of precision.

When President Trump's impeachment inquiry was in progress, numerous polls were conducted to gauge the sentiment of the U.S. electorate as to whether Trump should be impeached. One poll conducted by Fox News, using a sample of 1,000 voters, reported that 51% of voters support impeaching Trump with a margin of error of 3 percentage points. These results garnered national attention as the first time a majority of U.S. voters supported the impeachment of Trump. Regardless of one's opinion on the matter or whether a national majority persuades elected officials, the claim that a majority of voters supported impeachment based on this poll is dubious. 

A sample of U.S. voters was taken. From this sample, the proportion of voters in support of impeachment was calculated to serve as an estimate of the population parameter. This sample produced an estimate of 51 percent. The margin or error in surveys or polls, unless noted otherwise, refers to one-half of the 95% confidence interval, or roughly two standard errors. Therefore, the 95% confidence interval of the polling results was 48 to 54 percent. The confidence interval used to capture the unobserved population parameter includes a minority of voters supporting impeachment.

A common mistake made when interpreting estimates and confidence intervals is that the estimate is the most likely value within the confidence interval for the population parameter. **The population parameter is no more likely to equal the estimate than any other value within the confidence interval**. A confidence interval either captures the population parameter or it does not. Therefore, it was just as likely that 48 percent of voters supported the impeachment as it was 54 percent did or any percentage in between.

As long as our estimate is unbiased, we cannot influence its value. Any attempt to do so would be bias by definition. Thus, the pollsters were stuck with an estimate of 51 percent. The estimate of 51 percent was not the issue for making the conclusion that a majority of voters supported impeachment. The issue was the critical lack of precision around the estimate. Unlike the estimate, we *can* influence the precision of the confidence interval.

How many voters would the pollsters have had to survey in order to achieve a margin of error of 1 percentage point and conclude a majority of voters support impeachment?

If the outcome is dichotomous or binary, such as whether or not a respondent supports impeachment, then the equation for determining the desired sample size is as follows

\begin{equation}
n=p(1-p)({\frac{Z}{E}})^2
(\#eq:sampsizeprop)
\end{equation}

- n is the sample size
- p is the proportion of yes/true/success
- Z is the number of standard deviations we set according to what confidence interval is desired
- E is the desired margin of error

It is important to point out that the calculation in Equation \@ref(eq:sampsizeprop) occurs *prior* to the poll. Therefore, we do not know the value of $p$. Unless there is reason to expect $p$ to equal a particular proportion, it is customary to input 0.50. If we want to use a 95% confidence interval then we input 2 for $Z$. Lastly, we replace $E$ with how far we want each side of our chosen confidence interval to be below and above our estimate.

Suppose the primary purpose of the Fox News poll was to conclude a majority opinion. Then, a margin of error equal to 1 percentage point would allow a valid conclusion that a majority of voters support or oppose impeachment if the result of the poll were slightly below 49% or above 51% in favor. Choosing to use a 95% confidence interval, then the sample size necessary to achieve a margin of error equal to 1 percentage point (0.01) is

\begin{equation}
n=0.5(1-0.5)({\frac{1.96}{0.01}})^2\\
n=9,604
\end{equation}

which is likely impractical for the kind of polling that news organizations tend to conduct.

If we wish to determine the sample size needed for estimating a 95% confidence interval within a certain distance from a continuous estimate, such as the mean, we can use the following equation 

\begin{equation}
n=(\frac{sZ}{E})^2
(\#eq:sampsizemean)
\end{equation}

where $s$ is the sample standard deviation. Again, we do not have the sample standard deviation prior to obtaining the sample. We must rely on past analyses of the variable in question or a pilot study with a small sample in order to input the sample standard deviation. 

## Survey weights

Ideally, the demographic composition of survey respondents should match the demographic composition of the survey's intended population. Thanks to Census data, we have a reasonably accurate understanding of population demographics such as age, sex, race, and ethnicity for multiple geographic areas and government jurisdictions. Other organizations like Pew Research Center and Gallup provide population proportions of other various ways to form groups, such as political party or religious affiliation. 

Unfortunately, it is unlikely for the demographic composition of survey respondents to match the population. Recipients choose not to respond and surveys tend to reach some demographics disproportionately more than others. This results in over- or under-representation of certain demographic groups in our survey, which limits our ability to generalize survey results and threatens the internal validity of any estimate.

Weighting is a way to correct for a demographic mismatch between the composition of respondents and the intended population. There are multiple methods of weighting, some of which are complex, but the basic method described below can work for most cases where maximum correction is not necessary or feasible.

Suppose a poll targeted to the general U.S. public asked if workers who have illegally entered the U.S. should be 1) allowed to keep their jobs and apply for citizenship, 2) allowed to keep their jobs as temporary guest workers but not allowed to apply for citizenship, and 3) lose their jobs and have to leave the country. The poll also asked for political party affiliation. A total of 890 responses were collected, generating the following results.

```{r, include=FALSE}
poll <- immigration
poll <- rename(poll, party = political)
poll$party <- fct_recode(poll$party, Republican = "conservative", Democrat = "liberal", Independent = "moderate")
poll <- filter(poll, response != "Not sure")
poll$response <- fct_drop(poll$response)
```

```{r, echo=FALSE}
summary(poll) %>% 
  kable(format = 'html', caption = 'Illegal immigration poll results')
```

Based on the results, a plurality of 39% of the U.S. public believes illegal immigrants should leave the country and 31% believe they should be allowed to apply for citizenship. The question these estimates are biased by the composition of political party affiliation. About 40% of the respondents are Republican and Independent, while about 20% are Democrat. Suppose we find a national survey reporting that the U.S. is 30% Republican, 36% Independent, and 31% Democrat. Therefore, Republicans and Independents are over-represented in our survey, while Democrats are under-represented. We need to correct for this using weights.

To calculate weights, we can use the following formula.

\begin{equation}
Weight = \frac{Population}{Sample}
(\#eq:weight)
\end{equation}

Using Equation \@ref(eq:weight), we obtain the following weights for our survey

- Republican:  30/40 = 0.75
- Independent: 36/40 = 0.9
- Democrats: 31/20 = 1.55

These weights mean that each Republican response counts as only three-quarters of a response and each Democrat response counts as about 1.5 responses.

Next, we need to tabulate how many of each response was made by the three parties. 

```{r pollparty, echo=FALSE}
polltab <- table(poll$response, poll$party)
kable(polltab, format = 'html', caption = 'Response by political party')
```

Then, we multiply the values by their corresponding weight. For example, applying the weight for Republicans results in 134 responses for "Leave the country" ($179 \times 0.75$). This process gives us the following counts

```{r, include=FALSE}
pollsum <- poll %>% 
  group_by(party, response) %>% 
  summarize(total = n()) 

pollsum <- pollsum %>% 
  mutate(weight = if_else(party == "Republican", 0.75, 
                          if_else(party == "Democrat", 1.55, 0.9)),
         w.total = round(total*weight, 0))
```

```{r, echo=FALSE}
kable(pollsum, format = 'html', caption = 'Weighted survey counts')
```

According to our weighted counts, 36% of the U.S. believes illegal immigrants should leave the country, while 35% believe they should be allowed to apply for citizenship. Weighting has changed a 8 percentage point gap in these two responses to a 1 point gap.

In case it was not obvious in the example, we have to ask survey recipients to provide the demographic information upon which we plan to weight. Survey administrators must consider what variables might bias the response(s) of interest if there is a mismatch between the sample and population. Wisely, the designers of the survey above suspected if a disproportionate number of Republicans or any other political party responded, this would bias their estimates. Presumably, race and ethnicity are correlated with this response, but we do not have this information to construct a weight.

## Hypothesis testing

At its foundation, hypothesis testing is a simple procedural process. It does involve some application of statistical theory, the most fascinating and misunderstood aspect of which is the p-value. This section covers how to set up and use a hypothesis test to determine if an estimate is statistically significant.

### Null and alternative hypos

Setting up a hypothesis test first involves establishing two mutually exclusive, competing statements:

- Null hypothesis: the condition I intend to test for is not true, not the case
- Alternative hypothesis: the condition I intend to test for is true, is the case

The condition is based on our research question that warranted the analysis in the first place. For example, is the average age of MPA students less than the average age of all graduate students? Are females more likely to enroll in an MPA program than males? Does obtaining an MPA increase earnings? 

Based on our question, we make a hypothesis *before* computing an estimate that would answer the question. For example, the average age of MPA students is less than the average age of all graduate students. Females are more likely to enroll in an MPA program than males. The effect of attaining an MPA increases earnings. These examples are alternative hypotheses; the affirmative of the condition we set out to test. The null hypothesis is the negative of the condition. For example, average age of MPA students is equal to graduate students. The likelihood of enrolling in an MPA program are equal between males and females. An MPA degree does not increase earnings.

Note that the examples of alternative hypotheses were all directional. They used words like less/more than or increase/decrease. An alternative hypothesis need not be directional even though we may expect one direction over the other. For example, our alternative hypotheses could be that average age differs between MPA students and other grad students, the likelihood of enrolling differs between males and females, and attaining an MPA effects income.

I encourage students to stick with non-directional hypotheses. In additional to simplifying the analysis, doing so reduces the likelihood that we report a statistically significant result when in fact there is not one (i.e. false positive). A strong case can be and is made that the statistical analyses allow too high of a likelihood for false positives. Claiming a direction means we have theoretically ruled out half of the possible results of our analysis before we even conduct the test, thereby increasing the likelihood we get the results we expected. If we are able to do this, one might wonder why conduct the test in the first place.

Translating the above into more mathematical concepts, most research questions involve differences:  the difference in mean age between MPA students and other grad students, the difference in the proportions of female and male MPA graduates, the difference in the slopes of regression lines drawn through data points for income and education level or degree type.

Thus, the null hypothesis states that any of these differences equals zero. The alternative hypothesis states that any of these differences does not equal zero. The null hypothesis is denoted by $H_0$ and the alternative hypothesis is denoted by $H_A$.

- $H_0: \mu_{MPA}-\mu_{grad} = 0$ or $H_A: \mu_{MPA}-\mu_{grad} \neq 0$
- $H_0: \rho_{female}-\rho_{male} = 0$ or $H_A: \rho_{female}-\rho_{male} \neq 0$
- $H_0: \beta_{MPA} = 0$ or $H_A: \beta_{MPA} \neq 0$

Note the use of population parameters above. Again, inference computes a sample estimate to make inferences about a population. Therefore, our hypotheses include the unobserved population parameter. Our estimate and confidence interval represents our best guess of that population parameter. The purpose of the hypothesis test is to establish a threshold at which we are sufficiently confident in our results to make a conclusion concerning our hypotheses *prior* to viewing the results.

### Conclusion and error type

There are two possible outcomes of a hypothesis test. We either

- reject the null hypothesis, or
- fail to reject the null hypothesis.

We never accept the null hypothesis or reject the alternative hypothesis. This may seem like semantics, but it actually has important implications for our conclusions.

Suppose our results do not meet the threshold to reject the hypothesis, thus leading us to fail to reject the null hypothesis. This does not mean the null hypothesis is true. Our null hypothesis states whatever parameter of interest in our study equals zero. We do not observe the population parameter. Therefore, we cannot say that our null hypothesis is true. Instead, we say that we do not have sufficient evidence to reject the null. It may be true or false, but we cannot determine which based on our particular estimate from our particular sample. 

Conversely, if we reject the null, then our conclusion is that the null hypothesis is false. We can rule out with reasonable confidence that the population parameter does not equal zero because doing so does not require us to claim a particular value for the population parameter; just that it is not equal to zero.

A popular example of hypothesis testing is a jury decision in a court case. A defendant is accused of committing a crime. In truth, the defendant is either innocent or guilty of that crime. Ideally, the defendant is presumed innocent until proven guilty according to a jury of their peers. Therefore, the null hypothesis is that the defendant is innocent and the alternative hypothesis is that the defendant is guilty. The jury decides either that the defendant is guilty or not guilty. If guilty, then the jury has rejected the null hypothesis of innocence. If not guilty, then the jury has failed to reject the null hypothesis. Note that the jury does not decide the defendant is innocent, which would be equivalent to accepting the null or rejecting the alternative.

Despite our best efforts, there always remains some chance that we have reached the wrong conclusion with our hypothesis test. We can make one of two possible errors:

- Type I error or false positive: rejecting the null when the null is actually true
- Type II error or false negative: failing to reject the null when the null is actually false

For instance, Type I error is finding an innocent defendant guilty, a healthy patient sick, or an ineffective program effective. Type II error is finding a guilty defendant not guilty, no evidence of illness in a sick patient, or no evidence of efficacy in an effective program. Again, the null hypothesis involves the population parameter, so we do not *know* if it is actually true or not. The null must be true or false, and the outcome of our hypothesis test claims whether the null does not appear to be false or is false. Therefore, there is a probability that we have reached the incorrect conclusion.

It is impossible to eliminate the chance of Types I and II errors, though it is possible to increase or decrease their likelihoods. However, the two share an inverse relationship; as we reduce the chance of one type of error, we increase the chance of the other type of error. Depending on the context of our research question, we may be more or less concerned about Type II error, but the focus of a hypothesis test is placed on Type I error, which serves as the threshold for our decision.

### Decision rule

Before testing our hypothesis, we ask ourselves the following question:  "What is the maximum probability of Type I error that I or others should be willing to tolerate?" Actually, this question has been answered for us in most disciplines. The common threshold for this tolerance is 5% or 1% probability of rejecting the null hypothesis when the null is actually true. Social sciences typically use 5%.

With our threshold set, we can now test our hypothesis. We calculate the sample estimate and the standard error of our estimate, which are used to calculate the confidence interval. The confidence level of our confidence interval depends on our chosen threshold for Type I error. If our threshold for Type I error is 5%, then we calculate the 95% confidence interval. If our threshold is 1%, we use a 99% confidence interval.

Our confidence interval is our best guess of the plausible range of values for the population parameter. We have decided to tolerate the chance that our confidence interval is one of the five out of 100 confidence intervals--or 1 out of 100--expected to fail to capture the parameter. Our null hypothesis states that the parameter equals zero. Therefore, if our confidence interval does not contain zero, we reject the null hypothesis. If our confidence interval does contain zero, then we have failed to reject the null hypothesis because the parameter *might* equal zero with a higher probability than we decided to tolerate.

In most cases, we do not need more information than the estimate, standard error, and confidence interval to make a decision regarding our hypothesis test. However, an analysis provides us an additional piece of information that allows us to arrive at the same conclusion but from a different perspective called the **p-value**.

> The p-value tells us the probability of obtaining the estimate we did, or an estimate further away from the null hypothesis, if the null hypothesis were actually true.

The p-value provides us a concise decision rule. The tolerance threshold we set is often referred to as the significance level and denoted by the Greek letter $\alpha$ (alpha).

- If $p<\alpha$, reject the null hypothesis.
- If $p\geq \alpha$, fail to reject the null hypothesis.

Again, a typical value for $\alpha$ is 5%, or 0.05. Having chosen a 5% significance level, if our results generate a p-value of 0.04, for instance, then the likelihood of obtaining our result in a world where the null is true is 4%. Therefore, we reject the null hypothesis. If our p-value were equal to 0.06, then the likelihood of obtaining our result in a world where the null is true is 6%. This exceeds our maximum tolerance, thus we fail to reject the null hypothesis.

## Chi-square test

The Chi-square ($\chi^2$) test is a common choice for introducing the application of hypothesis testing. Chi-square is used to test whether two *nominal* variables are associated to a statistically significant extent. A nominal variable, such as race, sex, or political party affiliation, has two or more of levels. If one wanted to test if, given the level for a unit of analysis in one nominal variable (e.g. male), there is a higher likelihood for a particular level in another nominal variable to occur (e.g. Republican), a Chi-square test is an appropriate choice.

For an example, let us return to the immigration survey. We saw in \@ref(tab:pollparty) that 179 out of 357 Republicans (50%) responded that illegal immigrants should be forced to leave the country, while 101 out of 174 Democrats (58%) responded that illegal immigrants should be allowed to apply for citizenship. Is there a statistically significant pattern in responses conditional on political party affiliation, or are these differences due to random noise?

The null hypothesis for this question is that there is no association between the opinion on illegal immigration and political party affiliation. That is to say, if we chose any of the three political party levels in our data, the probability of an individual providing one of the three opinions is equal the other opinions, or

$H_0: P_{leave} = P_{guest} = P_{citizen}$

The alternative hypothesis is that there is an association between opinion on illegal immigration and political party affiliation, or 

$H_A$: at least one $P$ is not equal to the others

Next, suppose we chose to use the customary 5% statistical significance level, or $\alpha=0.05$. Now we are ready to test our hypothesis using a Chi-square test. Doing so generates the following results.

```{r, include=FALSE}
immigration_poll <- polltab
```

```{r, echo=FALSE}
chisq.test(immigration_poll)
```

This is one case where there are no confidence intervals to compute since our variables are not numerical. Instead, we rely on the p-value. Our p-value is less than 0.00000000000000022. More concisely, $p<0.05$. Therefore, we reject the null hypothesis that there is no association between the three illegal immigration opinions in the survey and political party affiliation. Furthermore, the probability for us to get the values in Table \@ref(tab:pollparty) or more extreme in a world where the null hypothesis is actually true is equal to an infinitesimal percent, not to suggest that such a small p-value is required to make inferences.

Our results allow us to make inferences such as Republicans are more likely to believe illegal immigrants should lose their jobs and have to leave the country, while Democrats are more likely to believe they should be allowed to keep their jobs and apply for citizenship. Not a particularly surprising inference, but perhaps that is because many such inferences in the past have been made using similar techniques and reported many times.

## T-test

The t-test is another common introductory application of hypothesis testing. A t-test is used to test the association between a nominal variable with two levels and a numerical variable. It is frequently used in simple program evaluations with a pre/post or treatment/control design. Both involve a nominal variable with two levels. If one want to test if a numerical outcome is different between the two levels, then a t-test is an appropriation choice.

There are two varieties of the t-test. To test if an average of a numerical outcome is different between two groups, such as a treatment and control group, then we use an **independent t-test**. To test if an average numerical outcome is different before and after a treatment for the *same* units of analysis, we use a **dependent t-test**. The difference between the two t-tests concerns how we approximate the sampling distributions and confidence intervals, but their use for hypothesis testing is essentially the same.

Suppose we work for a nonprofit that provides job training workshops and want to evaluate their effectiveness. One way to go about such a task is to compare the earnings of participants (i.e. treatment group) to the earnings of non-participants (i.e. control group). We have earnings data for 185 participants and 128 non-participants, some of which is previewed in the table below.

```{r, include=FALSE}
jobtrain <- nsw74psid3 %>% 
  select(trt, re78) %>% 
  rename(treatment = trt, earnings = re78) %>% 
  mutate(earnings = earnings*4.1)
```

```{r jobtraindata, echo=FALSE}
set.seed(383)
sample_n(jobtrain, 6) %>% 
  kable(format = 'html', caption = 'Preview of job training data')
```

Before constructing the null and alternative hypotheses, a note about the population in this example. In program evaluations or generally any analysis aimed toward testing whether some event caused an effect on an outcome of interest, there are two populations. There is the entire population of units of analysis (e.g. all people, all nations, all dogs) and the subset of that population for whom/which the program, policy, or intervention is intended. 

The choice of population leads to two slightly different questions. If the entire population is our research population, then our intent is to estimate the average effect of the program on a randomly chosen unit from the entire population. This is referred to as the **average treatment effect** (ATE). If the subset that the program targets is our research population, then our intent is to report the average effect of a randomly chosen targeted unit. This is referred to as the **average treatment on the treated** (ATT). The detailed differences between the two are beyond the scope of this book and more appropriate for a class in program evaluation or causal inference, but the existence of this difference is worth being aware of.

Given that job training programs are not intended for all people, the presumption is that we want to estimate the average effect on those the program targets. Of course, we could choose as our population only those who participated in the program. In that case, we need not bother with hypothesis tests, as we would be calculating the population parameters directly from the observed data. However, we would not be able to generalize the results.

With the population in mind, our null hypothesis is that the average earnings of participants is equal to the average earnings of non-participants, or

$H_0: \mu_{treated} = \mu_{untreated}$

Our alternative hypothesis is that the average earnings of participants is not equal to the average earnings of non-participants, or

$H_A: \mu_{treated} \neq \mu_{untreated}$

Choosing a significance level of 5%, we are ready to test our hypothesis.

A simple computation of the average earnings between the two groups provides the following information. 

```{r, include=FALSE}
jobtrainsum <- jobtrain %>% 
  group_by(treatment) %>% 
  summarize("Average Earnings" = mean(earnings))
```

```{r jobtrainsum, echo=FALSE}
kable(jobtrainsum, format = 'html', caption = 'Comparison of means between treated and untreated')
```

Participants have a higher average earnings than non-participants, but is this difference statistically significant? For that, we should use an *independent* t-test because participants and non-participants are two different groups. Running the t-test provides the following results

```{r, echo=FALSE}
t.test(earnings ~ treatment, data = jobtrain)
```

Our p-value is greater than our significance level, $0.23>0.05$. Therefore, we fail to reject the null hypothesis and conclude that there is not statistically significant evidence that participants earn more than non-participants, on average. Note that our 95% confidence interval ranges between -11,630 and 2,856. It includes zero, thus we cannot claim the difference between the means is not equal to zero with reasonable confidence.

<br>

> **To learn how to conduct chi-square and t-tests in R, proceed to Chapter \@ref(r-evaluations).**

## Key terms and concepts {#kt11}

```{block, type='learncheck', purl=FALSE}
- Margin of error
- Survey weight
- Null and alternative hypotheses
- Rejecting the null
- Failing to reject the null
- Types I and II error
- Confidence level
- Significance level
- P-value
- Chi-square test
- T-test
```