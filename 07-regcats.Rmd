# Categorical Variables and Interactions

> *"For how can one know color in perpetual green, and what good is warmth without the cold to give it sweetness?"*
>
>---John Steinbeck, Travels with Charley

## Learning objectives

```{block, type='learncheck', purl=FALSE}
- Explain why and how to extend simple or multiple regression models to a parallel slopes model
- Interpret results of a parallel slopes model
- Explain why and how to extend regression models to an interaction model
- Interpret results of an interaction model
- Provide advice on choosing between parallel slopes and interaction model
- Explain why and how to extend regression models to a linear probability model
- Interpret results of a linear probability model
```

Chapter \@ref(simple-and-multiple-regression) introduced regression using models containing quantitative variables only. In this chapter, we build our regression toolbox to include models that contain qualitative variables. We will cover three models in particular:

- Parallel slopes model: including a categorical explanatory variable
- Interaction model: allowing the slope of the regression line for each level of a categorical variable to differ (i.e. not parallel)
- Linear probability model: including a two-level (binary) categorical outcome

## Parallel slopes model

To introduce the inclusion of categorical variables in regression, the simplest type of categorical variable will be used. The simplest categorical variable is commonly referred to as a **dummy variable**. A dummy variable is a two-level or binary categorical variable. It takes on values of either 1 or 0, where 1 corresponds to "yes" or "true" and 0 corresponds to "no" or "false". 

For example, a common way to represent biological sex in data is to use a dummy variable where either male or female is coded as 1 and the other sex is coded as 0. It is common to name such a variable as the level coded as 1. For example, a dummy variable coded as 1 for male and 0 for female will often be named "male" in a dataset. A variable coded as 1 to denote a person attained a college degree and 0 to denote they did not might be named "college."

The parallel slopes model using a dummy variable is represented in Equation \@ref(eq:pslope).

\begin{equation}
y = \beta_0 + \beta_1x + \beta_2d + \epsilon
(\#eq:pslope)
\end{equation}

where $d$ is simply used to distinguish the variable as a dummy variable from the quantitative $x$ variable. The sample regression equation for the parallel slopes model is represented by Equation \@ref(eq:pslopesamp).

\begin{equation}
\hat{y} = b_0 + b_1x + b_2d
(\#eq:pslopesamp)
\end{equation}

Knowing that $d$ can equal only 1 or 0, we can plug these values into Equation \@ref(eq:pslopesamp) to understand the logic of the parallel slopes model before considering the results from an example. If $d=0$, then $b_2$ drops out of the equation because anything multiplied by 0 equals 0. In that case, our regression equation is,

\begin{equation}
\hat{y} = b_0 + b_1x
(\#eq:pslopesamp0)
\end{equation}

and we can plug in our results to predict changes or values of $y$ just like before. If $d=1$, then $b_2$ stays in the model. Anything multiplied by 1 is equal to itself, and since $d$ can only equal 1 if not equal to 0, we can drop $d$ out of the equation like so

\begin{equation}
\hat{y} = b_0 + b_1x + b_2
(\#eq:pslopesamp1)
\end{equation}

Note that $b_2$ is not multiplied by the value of another variable like $b_1$ is multiplied by some change or value of $x$. Instead, it is a constant like the y-intercept $b_0$. In fact, combining $b_0$ and $b_2$ gives us a new y-intercept as shown in Equation \@ref(eq:pslopesamp1alt).

\begin{equation}
\hat{y} = (b_0 + b_2) + b_1x
(\#eq:pslopesamp1alt)
\end{equation}

This volley of equations is not to meant to repulse the math averse. Rather, it is meant to show you that the logic of the parallel slopes model is quite simple. Including a dummy variable $d$ is analogous to drawing two separate regression lines--one line through the observations for which $d=0$ and another line through the observations for which $d=1$. The second line will be either above or below the first regression line by a constant amount equal to $b_2$, resulting in two regression lines running parallel to each other.

### Using parallel slopes

Suppose we are interested in whether state laws mandating a jail sentence for drunk driving affects traffic fatalities, presumably by deterring drunk driving. To investigate, we collect the following data, some of which is previewed in Table \@ref(tab:trdeath).

```{r, include=FALSE}
load('_bookdown_files/trdeath.RData')
```

```{r trdeath, echo=FALSE}
trdeath %>% 
  sample_n(7) %>% 
  kable(format = 'html', caption = 'Sample of state traffic data')
```

where `mrall` is number of traffic deaths per 10,000 population, `jaild` is the dummy variable for whether the state has a mandatory jail sentence for drunk driving, `vmiles` is the average miles driven per driver in a state, `mlda` is the minimum legal drinking age at the time, and `unrate` is the state's unemployment rate. There are 336 observations in this dataset (48 states from 1982 to 1988; panel data but used like a pooled cross-sectional for this example).

Suppose we choose to use the following model

\begin{equation}
mrall = \beta_0 + \beta_1vmiles + \beta_2jaild + \epsilon
(\#eq:pslopeexamp)
\end{equation}

First, let's visualize the relationship between these variables using a scatter plot with `vmiles` on the x axis and using color to differentiate states with and without mandatory jail sentences for drunk driving. Note in Figure \@ref(fig:pslopescatter) below there appears to be a positive relationship between the average miles people drive and the traffic fatalities. That makes sense.

Now, imagine drawing a straight line through the red points that denote states with no mandatory jail for drunk driving and a separate line through the blue dots denoting states with mandatory jail sentencing. Do not force your imaginary lines to be parallel just yet. How do your two lines compare? Based on the data, our blue line should be above our red line, as the group of blue dots appear to be systematically higher than the group of red dots. The slopes are less obvious. It *may* be the case that our red line should have a steeper slope than our *blue* line, but it is difficult to tell. 

```{r pslopescatter, echo=FALSE, fig.cap='Relationship between miles driven and traffic deaths by whether state has mandatory jail for drunk driving'}
ggplot(trdeath, aes(x = vmiles, y = mrall, color = jaild)) +
  geom_point() +
  labs(x = 'Average miles per driver',
       y = 'Traffic deaths per 10,000',
       color = 'Mandatory jail') +
  theme_minimal()
```

The imaginary exercise you just went through is critical. Considering whether the slopes of our regression line to differ between the two groups is the difference between the parallel slopes model and the interaction model. Why not just add the interaction and if they are the same slope, so be it? Again, because we pay a penalty for adding superfluous explanatory variables. Moreover, an interaction model is more difficult to interpret and communicate to an audience. Most importantly, we should choose the model that reflects our theory that is based on our subject matter expertise. **Choosing to use a parallel slopes model forces the slopes between the two groups to be drawn (i.e. estimated) as if they are parallel whether they actually are or not**.

Now having a sense of the relationship, we run our parallel slopes model, which generates the following results

```{r psloperesults, echo=FALSE}
pslopemod <- lm(mrall ~ vmiles + jaild, data = trdeath)

get_regression_table(pslopemod) %>% 
  kable(format = 'html', caption = 'Parallel slopes results')
```

Now we can plug our results into the sample regression equation to answer whatever questions we may have or encounter.

\begin{equation}
\hat{mrall} = -0.238 + 0.281\times vmiles + 0.265 \times jaild
\end{equation}

I strongly recommend that you compare this equation to Equations 7.2-7.5. Otherwise, the suffering was in vain. The `jaild` variable is the $d$ variable. It equals either 0 or 1. If a state does **not** have a mandatory jail sentence (jaild = 0), then we have 

\begin{equation}
\hat{mrall} = -0.238 + 0.281\times vmiles
\end{equation}

and if a state does have a mandatory jail sentence, then we have

\begin{equation}
\hat{mrall} = -0.238 + 0.281\times vmiles + 0.265\\
\hat{mrall} = (-0.238 + 0.265) + 0.281\times vmiles\\
\hat{mrall} = 0.027 + 0.281\times vmiles
\end{equation}

The results tell us that, on average, states with mandatory jail sentencing have a higher traffic fatality rate of 0.265 per 10,000, all else equal. 

We already know how to interpret the estimate for `vmiles` in Table \@ref(tab:psloperesults): The results suggest that, on average, as the average miles driven per driver in a state increases 1 mile, traffic fatalities per 10,000 increase 0.281, all else equal.

To help make this concrete, Figure \@ref(fig:pslopescatter2) visualizes the results of our parallel slopes model. Note that, as expected, the blue line is above the red line. Based on the results, we know the blue line is above the red line by 0.265. We also know that the slope for both lines is 0.281.

```{r pslopescatter2, echo=FALSE, fig.cap='Parallel slopes visualization'}
ggplot(trdeath, aes(x = vmiles, y = mrall, color = jaild)) +
  geom_point() +
  geom_parallel_slopes(se = FALSE) +
  labs(x = 'Average miles per driver',
       y = 'Traffic deaths per 10,000',
       color = 'Mandatory jail') +
  theme_minimal()
```

### Variations

We can include a categorical variable with more than two levels. Suppose we were interested in whether traffic fatalities differ across U.S. regions. From the variables in Table \@ref(tab:trdeath), we might choose to construct the following model.

\begin{equation}
mrall = \beta_0 + \beta_1vmiles + \beta_2region + \epsilon
(\#eq:pslopeexamp2)
\end{equation}

where region is a four-level categorical variable containing South, West, N. East, and Midwest. Figure \@ref(fig:pslopescatter2) visualizes this model. Note that there are now four regression lines, each corresponding to one of the four regions. There are clear differences between the regions. It appears states in the West and South are somehow different than states in the Midwest and Northeast with respect to traffic fatality rates.

```{r pslopescatter3, echo=FALSE, fig.cap='Parallel slopes for 4 groups'}
ggplot(trdeath, aes(x = vmiles, y = mrall, color = region)) +
  geom_point() +
  geom_parallel_slopes(se = FALSE) +
  labs(x = 'Average miles per driver',
       y = 'Traffic deaths per 10,000',
       color = 'Region') +
  theme_minimal()
```

Running this model produces the following results.

```{r psloperesults2, echo=FALSE}
pslopemod2 <- lm(mrall ~ vmiles + region, data = trdeath)

get_regression_table(pslopemod2) %>% 
  kable(format = 'html', caption = 'Parallel slopes for regions')
```

Note that Table \@ref(tab:psloperesults2) provides estimates for three of the four regions. This is no different from our previous model; `jaild` has two levels, yes and no, but Table \@ref(tab:psloperesults) provides only one estimate for `jaild=yes`. No matter how many levels in a categorical variable, one of the levels is set such that $d=0$ and so that level drops out of the equation. Just like with the previous model where the estimate for `jaild=yes`
indicates how far above or below the regression line is relative to the line for `jaild=no`, the estimates for whatever levels remain in the equation indicate how far above or below the regression lines are relative to the excluded level.

In Table \@ref(tab:psloperesults2), Midwest is excluded. Look at Figure \@ref(fig:pslopescatter2), noting where the Midwest line is relative to the other regions. Northeast is below Midwest, while South and West are above it. The estimates for the three included regions in Table \@ref(tab:psloperesults2) tell us how far the regions are above and below Midwest. Therefore, we could report something like

> On average and controlling for average miles driven per driver, states in the Northeast region experience fewer traffic fatalities by approximately 0.08 per 10,000, while states in the South and West experience higher traffic fatality rates by 0.52 and 0.64, respectively.

## Interaction model

But what if we allowed the two slopes in Figure \@ref(fig:pslopescatter2) to differ? If our expertise leads us to theorize the two slopes should differ between states with and without mandatory jail sentencing for drunk driving, and/or if the visualizing the data suggests they do, then we can choose to use an interaction model.

To be clear, allowing the two slopes to differ means we allow the marginal effect of the average miles driven per driver to differ between the two groups of states. Figure \@ref(fig:pslopescatter3) visualizes this added flexibility. Note that the slope of the red line is indeed slightly steeper than the blue line. Thus, this visualization suggests that average miles driven per mile in states without mandatory jail sentences for drunk driving increases the traffic fatality rate by slightly more than it does in states with such laws.

```{r interactscatter, echo=FALSE, message=FALSE,fig.cap='Interaction model visualization'}
ggplot(trdeath, aes(x = vmiles, y = mrall, color = jaild)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE) +
  labs(x = 'Average miles per driver',
       y = 'Traffic deaths per 10,000',
       color = 'Mandatory jail') +
  theme_minimal()
```

This version of an interaction model involves interacting a categorical variable with a numerical variable. Equation \@ref(eq:interaction) represents this version of the interaction model.

\begin{equation}
y = \beta_0 + \beta_1x + \beta_2d + \beta_3xd + \epsilon
(\#eq:interaction)
\end{equation}

Equation \@ref(eq:interaction) is similar to Equation \@ref(eq:pslope) for the parallel slopes model. The difference in $\beta_3xd$. This is the interaction--multiplying two of the explanatory variables in our regression model. Equation \@ref(eq:interactionsamp) provides the sample equation of this interaction model.

\begin{equation}
\hat{y} = b_0 + b_1x + b_2d + b_3xd
(\#eq:interactionsamp)
\end{equation}

Following the same process as with the parallel slopes model, we can rearrange Equation \@ref(eq:interactionsamp) to examine the logic of this interaction model. If $d=0$, then $b_2$ and $b_3x$ drop out of the model because they are multiplied by 0. Thus, we have the same sample regression equation as Equation \@ref(eq:pslopesamp0).

\begin{equation}
\hat{y} = b_0 + b_1x
(\#eq:interactionsamp0)
\end{equation}

If $d=1$, then $b_2$ and $b_3x$ remain in our model. As with the parallel slopes model, $b_2$ combines with $b_0$. This shifts the y-intercept for the group for which $d=1$ either above or below the group for which $d=0$ by the amount equal to $b_2$. Because the lines are not parallel, just because a line starts above or below another does not mean it will stay above or below. It depends on the value of the $b_3x$. The term $b_3x$ is combined with $b_1x$. Thus, if $d=1$, we have the following sample regression equation

\begin{equation}
\hat{y} = b_0 + b_1x + b_2d + b_3xd\\
\hat{y} = b_0 + b_1x + b_2 + b_3x\\
\hat{y} = (b_0 + b_2) + (b_1 + b_3)x
(\#eq:interactionsamp1)
\end{equation}

In addition to the regression line for which $d=1$ having an intercept above or below the regression line for which $d=0$ by the amount $b_2$, it will have a slope greater or lesser by the amount $b_3$. Hopefully, it is clear how these equations correspond with the lines in Figure \@ref(fig:interactscatter).

### Using an interaction

Running this interaction model on these data produces the following results

```{r interactionmod, echo=FALSE}
interactmod <- lm(mrall ~ vmiles + jaild + vmiles*jaild, data = trdeath)

get_regression_table(interactmod) %>% 
  kable(format = 'html', caption = 'Interaction model results')
```

Once again, we can plug these values into our generic sample regression equation to obtain the following equation.

\begin{equation}
\hat{mrall} = -0.38 + 0.3\times vmiles + 0.73\times jaild - 0.06\times vmiles \times jaild
(\#eq:interactionex)
\end{equation}

For states without mandatory jail sentencing (jaild = 0), the equation is 

\begin{equation}
\hat{mrall} = -0.38 + 0.3\times vmiles
(\#eq:interactionex0)
\end{equation}

and for states with mandatory jail sentencing (jaild = 1), the equation is

\begin{equation}
\hat{mrall} = -0.38 + 0.3\times vmiles + 0.73\times jaild - 0.06\times vmiles \times jaild\\
\hat{mrall} = 0.35 + 0.24\times vmiles
(\#eq:interactionex1)
\end{equation}

### Variations

Variations on interaction models are beyond the scope of this book, but suffice it to say we can interact any two variables we deem necessary (or more). If you suspect that the effect of a variable on an outcome *depends* on the value of another variable, then an interaction is how to model such a relationship. 

## Linear probability model

We have now covered the inclusion of categorical variables on the explanatory side of a regression model. We can also include categorical variables as an outcome. In fact, many interesting question involve outcomes of a categorical nature, particularly binary. For example, 

- Did the person graduate from college (yes or no)?
- Did the government default on its bond payments?
- Did the program participant get a job afterward?
- Did the nonprofit receive the grant it applied for?

As before, we may want to explain or predict such outcomes based on a set of explanatory variables.

A **linear probability model** (LPM) is just a special name we give the kind of regression we have already covered when the outcome is a dummy variable. The key difference between an LPM and what we have already done concerns probability. Whereas regression with a numerical outcome explains or predicts changes or values of the outcome in terms of the outcome's units, **the LPM explains or predicts changes or values in the _probability_ that the dummy outcome equals 1. If the dummy outcome is coded such that $d=1$ means the event did occur, then the LPM estimates the probability that the event in question occurs**.

Equation \@ref(eq:lpm) shows the generic population LPM, which is the same as the generic multiple regression population model except for the left-hand side. All this equation is trying to denote is that our estimates on the right-hand side pertain to the *probability* (Pr) that $y=1$. Equation \@ref(eq:lpmsample) shows the sample LMP equation.

\begin{equation}
Pr(y=1)=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_kx_k+\epsilon
(\#eq:lpm)
\end{equation}

\begin{equation}
\hat{Pr(y=1)}=b_0+b_1x_1+b_2x_2+\cdots +b_kx_k
(\#eq:lpmsample)
\end{equation}

Again, nothing is different with respect to how we use the above equations to answer questions concerning the predicted change or value of the outcome. We simply need to remember that those changes or values will be expressed as probabilities that $y=1$ and what it means for $y$ to equal 1 in the context of our particular question.

### Using LPM

What if instead of modeling traffic fatality rates as an outcome dependent on miles driven and mandatory jail sentencing for drunk driving, we modeled whether a state has mandatory sentencing as an outcome dependent on traffic fatality rates and region? Perhaps states passed such laws because they have high traffic fatality rates. Equation \@ref(eq:lpmex) represents our model in this case.

\begin{equation}
Pr(jaild=1)=\beta_0+\beta_1vmiles+\beta_2region+\epsilon
(\#eq:lpmex)
\end{equation}

Scatter plots for LPMs are not particularly useful for communicating to an audience, but they can provide insight to what it is we are trying to do with an LPM, if it is not yet clear. Figure \@ref(lpmscatter) changes the coding of `jaild` from yes/no to 1/0 and plots it on the y axis against traffic fatality rates on the x axis. Regions are excluded for simplicity. 

```{r, include=FALSE}
trdeath2 <- trdeath %>% 
  mutate(jaild = if_else(jaild == 'yes', 1, 0))
```

```{r lpmscatter, echo=FALSE, message=FALSE, fig.cap='LPM visualization'}
ggplot(trdeath2, aes(x = mrall, y = jaild)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  labs(x = 'Traffic fatality rate',
       y = 'Mandatory jail for drunk driving') +
  theme_minimal()
```

Immediately, we should notice Figure \@ref(fig:lpmscatter) does not look like the typical scatter plots we have viewed thus far. This is because all observations fall into one of two values for `jaild`. It is also difficult to tell how our regression line is being drawn through the data.

The points along the x axis are states for which `jaild = 0`, meaning they do not impose mandatory jail sentencing for drunk driving. The points at the top are states for which `jaild = 1`. Compared to the points at the bottom, note the slight shift to the right the points at the top seem to have made. This shift is what informs the regression line to slope upward. The pattern of these data suggests there is a positive association between traffic fatality rate and passing a mandatory jail sentencing for drunk driving.

The values of $y$ along the regression line are the predicted probabilities that a state has a mandatory jail law given the corresponding values for traffic fatality rate. For example, states with a rate of 3 appear to have a probability of 0.5 or 50% of passing a mandatory jail law.

Running this model produces the following results

```{r lpmresults, echo=FALSE}
lpm_mod <- lm(jaild ~ mrall + region, data = trdeath2)

get_regression_table(lpm_mod) %>% 
  kable(format = 'html', caption = 'LPM results')
```

Plugging these results into our sample regression equation gives us

\begin{equation}
\hat{Pr(jaild=1)}= -0.07 + 0.12 \times vmiles + 0.06 \times neast + 0.04 \times south + 0.36 \times west
(\#eq:lpmexsample)
\end{equation}

Once again, we are back to plug-and-chug. What is the predicted probability that state in the Midwest with a traffic fatality rate of 2.5 has a mandatory jail sentence for drunk driving? Answer:

```{r}
-0.07 + 0.12*2.5
```

there is a 23% likelihood that such a state has such a law. How would an increase of 2 fatalities per 10,000 affect the probability that a state imposes a mandatory jail law? Answer:

```{r}
0.12*2
```

an increase in probability by about 24%. Furthermore, our results suggest states in the West are substantially more likely to have this law. Compared to the Midwest, the West is 36% more likely to have a law and about 30% more likely than states in the South and Northeast. 

```{r, eval=FALSE, include=FALSE}
trdeath %>% 
  mutate(jaild = if_else(jaild == 'yes', 1, 0)) %>% 
  ggplot(aes(x = mrall, y = jaild)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  geom_smooth(method = glm, method.args = list(family = 'binomial'), se = FALSE, color = 'red')
```

### Fit

Because our outcome is a dummy variable, it does not have the same kind of variation we need to assess model fit like before using $R^2$ or RMSE. Since we are explaining or predicting whether or not an outcome occurs, we can assess the fit of the model based on how well it predicts the observed outcomes.

We could change this threshold, but suppose we decide that if our model predicts the likelihood an outcome at 50% or greater, then we say that our model predicts the outcome will occur $y=1$ and so $y=0$ otherwise. Table \@ref(tab: lpmpointstab) shows a few rows applying this logic. Note that each row shows the observed data for each variable in our model, then the predicted probability in the `jaild_hat` column, then the rounding of that probability to 0 or 1 in the `prediction` column. Note the similarities and differences between the observed outcomes in `jaild` and the predicted outcomes `prediction`. Sometimes our model predicts correctly, and sometimes it does not.

```{r, include=FALSE}
lpm_points <- get_regression_points(lpm_mod) %>% 
  select(-residual) %>%
  mutate(prediction = if_else(jaild_hat >=0.5, 1, 0))
```

```{r lpmpointstab, echo=FALSE}
set.seed(383)
lpm_points %>% 
  sample_n(7) %>% 
  kable(format = 'html', caption = 'Binary predictions from LPM')
```

Table \@ref(tab:confumat) below is referred to as a **confusion matrix**. It is simply a cross-tabulation of the observed outcomes and the predicted outcomes with the predictions along the top as columns.

```{r, include=FALSE}
confusion <- table(lpm_points$jaild, lpm_points$prediction)
```

```{r confumat, echo=FALSE}
kable(confusion, format = 'html', caption = 'Confusion matrix for LPM')
```

We can see that there are 248 cases where our model correctly predicts the outcome (210 + 38). There are 87 cases where our model incorrectly predicts the outcome. Specifically, there are 31 cases where our model predicts a state has a law but doesn't and 56 cases where our model predicts a state does not have a law but does. We can also convert these to percentages like the table below. These confusion matrices help us assess and communicate how accurate our model is. 

```{r, echo=FALSE}
prop.table(confusion, 2) %>% 
  kable(format = 'html', caption = 'Confusion matrix for LPM (in proportions)')
```

<br>

> **To Learn how to include interactions in a regression using R, proceed to Chapter \@ref(r-interactions).**

## Key terms and concepts

```{block, type='learncheck', purl=FALSE}
- Dummy variable
- Parallel slopes
- Interaction
- Difference between parallel slopes and interaction models
- Linear probability model (LPM)
- Confusion matrix
```