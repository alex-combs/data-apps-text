% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Data Applications in Public Administration},
  pdfauthor={Alex Combs},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=Blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother


\newenvironment{announcement}% environment name 
{% begin code 
  \par\vspace{\baselineskip}\noindent 
  \color{Announcement}\begin{itshape}% 
  \par\vspace{\baselineskip}\noindent\ignorespaces 
}% 
{% end code 
  \end{itshape}\ignorespacesafterend 
}
\newenvironment{learncheck}% environment name 
{% begin code 
  \par\vspace{\baselineskip}\noindent 
  \color{Exercise}\begin{itshape}% 
  \par\vspace{\baselineskip}\noindent\ignorespaces 
}% 
{% end code 
  \end{itshape}\ignorespacesafterend 
}
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Data Applications in Public Administration}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Using R to Learn Concepts and Skills}
\author{Alex Combs}
\date{Last updated on 17 August 2020}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\begin{announcement}
\textbf{Disclaimer}

This is a companion resource for PADP 7120: Data Applications in Public
Administration. This resource is not peer-reviewed nor is it intended to
supplant published textbooks. It is meant for distribution only to those
enrolled in PADP 7120. I chose not to adopt a textbook for the course
and offer a written version of lecture and presentation notes instead. I
do not benefit monetarily from this resource in any way.
\end{announcement}

This resource was developed primarily out of uncertainty regarding the future of in-person vs.~online teaching. If in-person instruction becomes untenable, then this resource provides an imperfect substitute for the time in class spent converting content from other textbooks into what I want and expect students to know. Regardless of what happens to instruction during class, this book provides a one-way lesson on relevant topics directly from the person in charge of evaluating your understanding and performance. Secondarily, I have harbored some disappointment with existing texts used to teach MPA students statistics and statistical software for several years. Curating sections and subsections of chapters from numerous sources in order to provide partially relevant information presented with inconsistent levels of rigor has proven counterproductive to teaching and learning. This book aims to provide a standalone resource that is appropriate and relevant for students in PADP 7120.

\hypertarget{objective}{%
\subsection*{Objective}\label{objective}}
\addcontentsline{toc}{subsection}{Objective}

The objective of this book is to help students in public administration be as competitive as possible in their desired job markets via competency in statistics and statistical software. It aims to simultaneously teach students key concepts in statistics and applications of those concepts using R.

This book is intended for students with minimal background in statistics or interest in pursuing a career in academic research. For students wanting to learn core statistical concepts and skills that are applicable to careers in public, non-profit, and health sectors with minimal need to sift through excessively theoretical or technical material, this book was developed with you in mind.

\hypertarget{style-and-structure}{%
\subsection*{Style and Structure}\label{style-and-structure}}
\addcontentsline{toc}{subsection}{Style and Structure}

Since this book is based on my lecture notes, most of the material is presented in a conversational tone. I have basically taken the core pieces of what I would say in class, adding examples and other supporting material so the content is self-contained.

Rather than provide thorough coverage of complex statistical concepts, I take some liberties in presenting stylized facts for the benefit of the reader. When using statistical software, there are multiple options to achieve an intended outcome. I provide what I consider or understand to be the best option.

This book is organized along two parallel tracks. The first track covers statistical concepts and is self-contained. The second track applies the concepts in the first track using R. The chapters in the applied track are referred to as R chapters, each of which corresponds to a conceptual chapter in the first track. For example, the \texttt{R\ Data} chapter corresponds to the \texttt{Data} chapter in the first track.

The conceptual track is divided into four sections:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data and description
\item
  Regression models
\item
  Inference
\item
  Advanced topics
\end{enumerate}

\hypertarget{software-requirements}{%
\subsection*{Software requirements}\label{software-requirements}}
\addcontentsline{toc}{subsection}{Software requirements}

This book provides examples and exercises using R. Students who intend to use a personal computer must download and install the following software:

\begin{itemize}
\tightlist
\item
  \href{https://cloud.r-project.org}{R}
\item
  \href{https://rstudio.com/products/rstudio/download/}{RStudio}
\end{itemize}

\hypertarget{supplemental-resources}{%
\subsection*{Supplemental resources}\label{supplemental-resources}}
\addcontentsline{toc}{subsection}{Supplemental resources}

There are numerous free materials that teach statistics and R. The below list includes a few books and websites that offer broader and deeper treatments of some concepts and skills covered in this book.

\begin{itemize}
\tightlist
\item
  \href{https://www.openintro.org/book/os/}{OpenIntro Statistics} by David Diez, Mine Cetinkaya-Rundel, and Christopher Barr
\item
  \href{https://open.umn.edu/opentextbooks/textbooks/quantitative-research-methods-for-political-science-public-policy-and-public-administration-with-applications-in-r-3rd-edition}{Quantitative Research Methods for Political Science, Public Policy and Public Administration (With Applications in R) - 3rd Edition} by Hank Jenkins-Smith and Joseph Ripberger
\item
  \href{https://r4ds.had.co.nz}{R for Data Science} by Garrett Grolemund and Hadley Wickham
\item
  \href{https://moderndive.com/index.html}{Statistical Inference via Data Science} by Chester Ismay and Albert Y. Kim
\item
  \href{https://www.khanacademy.org/math/statistics-probability}{Kahn Academy: Statistics and probability}
\item
  \href{https://socviz.co/index.html\#preface}{Data Visualization: A practical introduction} by Kieran Healy
\item
  \href{https://rmarkdown.rstudio.com/lesson-1.html}{R Markdown from RStudio}
\item
  \href{https://bookdown.org/yihui/rmarkdown/}{R Markdown: The Definitive Guide} by Yihui Xie, J.J. Allaire, and Garrett Grolemund
\item
  \href{https://otexts.com/fpp2/}{Forecasting: Principles and Practice} by Rob J. Hyndman and George Athanasopoulos
\end{itemize}

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

\begin{quote}
\emph{``Data, data everywhere, and not a thought to think.''}

---John Allen Paulos
\end{quote}

\hypertarget{why-statistics}{%
\section{Why statistics}\label{why-statistics}}

Statistics converts raw information (i.e.~data) into something useful. If we want to make evidence-based decisions, we need statistics. If we want to allow ourselves to be misled by nefarious or mistaken analyses of data, we should resist learning statistics.

\hypertarget{professional-standards}{%
\section{Professional standards}\label{professional-standards}}

The Network of Schools of Public Policy, Affairs, and Administration (NASPAA) is the accrediting authority for MPA programs. NASPAA promotes the following universal competencies:

\begin{itemize}
\tightlist
\item
  to lead and manage in the public interest;
\item
  to participate in, and contribute to, the policy process;
\item
  to analyze, synthesize, think critically, solve problems and make evidence-informed decisions in a complex and dynamic environment;
\item
  to articulate, apply, and advance a public service perspective;
\item
  to communicate and interact productively and in culturally responsive ways with a diverse and changing workforce and society at large.
\end{itemize}

Statistics will help you develop all of the above competencies. You would not sufficiently possess one or more of the above competencies without knowledge and skills in statistics.

\hypertarget{statistics-in-pa}{%
\section{Statistics in PA}\label{statistics-in-pa}}

The use of statistics is ubiquitous in public administration. Agencies and nonprofits use statistics to describe their clients and assess their needs. Agencies like the Government Accountability Office and watchdog organizations use statistics to monitor performance and guard against fraud. Service-oriented organizations like schools and hospitals use statistics to evaluate services and communicate to stakeholders. The Congressional Budget Office, Office of Management and Budget, and employees at every level of government use statistics to assess finances and forecast trends.

\hypertarget{using-r}{%
\section{Using R}\label{using-r}}

Before moving forward with this book, you need to learn how to operate R at a very basic level. The goal of this book is not to train you to become an expert in R or even a data scientist or analyst. Rather, the goal is to train you enough so R becomes a legitimate alternative to inferior spreadsheet software like Excel to perform tasks that may be expected of a master in public administration.

MPA students may be reluctant to learn something referred to as a statistical computing language and its relevancy to their career goals may not be clear. I firmly believe that not training you to use statistical software in a course such as this would be doing you a disservice.

Demand for those competent in statistical software like R continues to rise. Even if you plan to pursue a managerial role with minimal analytic tasks, chances are high that you will supervise or work with those who conduct analyses, and you will need to interpret their findings, applying your own managerial or subject matter expertise toward making an evidence-based decision. People in both roles--consumers and producers of statistical analyses--need to be able to communicate with the other. The best way to become a competent consumer of statistical information is to learn the basics of producing it.

In addition, R is free, comes with many free educational resources, and is popular across many disciplines. If you study statistics and data applications for a semester, you might as well spend part of that semester learning software like R. There is only upside in doing so with respect to employment prospects.

\begin{quote}
\textbf{For a brief orientation to R, proceed to Chapter \ref{r-introduction}.}
\end{quote}

\hypertarget{part-data-and-description}{%
\part{Data and Description}\label{part-data-and-description}}

\hypertarget{data}{%
\chapter{Data}\label{data}}

\begin{quote}
\emph{Nothing exists except atoms and empty space; everything else is opinion.}

---Democritus
\end{quote}

We cannot effectively convert the raw material of knowledge into a useful product without first understanding the raw material. Therefore, learning statistics naturally begins with learning the types and structures of data.

\hypertarget{lo2}{%
\section{Learning objectives}\label{lo2}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Understand the organization of rectangular data
\item
  Identify the unit of analysis within a dataset
\item
  Identify and distinguish types of variables
\item
  Identify and distinguish types of dataset structures
\end{itemize}
\end{learncheck}

\hypertarget{rectangular-data}{%
\section{Rectangular Data}\label{rectangular-data}}

Most data are rectangular, often represented using a standard spreadsheet organized by rows and columns. A rectangle of data is commonly referred to as a \textbf{dataset}.

\label{tab:print-generic}Generic rectangular data

ID

Variable\_1

Variable\_2

Unit of Analysis

Datum

Datum

Unit of Analysis

Datum

Datum

Unit of Analysis

Datum

Datum

A rectangular dataset has three components. Not all datasets will fit the below description because many datasets are not organized in a tidy manner. The elements of tidy data will be covered in a later chapter.

\begin{itemize}
\tightlist
\item
  \textbf{Unit of analysis or observation:} The generic entity or subject a row of data refers to. The unit of analysis uniquely identifies each row of a dataset. If we have a dataset of 50 states and some variables measured in 2020, then our unit of analysis is states. If you were told a specific state, then you could find the row in the dataset. If we have 50 states measured in 2019 and 2020, then the unit of analysis is state-year because you will need to know the state and year to find a specific row.
\item
  \textbf{Variable:} A measured characteristic of the unit of analysis. State unemployment rate is a variable for a state unit of analysis.
\item
  \textbf{Datum:} The intersection of a variable (column) and a unit of analysis (row) resulting in a cell. The datum is a particular piece of information. A cell could contain something like 4.8 as the unemployment rate for Georgia in 2020.
\end{itemize}

\hypertarget{types-of-variables}{%
\section{Types of variables}\label{types-of-variables}}

The variables in a given dataset can be of several types. Types of variables are important to learn because the types of variables one is dealing with has consequences for data applications, such as description, visualization, and inference.

A variable provides us raw information about the units of analysis. If statistics is a discipline to convert raw information into something useful, then it stands to reason that we should want to know what type of information a variable provides us, especially the specificity of that information.

For example, suppose you ask two strangers to report their annual income. What options do they have for answers? If virtually any value, then you know to a precise degree the income each earns and can compute the precise difference between the two incomes. What if their choices are either more or less than \$50,000? Then, you have a coarse understanding of how much they earn. If they provide different answers, you can only conclude whether one makes more than the other but not by how much. If they provide the same answer, then the two are grouped together even though it is highly unlikely they earn equal incomes. This makes a serious difference for statistical analysis.

\begin{figure}
\includegraphics[width=13.99in]{images/variables} \caption{Variable Types}\label{fig:vartypefig}
\end{figure}

All variables belong to one of two broad types: qualitative (or categorical) and quantitative (or numeric).

\begin{itemize}
\tightlist
\item
  \textbf{Qualitative} variables take on values that have no intrinsic numerical meaning. They are expressed in words.
\item
  \textbf{Quantitative} variables take on values that do have intrinsic numerical meaning.
\end{itemize}

\hypertarget{qualitative-variables}{%
\subsection{Qualitative variables}\label{qualitative-variables}}

Qualitative variables can be further differentiated into two types: nominal and ordinal.

\begin{itemize}
\tightlist
\item
  \textbf{Nominal} variables take on values that differ in name only.
\item
  \textbf{Ordinal} variables take on values that can be ranked relative to each other but the difference between rankings has no numerical value.
\end{itemize}

The values that categorical variables take on are commonly referred to as levels. Categorical variables can contain virtually any number of levels, though the number of levels is usually limited.

A variable such as sex contains two levels: male and female. The variable sex is nominal, as its values have no numerical meaning and the two levels have no ranking. Race, state, country, political party, and any variable coded as yes/no such as unemployed, married, and below the federal poverty line are all examples of nominal variables.

If you have ever participated in a customer satisfaction survey, then you have almost surely contributed data to an ordinal variable. Those scales that provide some number of options from ``disagree'' to ``agree'' are called Likert scales. Your answer has no intrinsic numerical value but it can be ranked against the answers of others. One respondent can be said to be more satisfied than another but not by how much. Moreover, one can only trust the results insofar as respondents have the same understanding or frame of reference--the service that satisfied one respondent may not have satisfied another. Other ordinal variables, such as education degree and income level do not have this issue.

\hypertarget{quantitative-variables}{%
\subsection{Quantitative variables}\label{quantitative-variables}}

Quantitative variables can be further differentiated into two types: discrete and continuous.

\begin{itemize}
\tightlist
\item
  \textbf{Discrete} variables take on countable or indivisible values.
\item
  \textbf{Continuous} variables take on infinitely divisible values (at least in theory).
\end{itemize}

The distinction between discrete and continuous can be more difficult to discern but also less consequential for analysis. It is often the case that analytical models treat the two variables the same. However, for a purpose such as data visualization, the distinction can be informative.

Any variable that is a count of persons, places, events, or things is a discrete variable, usually taking on integer values (e.g.~0, 1, 2, 3,\ldots). By contrast, a continuous variable can contain values with an infinite number of decimal places. Even so, continuous variables take on a limited number of decimal places because either we measure phenomena with finite precision or it simply becomes impractical to include so many decimal places.

\hypertarget{index-variables}{%
\subsection{Index variables}\label{index-variables}}

Index variables are continuous variables but warrant separate discussion. An index variable is a composite measure of multiple variables. They can be used to make a continuous variable out of multiple categorical variables or simplify multiple quantitative variables into one. Purposes such as ranking colleges, measuring poverty beyond income, and determining political ideology make use of index variables.

Index variables mask underlying information. This can be helpful or harmful. In either case, it is important to consider how an index variable is constructed. Doing so can offer insight or uncover problems.

An instructive example familiar to readers is college rankings. U.S. News and World Report \href{https://www.usnews.com/education/best-colleges/articles/ranking-criteria-and-weights}{describes} how rankings are determined.

What makes a college good? According to these rankings, five percent of what makes a college good is the percent of undergraduate alumni giving a donation as a proxy of student satisfaction. Another 20\% is based on the opinions of administrators at peer institutions.

Are these choices wise? This is difficult to say and besides the point. The point is that index variables involve choices made by people and are not naturally occurring data. They are synthetic materials of knowledge and worthy of our discernment.

\hypertarget{dataset-structures}{%
\section{Dataset structures}\label{dataset-structures}}

Just as the type of variable one is dealing with impacts the kinds of visualizations or analyses one should use, so too does the structure of a dataset. Datasets come in three varieties depending on their unit of analysis.

\begin{itemize}
\tightlist
\item
  Cross-sectional

  \begin{itemize}
  \tightlist
  \item
    Pooled cross-sectional
  \end{itemize}
\item
  Time series
\item
  Panel or longitudinal
\end{itemize}

\textbf{Cross-sectional} data is a snapshot in time measuring some size sample of units. One column serves as the identifier of the unit of analysis, such as the name or ID number of the unit. Notice in Table 2.2 that all one needs to know is the country in order to identify a specific row.

\label{tab:cross-sec-fig}Cross-section example

country

continent

year

lifeExp

pop

gdpPercap

Argentina

Americas

2007

75.320

40301927

12779.380

Bolivia

Americas

2007

65.554

9119152

3822.137

Brazil

Americas

2007

72.390

190010647

9065.801

\textbf{Pooled cross-sectional} data could be considered a fourth structure but is simply multiple cross-sections stacked atop each other. The critical quality of pooled cross-sectional data is that each cross-section contains \emph{different} units measured at different times, not the same units measured at different times. Notice in Table 2.3 that the countries included from 2002 are not the same as those included from 2007.

\label{tab:pooled}Pooled cross-section example

country

continent

year

lifeExp

pop

gdpPercap

Algeria

Africa

2002

70.994

31287142

5288.040

Angola

Africa

2002

41.003

10866106

2773.287

Benin

Africa

2002

54.406

7026113

1372.878

Botswana

Africa

2002

46.634

1630347

11003.605

Argentina

Americas

2007

75.320

40301927

12779.380

Bolivia

Americas

2007

65.554

9119152

3822.137

Brazil

Americas

2007

72.390

190010647

9065.801

\textbf{Time series} data measures one unit over multiple time periods. The unit of analysis in time series data is time, as it uniquely identifies each row. Notice in Table 2.4 that one country is tracked over multiple years.

\label{tab:timeseries}Time series example

country

continent

year

lifeExp

pop

gdpPercap

Argentina

Americas

1977

68.481

26983828

10079.027

Argentina

Americas

1982

69.942

29341374

8997.897

Argentina

Americas

1987

70.774

31620918

9139.671

Argentina

Americas

1992

71.868

33958947

9308.419

Argentina

Americas

1997

73.275

36203463

10967.282

Argentina

Americas

2002

74.340

38331121

8797.641

Argentina

Americas

2007

75.320

40301927

12779.380

\textbf{Panel} (or longitudinal) data measures the same units over multiple time periods. The unit of analysis is pair of unit and time period. Notice in Table 2.5 that in order to identify a specific row, you would need to know the country \emph{and} year. One could also think of panel data as numerous time series.

\label{tab:panel}Panel example

country

continent

year

lifeExp

pop

gdpPercap

Argentina

Americas

1997

73.275

36203463

10967.282

Argentina

Americas

2002

74.340

38331121

8797.641

Argentina

Americas

2007

75.320

40301927

12779.380

Bolivia

Americas

1997

62.050

7693188

3326.143

Bolivia

Americas

2002

63.883

8445134

3413.263

Bolivia

Americas

2007

65.554

9119152

3822.137

\begin{quote}
\textbf{To learn how to examine data in R, proceed to Chapter \ref{r-data}.}
\end{quote}

\hypertarget{kt1}{%
\section{Key terms and concepts}\label{kt1}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Unit of analysis
\item
  Variable
\item
  Types of variables: qualitative, quantitative, nominal, ordinal,
  discrete, continuous, index
\item
  Data structures: cross-sectional, pooled cross-sectional, time series,
  panel
\end{itemize}
\end{learncheck}

\hypertarget{measurement-and-missing}{%
\chapter{Measurement and Missing}\label{measurement-and-missing}}

\begin{quote}
\emph{"Will he not fancy that the shadows which he formerly saw are truer than the objects which are now shown to him?}

---Plato
\end{quote}

Once we know the types and structure of our data, we need to understand how the data relates to reality before drawing conclusions from it. Variables and the data they contain are measured representations of reality. They are shadows on the allegorical cave discussed in Plato's \emph{Republic}. We should not immediately assume these shadows are good representations of reality.

\hypertarget{lo3}{%
\section{Learning objectives}\label{lo3}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Assess the measurement validity of variables
\item
  Assess the measurement reliability of variables
\item
  Explain the difference between accuracy and precision
\end{itemize}
\end{learncheck}

\hypertarget{credible-analysis}{%
\section{Credible analysis}\label{credible-analysis}}

Measurement validity and reliability are the foundations of credible analysis, the components of which are depicted in Figure 3.1. Without the two, we have little or no basis to make conclusions from data.

\begin{figure}
\includegraphics[width=13.58in]{images/credible} \caption{Components of credible analysis}\label{fig:credfig}
\end{figure}

This book will address the remaining building blocks in subsequent chapters.

\hypertarget{measurement-validity-reliability}{%
\subsection{Measurement validity \& reliability}\label{measurement-validity-reliability}}

\begin{announcement}
The following discussion of measurement validity and reliability
pertains mostly to data we did not generate ourselves via a survey or
other instrument, but rather data generated by someone else that we may
want to use for our purposes (i.e.~administrative data). When we set out
to measure a concept ourselves, it affords us the chance to consider
additional aspects of validity and reliability that are not covered
here.
\end{announcement}

\textbf{Measurement Validity}: Does the variable accurately represent what it claims to represent? Are the values accurate representations of the intended concept/phenomenon?

\textbf{Measurement Reliability}: Does the way the variable is measured generate the same value given the same reality? Given two real and identical conditions, will my data contain identical values?

One way to visually represent the concepts of measurement validity and reliability is with the concentric circles of a target or a dart board. At the center of the target is the true concept of interest represented by a variable.

At the center of a yes/no variable such as poverty is perhaps economic stress or eligibility for means tested government welfare programs. The proximity of a variable's measurement to the center concept is the variable's measurement validity.

In addition to its proximity to the center of the target, there is also the issue of whether, given the same condition, repeated measures measures will result in the same value. If so, then the data points on the target should be clustered in close proximity.

If two cars are speeding in different towns at 80 mph, should we think a dataset recording instances of speeding would report these two cars differently? If not, then we believe the variable to have measurement reliability. If we think the two towns procedures or equipment result in different speeds for two cars traveling at the same speed, then we believe the variable to be unreliable.

The combination of validity and reliability presents four scenarios depicted in Figure 3.2 below.

\begin{figure}
\includegraphics[width=12.01in]{images/measure_targets} \caption{Representation of measurement validity and reliability}\label{fig:unnamed-chunk-6}
\end{figure}

Let us consider an example for each of the four combinations of measurement validity and reliability.

First, consider a before-tax earnings as a measure of labor income. Provided payments are not made under the table, this measure should be valid, as its purpose is to quantify labor income for official government use. As well, the collection of earnings data is reliable; two individuals with equal labor income can be expected to have equal before-tax income reported on their W-2.

What if we used self-reported income as a measure of labor income? Self-reported income is unlikely to be a valid measure because a non-trivial number of respondents may be inclined to provide inflated answers. Nor is there reason to expect exaggerations to follow a mathematical formula that results in equally invalid response given equal true income. Therefore, self-reported income is also an unreliable measure of income.

How about before-tax earnings as a measure of total wealth? This measure is likely to be invalid because wealth includes assets like property and investments. Gross earnings is arguably a reliable measure because it at least captures income in a reliable fashion, and two individuals with equal income might be expected to have somewhat similar levels of wealth, though it is possible for those with high wealth to strategically lower their income.

Lastly, what if we used income as a percentage of the federal poverty line (FPL) as a measure of poverty? Depending on how we define poverty, income relative to FPL may or may not be a valid measure. Let us suppose households earning 100\% of the FPL are highly likely to be impoverished and in need of assistance. In this case, we have a valid measure. However, two households of equal impoverishment are unlikely to have equal income relative to the FPL. Costs of living differ across regions, jobs provide various levels of health coverage, and households have various needs with respect to medicine or nutrition. Therefore, this may not be a reliable measure. Key to this measure remaining valid is that the lack of reliability does not result in systematic over- or under-reporting of poverty.

\hypertarget{why-this-matters}{%
\subsubsection{Why this matters}\label{why-this-matters}}

Let us consider another example where an agency needs to allocate resources to state governments according to the number of persons who are homeless in each state. Suppose the \emph{true} count of homeless persons for two states is the same. The four combinations of measurement validity and reliability are depicted in Figure 3.3.

\begin{figure}
\includegraphics[width=12.76in]{images/measure_lines} \caption{Comparing measurement validity and reliability}\label{fig:unnamed-chunk-7}
\end{figure}

Though techniques to count the number of homeless persons are improving, one way to count has been to designate a specific day of the year (January 1st) where staff and volunteers conduct a census of homeless people. As a measure, this is known to be invalid and unreliable.

In the case where a valid and reliable measure is taken, the two states receive equal and appropriate amounts of resources. If an invalid and reliable measure is used, the two states receive equal resources, but the amount of resources is less (or more) than what it should be. If a valid and unreliable measure is used, on average, the amount of resources provided is appropriate, but the two states receive different amounts. If an invalid and unreliable measure is used, the two states receive different amounts and the amount of resources provided is systematically less (or more) than what it should be.

The moral of this story is that when you collect data you did not generate yourself for your own purpose, take the time to consider if those data are valid and reliable measures for your intended purpose and what the consequences could be if they are not.

\hypertarget{missing-data}{%
\section{Missing data}\label{missing-data}}

It is not uncommon to encounter missing values in a data. Respondents skip or choose not to answer survey questions, administrators fail to contact respondents, entities that reported data last year may have dissolved or consolidated with another entity this year. Many reasons can lead to missing data. The key is to consider why data are missing and if it should affect your conclusions.

Using the previous example of self-reported income, suppose there are numerous missing values in the responses. Should we assume they are missing at random, or that there is some underlying reason or pattern? Perhaps those with no or low income do not wish to report. If we were to dismiss these missing values, and draw conclusions from the non-missing values, we may severely overestimate the income of the target population.

\hypertarget{types-of-missing-data}{%
\subsection{Types of missing data}\label{types-of-missing-data}}

Missing data come in two flavors:

\begin{itemize}
\tightlist
\item
  \textbf{Explicit:} data that we can see are missing in the data; cells containing a value that denotes missing
\item
  \textbf{Implicit:} data that we would expect to be included based on data structure but are not; no obvious sign of missing
\end{itemize}

Table 3.1 shows an example of data that are explicitly missing denoted by \texttt{NA}. Missing data is denoted in a variety of ways. For example, instead of \texttt{NA}, the cells could have been left empty, or filled with a period, or some other symbol. If data were obtained from an organization that regularly produces publicly available data, datasets are usually accompanied by a legend that explains what symbols denote missing.

\label{tab:unnamed-chunk-8}Example of explicitly missing data

country

continent

year

lifeExp

pop

gdpPercap

Argentina

Americas

2007

NA

40301927

12779.380

Bolivia

Americas

2007

65.554

9119152

NA

Brazil

Americas

2007

72.390

190010647

9065.801

\begin{announcement}
Beware ambiguous missing values. For instance, some survey questions are
dependent on previous questions. You do not want to conclude that a
value is missing because a respondent chose not to answer when they were
never asked the question. Or perhaps a value is missing when it should
actually equal 0 or vice versa. If missing data are consequential to
your analysis, then you may need to investigate further into how the
data were collected or coded in order to eliminate such ambiguity.
\end{announcement}

Table 3.2 shows an example of implicitly missing data. Argentina is observed in 1997, 2002, and 2007, but Bolivia is observed only in 1997 and 2007. What happened to the 2002 observation for Bolivia? This sort of entry and exit from the dataset is common in panel data where the same units are observed over multiple time periods.

\label{tab:unnamed-chunk-10}Example of implicitly missing data

country

continent

year

lifeExp

pop

gdpPercap

Argentina

Americas

1997

73.275

36203463

10967.282

Argentina

Americas

2002

74.340

38331121

8797.641

Argentina

Americas

2007

75.320

40301927

12779.380

Bolivia

Americas

1997

62.050

7693188

3326.143

Bolivia

Americas

2007

65.554

9119152

3822.137

Note that the missing Bolivia observation was easy to spot because the dataset is extremely small. If we were dealing with a large dataset, this would not have been so obvious. A quick way to check whether there may be implicitly missing observations is to check the number of observations in your data. If you are under the impression that your data contains all 50 states for 10 years, then you should have 500 observations. If not, some states or years must be missing.

\begin{quote}
\textbf{To learn how to work with missing data in R, proceed to Chapter \ref{r-missing-data}.}
\end{quote}

\hypertarget{kt2}{%
\section{Key terms and concepts}\label{kt2}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Measurement validity
\item
  Measurement reliability
\item
  Measurement precision
\item
  Implicitly missing data
\item
  Explicitly missing data
\end{itemize}
\end{learncheck}

\hypertarget{descriptive-statistics}{%
\chapter{Descriptive Statistics}\label{descriptive-statistics}}

\begin{quote}
\emph{"Just the facts, ma'am.}

---Joe Friday, Dragnet
\end{quote}

\hypertarget{lo4}{%
\section{Learning objectives}\label{lo4}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Explain how descriptive and inferential statistics differ in purpose
  and the information they provide
\item
  Identify and differentiate population, sample, parameter, statistic in
  a given research proposal or question
\item
  Explain what is a distribution of a random variable
\item
  Recall the descriptive measures of center, spread, and association
\item
  Choose the preferable measures of center and spread given a
  distribution and explain their strengths and weaknesses
\item
  Determine the direction and strength of association given a
  scatterplot or correlation coefficient
\item
  Explain the possible shortcomings of correlation
\end{itemize}
\end{learncheck}

\hypertarget{two-kinds-of-statistics}{%
\section{Two kinds of statistics}\label{two-kinds-of-statistics}}

There are two kinds of statistics, each having a specific goal:

\begin{itemize}
\item
  \textbf{Descriptive statistics} summarizes the qualities of observed data, typically describing distributions of variables or the relationship between two variables.
\item
  \textbf{Inferential statistics} uses observed data in a sample to make inferences/conclusions about an unobserved population.
\end{itemize}

Descriptive statistics concerns just the facts. Inferential statistics uses those facts to make educated, scientific guesses about a group of people, places, or things for which we don't have data.

The above definitions include some terms that warrant further explanation.

\begin{itemize}
\tightlist
\item
  \textbf{Population:} all members of a specified group pertaining to a research question
\item
  \textbf{Sample:} a subset of that population
\end{itemize}

In many cases, we cannot study an entire population because of logistics or cost. Instead, we take a sample of that population to make inferences about it.

Sometimes our population is small or accessible enough to observe. If I wanted to know the average GPA of students in my class, my population is the students in my class, and I could compute the exact average for the entire population. Or if I worked in HR for an agency and wanted to know the racial diversity of a department, I could calculate percentages of each race for the population.

We can describe a sample or a population. Inference is specifically using a sample to describe a population. When we compute measures of a population or sample, these measures have specific names:

\begin{itemize}
\tightlist
\item
  \textbf{Parameter:} a measure pertaining to a population
\item
  \textbf{Statistic:} a measure pertaining to a sample
\end{itemize}

If my population is students in my class, and I compute the average GPA for all of the students in my class, that measure is a population parameter. If my population is all students at a university, and I use the students in my class as a sample, then the average GPA of the students in my class is a sample statistic. In inference, a statistic is often referred to as an \textbf{estimate} because it is used to estimate a population parameter. The parameter in this example would be the average GPA of all students at the university.

\hypertarget{distributions}{%
\section{Distributions}\label{distributions}}

The goal of descriptive statistics is to summarize characteristics of variable distributions. Before reviewing the measures used to summarize distributions, we should understand what a distribution is.

A \textbf{distribution} tells us the (possible) values of a variable and the frequency at which those values occur.

The values of a variable are the result of some random data-generating process. If it wasn't random, and instead deterministic, then there would be no uncertainty in the world. You do not know if you will get a job that requires your degree before you get the degree. An HR department does not know if an implicit bias workshop will reduce the number of racial insensitivity complaints before providing the workshop and measuring the number of complaints, nor does it know how many complaints occur at all before they are made. All of these are variables with some range of possible values, each of which occurs at some frequency. These frequencies are revealed to us when we take measures of the variable.

Sometimes we know all the possible values of a variable, or at least the range of possible values. We know a variable for biological sex has possible values of male or female. We know a variable for GPA has a possible range of 0 to 4, in most cases.

Sometimes we know what the frequency of values for a variable should be. Genetics tells us to expect 50\% males and females. Most of the time we don't know the function that determines frequency, or it is too complex. For example, we have some idea of the factors that influence GPAs, but there will always be some randomness that goes unaccounted.

This somewhat esoteric exposition underlies the main focus here: the distribution of a variable. To make this as concrete as possible, let's consider a variable of something that is simple and familiar to all of us: a roll of a six-sided die.

We know a roll of a six-sided die can take on a range of integers between 1 and 6. We also know the frequency of each value is the same at 1 in 6, or about 17\%. Therefore, we \emph{know} the distribution of this variable, which is depicted below in Figure 4.1.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-14-1.pdf}
\caption{\label{fig:unnamed-chunk-14}Probability distribution of a six-sided die}
\end{figure}

Therefore, if we were to roll the die six times, we would \emph{expect} the following data in Table 4.1.

\label{tab:unnamed-chunk-15}Expected results of 6 rolls

roll

value

1

1

2

2

3

3

4

4

5

5

6

6

And we could represent this distribution by counting the number of times each value occurred using a histogram as shown in Figure 4.2.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-16-1.pdf}
\caption{\label{fig:unnamed-chunk-16}Expected distribution of 6 rolls}
\end{figure}

Of course, this is just what is expected to happen, on average, given many rolls of a die. Anyone who has played a board game knows streaks can occur. Given a number or rolls, we probably will not observe a uniform number of values.

Suppose we roll 12 times and record the value of each roll, as is shown in Table 4.2.

\label{tab:unnamed-chunk-18}Observed results of 12 rolls

roll

value

1

5

2

2

3

4

4

2

5

5

6

1

7

1

8

4

9

3

10

2

11

5

12

1

We can visualize the distribution of these 12 rolls, as is done in Figure 4.3.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-19-1.pdf}
\caption{\label{fig:unnamed-chunk-19}Observed distribution of 12 die rolls}
\end{figure}

Here we can see the randomness of the variable. Values 1, 2, and 5 occur more frequently than 3 and 4, and 6 does not occur at all. If we were to roll the die many more times, it would look more like the distribution we would expect. But for \emph{this} sample of die rolls, the distribution is unique.

This is exactly the point of descriptive statistics: whether or not we know what to expect in terms of a variable's distribution, we want to know the characteristics of the distribution for a variable from a particular sample or population. When we ask for, say, a variable's average, we are asking for the approximate midpoint of that variable's distribution.

Descriptive measures help us summarize characteristics of distributions and some serve as the building blocks for other descriptive measures as well as inferential statistics.

\hypertarget{descriptive-measures}{%
\section{Descriptive Measures}\label{descriptive-measures}}

A die roll is uninteresting and unimportant. In our review of descriptive measures, let us consider them with respect to the distribution of the infant mortality rate across 222 countries in 2012. Infant mortality is the number deaths of infants under one year old per 1,000 live births. It is used as a measure of health in a country.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-20-1.pdf}
\caption{\label{fig:unnamed-chunk-20}Infant Mortality Rates}
\end{figure}

We could simply show this distribution, but it is usually helpful to summarize key characteristics of it, as doing so help answer some specific questions. Distributions are typically described in one of three ways:

\begin{itemize}
\tightlist
\item
  \textbf{Center:} what is the typical value of this variable?
\item
  \textbf{Spread:} how far away are values typically from the center?
\item
  \textbf{Association:} what is the typical value or spread of the distribution given a value of another variable?
\end{itemize}

The three types of descriptive measures above are defined using questions because there are multiple options for each, and which one is more appropriate to use depends on which one answers the question best given the shape of the distribution.

\hypertarget{measures-of-center}{%
\subsection{Measures of center}\label{measures-of-center}}

\hypertarget{mean}{%
\subsubsection*{Mean}\label{mean}}
\addcontentsline{toc}{subsubsection}{Mean}

The mean or average takes the values of a variable, adds them, then divides that sum by the total count of values.

\begin{equation}
{\displaystyle \bar{x}={\frac {1}{n}}\sum _{i=1}^{n}x_{i}={\frac {x_{1}+x_{2}+\cdots +x_{n}}{n}}}
\label{eq:mean}
\end{equation}

Infant mortality has 222 values, so \(n\) in equation \eqref{eq:mean} above would equal 222 in this case. If we were to pluck one country out of our pool of 222 countries at random, the mean tells us the infant mortality rate to expect. In other words, the mean tells us the typical infant mortality rate given our observed data. In this case, the average infant mortality rate is 26.7.

\hypertarget{median}{%
\subsubsection*{Median}\label{median}}
\addcontentsline{toc}{subsubsection}{Median}

If we took the 222 rates and listed them in ascending or descending numerical order, the median is the value that values exactly in the middle of that list. The median is also referred to as the 50th percentile because half of the values fall below it and half of the values fall above it. In the case of an even number of values, there is no naturally occurring middle value. In that case, we take the average of the two values in the middle. The median infant mortality rate is 15.6.

\hypertarget{mode}{%
\subsubsection*{Mode}\label{mode}}
\addcontentsline{toc}{subsubsection}{Mode}

The mode is the value that occurs most frequently. If all values occur only once, then a variable has no mode. If two or more values occur an equal number of times and it is more than other values, then a variable has two or more modes. For instance, the modes for our 12 die rolls from before are 1, 2, and 5. The mode for infant mortality rates is 11.6.

\hypertarget{choosing-a-center}{%
\subsubsection*{Choosing a center}\label{choosing-a-center}}
\addcontentsline{toc}{subsubsection}{Choosing a center}

As Figure 4.5 shows, three measures of center have provided us three different typical infant mortality rates. The mean is represented by the red line, the median by the purple line, and the mode by the green line.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/mortcenters-1.pdf}
\caption{\label{fig:mortcenters}Centers of infant mortality rates}
\end{figure}

Which should we choose to report? We could report all three, but let us apply some professional judgment to this dilemma. After all, people generally do not like nuance and prefer one best number instead of deciphering the meaning from three, assuming they understand all three measures in the first place.

For continuous variables reported at several decimal places, a value may not occur more than once because of the precision. More importantly, because of this low probability of repeat values, the mode is not guaranteed to represent a typical value. If it only takes two occurrences to qualify as the mode, that second occurrence could be an extreme value. Therefore, the mode is more commonly used to report frequencies of categorical or discrete variables for which there are relatively few possible values.

When choosing between mean and median, it usually comes down to the presence of extreme values or the extent to which a distribution is \textbf{skewed}. Skew pertains to the tails of a distribution--the taper to the left and right of its center. If the right or left tail extends far out from the center, we consider the distribution to be right- or left-skewed. Our distribution of infant mortality rates is right-skewed.

When a distribution is skewed, the median is generally a better choice for reporting its center. This is because the mean is sensitive to extreme values. Note in Figure \ref{fig:mortcenters} that the mean is being pulled to the right by the extreme values. As a result, the red line is to the right of the cluster of frequent values and may not be a good answer for the typical value of this distribution. The median is not sensitive to extreme values. No matter how far we stretch the values above the median to the right, the middle of the distribution stays put. If one were to use the median because of skew, one should also mention that in their report.

\hypertarget{measures-of-spread}{%
\subsection{Measures of spread}\label{measures-of-spread}}

Reporting the center of a distribution does not tell us how tightly values are grouped around the mean. Put differently, if the center is a good guess of the value for a unit drawn randomly from our data, the measure of spread is a good guess of how far off that guess will be from the random draw.

\hypertarget{variance}{%
\subsubsection*{Variance}\label{variance}}
\addcontentsline{toc}{subsubsection}{Variance}

Almost all values of our variable will not exactly equal the mean. This is referred to as \textbf{deviation from the mean}. The variance squares each observation's deviation from the mean, sums all the deviations, and divides this sum by the total count of observations minus one. Equation \eqref{eq:variance} displays this process mathematically.

\begin{equation}
{\displaystyle S^2={\frac {1}{n-1}}\sum _{i=1}^{n}(x_{i}-\bar{x})^2={\frac {(x_{1}-\bar{x})^2+(x_{2}-\bar{x})^2+\cdots +(x_{n}-\bar{x})^2}{n-1}}}
\label{eq:variance}
\end{equation}

The mean infant mortality rate is 26.7. If we subtract this mean from each country's rate, we have each country's deviation from the mean, some of which is shown in Table \ref{tab:varcalcs}. Then, we square these deviations as is also shown in the table. We then sum the 222 squared deviations and divide by 221. The variance for our infant mortality rates is 672.6.

\label{tab:varcalcs}Excerpt of variance calculations

country

inf\_mort\_rate

deviate

sq\_deviate

Afghanistan

121.63

94.93

9011.705

Niger

109.98

83.28

6935.558

Mali

109.08

82.38

6786.464

Somalia

103.72

77.02

5932.080

Central African Republic

97.17

70.47

4966.021

\hypertarget{standard-deviation}{%
\subsubsection*{Standard deviation}\label{standard-deviation}}
\addcontentsline{toc}{subsubsection}{Standard deviation}

Variance is an important building block for inference, but it is virtually useless as a descriptive measure because it is in squared units. If someone asks how far values are spread out from the mean, it would not help much to report values deviate from the mean by 672 squared-deaths.

The standard deviation is simply the square root of variance to return our units to their original meaning.

\begin{equation}
{\displaystyle s = \sqrt{S^2}}
\label{eq:sd}
\end{equation}

The standard deviation of our infant mortality rates data is 25.9. This tells us that, on average, infant mortality rates are about 26 deaths above or below the mean.

\hypertarget{interquartile-range}{%
\subsubsection*{Interquartile range}\label{interquartile-range}}
\addcontentsline{toc}{subsubsection}{Interquartile range}

Recall that the median is the 50th percentile of a distribution--half of the values fall below the median and half fall above it. The interquartile range (IQR) is equal to the 75th percentile minus the 25th percentile, thus providing the range that captures the middle 50\% of the values in the distribution. The IQR is the spread analog of the median. The IQR for our distribution is 35.6.

\hypertarget{range}{%
\subsubsection*{Range}\label{range}}
\addcontentsline{toc}{subsubsection}{Range}

The range is simply the maximum value in a distribution minus the minimum value of a distribution. Usually, the range is left implied in a table of summary statistics by reporting the maximum and minimum without differencing the two. The range of our distribution is 119.8.

\hypertarget{choosing-a-spread}{%
\subsubsection*{Choosing a spread}\label{choosing-a-spread}}
\addcontentsline{toc}{subsubsection}{Choosing a spread}

The same logic applies to choosing a measure of spread as choosing a measure of center. The standard deviation is based on the mean, and so it is also sensitive to extreme values that, if present, could exaggerate the typical spread of the distribution. The IQR is based on percentiles just like the median. Therefore, IQR is not sensitive to extreme values.

Figure \ref{fig:mortspread} displays the mean and plus-and-minus one standard deviation using red dashed and solid lines, respectively. The median and the IQR (25th and 75th percentiles) are represented by the purple dashed and solid lines, respectively. Note how wide the area contained by the standard deviation is--it contains most of the distribution. It is arguably not ideal for conveying the typical deviation from the center, as it contains plenty of values that are rather atypical deviations from the center. In the case of describing the distribution of infant mortality rates, the median and IQR are likely a better choice.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/mortspread-1.pdf}
\caption{\label{fig:mortspread}Center and spread of infant mortality rates}
\end{figure}

In most cases, the range (or minimum and maximum values) should be reported along with either the mean and standard deviation or median and IQR (or 25th and 75th percentiles), especially when a distribution is skewed. In addition to signaling the skew of a distribution, the range helps convey what may be the possible values of a variable and how different the most different units in the data are with respect to that variable.

In the case of infant mortality rates, we know the minimum possible value is 0 by definition, but the minimum value in our distribution is 1.8. Perhaps 0 deaths is unrealistic for any country. Moreover, the maximum value is 121.63. This range, along with the median and IQR, tells us the most different countries are \emph{very} different.

\hypertarget{the-normal-distribution}{%
\subsection{The normal distribution}\label{the-normal-distribution}}

As a brief aside, it should be mentioned that if a distribution is \textbf{normal}, then measures of center and spread will be similar to each other. This is one of several desirable features of the normal distribution.

Figure \ref{fig:normrates} shows a simulated scenario in which the infant mortality rates in our 222 countries exhibit a normal distribution. Note the peaks in the center and symmetry. There is no skew.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/normrates-1.pdf}
\caption{\label{fig:normrates}Simulated normal distribution of infant mortality rates}
\end{figure}

Table \ref{tab:normratesum} confirms the similarity between measures of center and spread for this simulated distribution. This is one reason it is important to visualize your distribution. If it appears approximately normal, then you should report the mean and standard deviation (along with minimum and maximum values), as the are more widely understood.

\label{tab:normratesum}Center and spread measures of simulated data

Mean

Median

Mode

SD

IQR

26

25.5

22.1

6.6

8.4

Again, the normal distribution has several desirable features that will be discussed further in the chapters pertaining to inference. However, you now know one desirable feature. If a distribution is approximately normal, then extreme values are not a concern and the mean and standard deviation are good measures of center and spread, respectively. Besides making our choice of measures convenient, why is this worth repeating? Because the mean and standard deviation are building blocks to the next category of descriptive measures: association.

\hypertarget{measures-of-association}{%
\subsection{Measures of association}\label{measures-of-association}}

With association, we now consider the distributions of two variables at a time. That is, given the value within one variable's distribution, what does the distribution of another variable look like?

We need a second variable to continue our example involving infant mortality rates. Table \ref{tab:gapdeathtab} shows a preview of a dataset that adds two more variables to our previous infant mortality data.

\label{tab:gapdeathtab}First five rows of country data

country

inf\_mort\_rate

lifeExp

gdpPercap

Afghanistan

121.63

43.828

974.5803

Niger

109.98

56.867

619.6769

Mali

109.08

54.467

1042.5816

Somalia

103.72

48.159

926.1411

Central African Republic

97.17

44.741

706.0165

Recalling that the mean infant mortality rate is about 26, the five countries included are in the right tail of the distribution. Also, you probably know enough about life expectancy to know that the values for these countries are quite low. Perhaps these two variables share a relationship?

In fact, we know they do. Life expectancy in a given year is the average age at which people in that country died. If a country has a high frequency of infants dying, then that will pull the mean downward. A common misunderstanding of life expectancy is that people in that country tend to die at the age of the country's life expectancy. This is certainly not the case if a country has a high infant mortality rate. While adults in countries with low life expectancy may die somewhat younger (or much younger if it is a war-torn country), adults tend to live longer than the average life expectancy. The key is making it out of infancy alive.

\hypertarget{visual-association}{%
\subsubsection*{Visual association}\label{visual-association}}
\addcontentsline{toc}{subsubsection}{Visual association}

As was the case with one variable, we want to visualize the distributions of two variables. When working with two continuous variables, the \textbf{scatter plot} is the most common choice to visualize association between two variables. Figure \ref{fig:scatterlifedeath} plots the paired values of infant mortality rate and life expectancy for each country.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/scatterlifedeath-1.pdf}
\caption{\label{fig:scatterlifedeath}Visualizing association between two continuous variables}
\end{figure}

Note that I plotted infant mortality rate along the x axis and life expectancy on the y axis. This choice was deliberate. If we suspect that one variable influences or affects the value of another variable, then the variable doing the influencing should be plotted on the x axis. Plotting a variable on the y axis implies to the viewer that it responds to the variable on the x axis.

Figure \ref{fig:scatterlifedeath} confirms our suspicion that the two variables are associated. There appears to be a rather strong association such that as infant mortality rate increases, the lower a country's life expectancy.

\hypertarget{quantified-association}{%
\subsubsection*{Quantified association}\label{quantified-association}}
\addcontentsline{toc}{subsubsection}{Quantified association}

As was the case with one variable, we want to describe the association between two variables using quantitative measures. The association between two or more variables can be described in terms of

\begin{itemize}
\tightlist
\item
  \textbf{Direction:} when one variable increases, does the other variable increase or decrease?
\item
  \textbf{Strength:} how much do the variables seem to be tied together?
\item
  \textbf{Magnitude:} given an specific increase or decrease in one variable, by how much does the other variable increase or decrease?
\end{itemize}

Again, we have several options to answer the above question.

\begin{itemize}
\tightlist
\item
  \textbf{Covariance:} measures direction of association between between two variables
\item
  \textbf{Correlation:} measures direction and strength of association between two variables
\item
  \textbf{Regression coefficient:} measures the direction and magnitude of association between an explanatory variable and an outcome variable
\item
  \textbf{Coefficient of determination (\(R^2\)):} measures the strength of association between a set of one or more explanatory variables and an outcome variable
\end{itemize}

The regression coefficient and coefficient of determination are discussed in Chapter \ref{r-simple-and-multiple-regression} involving regression models. Let us briefly consider covariance and correlation.

\hypertarget{covariance}{%
\subsubsection*{Covariance}\label{covariance}}
\addcontentsline{toc}{subsubsection}{Covariance}

Covariance tells us when our x variable is above (below) its mean, whether our y variable tends to be above or below its mean. If our y variable tends to be above its mean when x is above its mean, then the two have a positive covariance and are positively associated. If our y variable tends to be below its mean when x is above its mean, then the two have a negative covariance and are negatively associated. A covariance of 0 indicates no association.

Figure \ref{fig:scattercovar} adds references lines for the mean of each variable. Note that when infant mortality is above its mean (to the right of the red line), life expectancy is below its mean (below the purple line) in almost all cases. When infant mortality is below its mean, life expectancy is above its mean in almost all cases. Therefore, these two variables have a negative covariance and are negatively associated. In fact, the covariance between infant mortality rate and life expectancy is -312.7

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/scattercovar-1.pdf}
\caption{\label{fig:scattercovar}Visualizing covariance}
\end{figure}

Covariance is the association analog of variance. It is an important building block of other measures of association, but it is virtually useless for description because it only tells us direction. Correlation tells us direction and strength. Therefore, covariance is never used for description because correlation tells us twice as much information.

\hypertarget{correlation}{%
\subsubsection*{Correlation}\label{correlation}}
\addcontentsline{toc}{subsubsection}{Correlation}

Correlation tells us how much the paired values of two variables exhibit a straight line and whether that straight line is sloped positively or negatively. Correlation ranges between -1 and 1. If it is negative, then the two variables are negatively associated. If it is positive, the two variables are positively associated. The closer correlation is to -1 or 1, the more the two variables exhibit a straight line. A correlation equal to -1 or 1 indicates the two variables form a perfect straight line. A correlation of 0 indicates no association.

Based on the covariance and the scatter plot, we know to expect a negative correlation between infant mortality rate and life expectancy. We also know the correlation will not be -1 because the points do not form a perfect straight line. Nevertheless they do form a fairly tight negative path, so we should expect a correlation closer to -1 than 0. It turns out that the correlation is equal to -0.9. Infant mortality rate and life expectancy exhibit a very strong, negative association.

Another way to think about correlation is if we tried to draw a line through the data points on our scatter plot from left to right that could freely curve about however the data are scattered, would that line be a straight line and would it slope upward or downward? Figure \ref{fig:scattercorr} does exactly that with our data. Note that the data lead the line to be essentially straight until the end when it appears the association turns positively.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/scattercorr-1.pdf}
\caption{\label{fig:scattercorr}Drawing a free line through the data}
\end{figure}

Correlation has three qualities that can lead to misunderstandings or mistakes, some of which may have become apparent to you in the above discussion. First, correlation is sensitive to extreme values. A few dots on a scatter plot can have a substantial impact on the line that is drawn through the data. Second, correlation measures only the \textbf{linear} association, thus the repeated mentioning of \textbf{straight} lines. If two variables formed a perfect U-shape, they are almost certainly strongly associated. However, their correlation would suggest a weaker relationship because a straight line does not fit a U-shape well. Third, \textbf{correlation is a necessary but not sufficient condition of causation}. In order to validly claim that a change in the value of one variable \emph{causes} the values of another variable to change, they must be correlated, but a few more conditions must also be met. Those conditions are discussed in Chapter \ref{causation-and-bias}.

\begin{quote}
\textbf{To learn how to produce a summary table for a publication, proceed to Chapter \ref{r-description}.}
\end{quote}

\hypertarget{kt4}{%
\section{Key terms and concepts}\label{kt4}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Descriptive statistics
\item
  Inferential statistics
\item
  Population
\item
  Sample
\item
  Parameter
\item
  Statistic
\item
  Estimate
\item
  Distribution
\item
  Mean
\item
  Median
\item
  Mode
\item
  Skewed distribution
\item
  Standard deviation
\item
  Interquartile range
\item
  Range
\item
  Correlation
\end{itemize}
\end{learncheck}

\hypertarget{data-visualization}{%
\chapter{Data Visualization}\label{data-visualization}}

\begin{quote}
\emph{``The pen is mightier than the sword, especially if it draws a graph.''}
\end{quote}

\hypertarget{lo5}{%
\section{Learning objectives}\label{lo5}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Interpret the six visualizations included in this chapter
\item
  Recommend an appropriate type of visualization given the intended
  message and data
\end{itemize}
\end{learncheck}

\hypertarget{so-many-choices}{%
\section{So many choices}\label{so-many-choices}}

The world of data visualization is incredibly diverse and detailed. You could spend a substantial amount of time learning how to construct the best visualization given particular data and the the intended message. Such depth is far beyond the scope of this book. For more coverage on data visualization, I recommend the following resources:

\begin{itemize}
\tightlist
\item
  \href{https://flowingdata.com}{Flowing Data}
\item
  \href{https://socviz.co/index.html\#preface}{Data Viz by Kieran Healy}
\end{itemize}

At a basic level, most choices of visualization can be determined based on:

\begin{itemize}
\tightlist
\item
  The kind of description we want to convey, and
\item
  the kinds of variables we are working with.
\end{itemize}

This decision tree for visualizations is depicted in Figure \ref{fig:vizflow} below.

\begin{figure}
\includegraphics[width=21.61in]{images/vizflow} \caption{Basic data viz decisions}\label{fig:vizflow}
\end{figure}

The above flowchart contains variations on six visualizations that cover most needs:

\begin{itemize}
\tightlist
\item
  Histogram or density plot
\item
  Box plot
\item
  Pie chart
\item
  Bar chart or dot plot
\item
  Line graph
\item
  Scatter plot
\end{itemize}

Sections that follow briefly explain each of these six visualizations.

\hypertarget{distribution}{%
\section{Distribution}\label{distribution}}

\hypertarget{histogram}{%
\subsection{Histogram}\label{histogram}}

You have already seen several histograms. A histogram visualizes the distribution of a single variable by counting the number of occurrences for values that fall within a certain range. The frequency of occurrences within each range is represented by a vertical rectangle.

Figure \ref{fig:gradmedhist} shows the median earnings of those employed full time for different graduate degree majors. We can see that most graduate degrees result in a median pay for graduates of between 60 and 80 thousand dollars. There are a few graduate majors for which the median pay is above 100 thousand dollars.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gradmedhist-1.pdf}
\caption{\label{fig:gradmedhist}Histogram of full-time median earnings for different graduate school majors}
\end{figure}

These rectangles are called \texttt{bins} and the range each rectangle covers is called a \texttt{binwidth}. We can specify the number of bins and/or the binwidth. If we have more than 150 observations of a continuous variable, we may want to specify as many as 100 bins but should experiment with this number depending on the particular distribution of the variable. If we have less than 30 observations, we should not use a histogram. If we have more than 30 but less than 150 observations, we should experiment with some number of bins between 30 and 100. Regarding binwidth, if our variable is discrete, then our binwidth should equal the natural integer width. For example, if our variable is a count of weeks, then our binwidth should equal 1 so that each bin contains one week.

\hypertarget{density}{%
\subsection{Density}\label{density}}

A density plot is simply a variation of the histogram that requires many observations to work well. We should not use a density plot unless we have at least 1,000 observations. Instead of rectangular representations of the frequency of values, the density plot uses a smooth line, as if one traces a line over the tops of the rectangles of a histogram. One key difference between a density plot and histogram is that the height of a density plot is based on proportions of occurrences for values relative to the total number of observations, whereas a histogram counts the number of occurrences of values.

Figure \ref{fig:gradmeddens} is a density of the same data as in the histogram. These data have only 173 observations. This is why the proportions on the y axis are so small. Nevertheless, the density provides us the shape of the distribution.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gradmeddens-1.pdf}
\caption{\label{fig:gradmeddens}Density of full-time median earnings for different graduate school majors}
\end{figure}

\hypertarget{box-plot}{%
\subsection{Box plot}\label{box-plot}}

A box plot (or box-and-whiskers plot) is similar to the histogram and density plot, but a box plot tries to combine a complete view of a distribution and several visual markers denoting some of the descriptive measures covered in Chapter \ref{descriptive-statistics}. Figure \ref{fig:gradmedbox} shows the median pay data.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gradmedbox-1.pdf}
\caption{\label{fig:gradmedbox}Box plot of full-time median earnings for different graduate school majors}
\end{figure}

The line in the middle of the box denotes the median of the variable's distribution. The top and bottom edges of the box denote the 75th and 25th percentiles, respectively. Therefore, the length of the box denotes the IQR of the variable's distribution. The whiskers of a boxplot extend 1.5 times the length of the box (IQR). This 1.5*IQR is a standard threshold to identify extreme values also known as outliers. If a variable contains values beyond this threshold, a box plot will single them out with dots beyond the end of the whisker.

\hypertarget{compostion-of-a-category}{%
\section{Compostion of a category}\label{compostion-of-a-category}}

Suppose we deemed a graduate degree for which 5\% or more of its graduates are unemployed to be a ``high'' unemployment degree, and those with an unemployment rate less than 5\% as a ``low'' unemployment degree. We have 173 graduate degree majors. Suppose we want to visualize the composition of this categorical unemployment variable.

\hypertarget{pie-charts}{%
\subsection{Pie charts}\label{pie-charts}}

Pie charts are much derided. This derision is due to the fact that pie charts are often misused. Pie charts are acceptable if you want to show the composition of one categorical variable for which there are no more than 3 levels, though preferably no more than 2 levels. We should \emph{never} use pie charts to compare the composition of a categorical variable between two groups or time periods. Figure \ref{fig:emppie} shows the composition of our unemployment variable.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/emppie-1.pdf}
\caption{\label{fig:emppie}Graduate degrees with high/low unemployment}
\end{figure}

\hypertarget{bar-chart}{%
\subsection{Bar chart}\label{bar-chart}}

Bar charts can be used to present the same information as a pie chart. Moreover, bar charts are easier to interpret, can handle any number of levels, can present data as proportions or total counts, can be used to compare across groups or time, and are easier to make. In short, bar charts are better than pie charts, and we should choose bar charts unless someone forces us to use a pie chart for some reason.

The figures below show the three general types of bar charts. Figures \ref{fig:empbar} and \ref{fig:empbar2} show the composition of our unemployment variable in terms of absolute counts. Figure \ref{fig:empbar} is commonly referred to as a \texttt{stacked} bar chart, while Figure \ref{fig:empbar2} is referred to as \texttt{dodged}. Figure \ref{fig:empbar3} shows the composition in terms of proportions. That is, we can see that slightly over 75\% of graduate degrees have low unemployment.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/empbar-1.pdf}
\caption{\label{fig:empbar}Graduate degrees with high/low unemployment}
\end{figure}

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/empbar2-1.pdf}
\caption{\label{fig:empbar2}Graduate degrees with high/low unemployment}
\end{figure}

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/empbar3-1.pdf}
\caption{\label{fig:empbar3}Graduate degrees with high/low unemployment}
\end{figure}

\hypertarget{comparing-between-units}{%
\section{Comparing between units}\label{comparing-between-units}}

\hypertarget{bar-chart-1}{%
\subsection{Bar chart}\label{bar-chart-1}}

If we want to compare one variable across multiple groups or units that are not time, then a bar chart is a good choice.

Suppose we wanted to compare the median pay between two or more graduate degrees.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/spiapay-1.pdf}
\caption{\label{fig:spiapay}Comparison of median pay between degrees in public and international affairs}
\end{figure}

\hypertarget{dot-plot}{%
\subsection{Dot plot}\label{dot-plot}}

Dot plots serve the same purpose as bar charts, but are more appropriate for variables that measure something we would not naturally stack up for counting purposes. That is, money is stackable--we could imagine each bar as a stack of cash. By contrast, unemployment rates are not somethings we would stack on top of each other.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/spiaemp-1.pdf}
\caption{\label{fig:spiaemp}Comparison of unemployment rates between degrees in public and international affairs}
\end{figure}

\hypertarget{line-graph}{%
\subsection{Line graph}\label{line-graph}}

If we want to compare values of a variable across units of time (i.e.~change over time), then a line graph is probably the most common choice, though a bar chart or dot plot work too.

The graduate degree data is cross-sectional, so there is no good way to make a line graph. I trust you have seen one before.

\hypertarget{association}{%
\section{Association}\label{association}}

Associations involve two or more distributions. We can visualize multiple distributions using a scatter plot if both variables are continuous or discrete with many values, or we can use a histogram or box plot if we want to visualize how the distribution of a continuous variable changes for each level of a categorical variable.

\hypertarget{categorical-and-numerical}{%
\subsection{Categorical and numerical}\label{categorical-and-numerical}}

Suppose we wanted to visualize the association between attaining a graduate degree or not and median pay. Whether to attain a graduate degree is a categorical variable with two levels. Therefore, we can use a box plot to visualize the distribution of median pay for employees with undergraduate degrees in the 173 majors in our data and the distribution of median pay for employees with a graduate degree in those same majors. Figure \ref{fig:gradpaydiffbox} below does just that.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gradpaydiffbox-1.pdf}
\caption{\label{fig:gradpaydiffbox}Median pay for undergraduate and graduate degrees of the same group of majors}
\end{figure}

Overlaying histograms for each level could work too as is done in Figure \ref{fig:gradpaydiffhist}.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gradpaydiffhist-1.pdf}
\caption{\label{fig:gradpaydiffhist}Median pay for undergraduate and graduate degrees of the same group of majors}
\end{figure}

\hypertarget{scatter-plot}{%
\subsection{Scatter plot}\label{scatter-plot}}

The most common visualization for associations is the scatter plot, which you saw several times in Chapter \ref{descriptive-statistics}. It is also common to overlay a simple regression line for the two variables, thus providing a reader the full scatter of the two distributions as well as a tracing of how the two variables move in tandem, \emph{on average}.

Suppose we wanted to visualize the relationship between median pay of graduate degrees and the total number of people with that graduate degree. Do more people tend to enroll in the programs that pay the most?

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gradpayenroll-1.pdf}
\caption{\label{fig:gradpayenroll}Graduate degree median pay and total number of people with degree}
\end{figure}

\begin{quote}
\textbf{The logic of visualization choice discussed in this chapter applies regardless of what particular software one uses. To learn how to generate most of these graphs in R, proceed to Chapter \ref{r-visualization}.}
\end{quote}

\hypertarget{kt5}{%
\section{Key terms and concepts}\label{kt5}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Uses of a histogram
\item
  Uses of a box plot
\item
  Uses of a bar chart
\item
  Uses of a scatter plot
\item
  Distribution of a numerical variable
\item
  Comparison of one variable between two or more units of analysis
\item
  Composition of a categorical variable
\end{itemize}
\end{learncheck}

\hypertarget{part-regression-models}{%
\part{Regression Models}\label{part-regression-models}}

\hypertarget{simple-and-multiple-regression}{%
\chapter{Simple and Multiple Regression}\label{simple-and-multiple-regression}}

\begin{quote}
\emph{``You can lead a horse to water but you can't make him enter regional distribution codes in data field 97 to facilitate regression analysis on the back end.''}

---John Cleese
\end{quote}

\hypertarget{lo6}{%
\section{Learning objectives}\label{lo6}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Identify and explain the components of a population or sample
  regression model
\item
  Explain the difference between a deterministic equation of a line and
  a statistical, probabilistic equation of a line
\item
  Given regression results, provide the predicted change in the outcome
  given a change in the explanatory variable(s)
\item
  Given regression results, provide the predicted value of the outcome
  given a value of the explanatory variable(s)
\item
  Explain what the error term in a regression model represents
\item
  Interpret measures of fit in a regression model and explain their
  relative strengths and weaknesses
\end{itemize}
\end{learncheck}

\hypertarget{basic-idea}{%
\section{Basic idea}\label{basic-idea}}

The basic idea of regression is really quite simple. Regression calculates a line through a scatter plot of two variables so that we can summarize how much our variable on the y axis changes given a change in the variable on our x variable. Or, we can use a given value for our x variable to predict a value for our y variable. That's all it is--a line drawn to represent the association between two variables.

We all learned the equation for a line back in middle school, which probably looked something like the following:

\begin{equation}
y = mx + b
\label{eq:line}
\end{equation}

where \(m\) is the slope of the line and \(b\) is the y-intercept. If we know the slope and intercept for a line, then, given a value for \(x\), we can compute \(y\). Given a change in \(x\), we can compute a change in \(y\) by multiplying the change in \(x\) by \(m\).

Consider the following equation for an arbitrary line:

\[y = 5x + 10\]

Here are some questions we can now answer:

\begin{itemize}
\tightlist
\item
  How much does \(y\) change if \(x\) increases by 1? Answer: 5
\item
  How much does \(y\) change if \(x\) increases by 10? Answer: 50
\item
  How much does \(y\) change if \(x\) decreases by 10? Answer: -50
\item
  What does \(y\) equal if \(x\) equals 2? Answer: 20
\item
  What would \(y\) equal if \(x\) were 0? Answer: 10
\end{itemize}

If you understand how to answer the above questions, then you can interpret regression results for any given context because interpreting regression results involves either predicting the \emph{change} in \(y\) given a \emph{change} in \(x\) or predicting the \emph{value} of \(y\) given a \emph{value} of \(x\).

What is different in regression is how the equation of line is presented because there are population and sample versions of the relationship between \(x\) and \(y\). And, unlike a deterministic mathematical equation like the one above, because we generally use regression to measure relationships between social phenomena, there is inherent uncertainty in the line we calculate. This adds some complexity beyond solving a line's equation, but the process of running a regression to estimate the slope and intercept of a line to then predict changes or values of an outcome is fundamentally the same as the simple equation above.

\hypertarget{simple-linear-regression}{%
\section{Simple linear regression}\label{simple-linear-regression}}

Equation \eqref{eq:simreg} presents the population regression model.

\begin{equation}
y=\beta_0+\beta_1x+\epsilon
\label{eq:simreg}
\end{equation}

Only one element differs between Equations \eqref{eq:line} and \eqref{eq:simreg}. That is the symbol at the end, which is the Greek letter epsilon and is used to denote the aforementioned uncertainty of predicting real-world, particularly social, phenomena (more on that later).

The y-intercept denoted as \(b\) in Equation \eqref{eq:line} has been moved to the front of the right-hand side in Equation \eqref{eq:simreg} and is denoted by \(\beta_0\) (pronounced beta-naught). The slope denoted as \(m\) in Equation \eqref{eq:line} is now denoted as \(\beta_1\) in Equation \eqref{eq:simreg}. These beta, \(\beta\), symbols are simply the standard notation for \textbf{population parameters} in a statistical model and are used to signal that we intend to estimate these parameters using regression.

Recall that a parameter is a statistical measure of a population. In most cases, our research questions concern a population so large or inaccessible such that we do not observe all members. Instead, we take a sample of the population. From this sample, we calculate sample statistics, or \textbf{estimates} of the parameters and use methods of inference to decide if these estimates are valid guesses of the parameter (more on this later).

Equation \eqref{eq:simregsample} presents the sample regression equation.

\begin{equation}
\hat{y}=b_0+b_1x
\label{eq:simregsample}
\end{equation}

The carrot symbol atop our outcome variable \(y\) is called a hat, and so the term on the left-hand side is commonly referred to as ``y-hat.'' This is used to denote the fact that any value we calculate from Equation \eqref{eq:simregsample} is an \emph{estimate} of what has been or will be observed. Similarly, the \(b\) symbols are the sample estimate analogs of the \(\beta\) population parameters in Equation \eqref{eq:simreg}.

Equation \eqref{eq:simregsample} is the equation we use to interpret our regression results in the same way as was demonstrated using the mathematical equation of a line. Again, the only difference is that we are dealing with a statistical or probabilistic equation of a line--the outcome we calculate is a prediction based on observed data.

\hypertarget{using-regression}{%
\subsection{Using regression}\label{using-regression}}

Let's pause the theory to consider a simple example using data for U.S. counties. Table \ref{tab:countydata} provides a preview of the data

\label{tab:countydata}Preview of county data

name

state

fed\_spend

poverty

homeownership

income

Traverse County

Minnesota

20.038786

9.3

80.3

41287

Wabash County

Illinois

7.422533

13.0

80.1

46026

Pike County

Mississippi

9.091897

25.3

72.9

30779

Greenbrier County

West Virginia

9.029030

19.4

75.0

33732

Ray County

Missouri

5.795480

9.4

78.7

53343

Hamilton County

Tennessee

10.188056

14.7

65.5

45408

Ballard County

Kentucky

11.907989

13.0

83.3

41228

where \texttt{fed\_spending} is the amount of federal funds allocated to the county per capita, \texttt{poverty} is the percent of the population in poverty, \texttt{homeownership} is the percent of the population that owns a home, and \texttt{income} is per capita income. There are 3,143 observations in this dataset.

Suppose we wanted to examine the association between federal spending and poverty for U.S. counties such that poverty \emph{explains} federal spending. After all, a substantial portion of federal dollars are dedicated to assist those in poverty. First, we might visualize the relationship between the two variables.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/povfedscatter-1.pdf}
\caption{\label{fig:povfedscatter}Federal spending and poverty among U.S. counties}
\end{figure}

If we were to trace a line through these points, it would clearly slope upward. Thus, this suggests to us that as the percent of the population of a county increases, the amount of federal spending it receives increases. But by how much? That is precisely what regression estimates for us.

Equation \eqref{eq:simregexample} represents the relationship between federal spending and poverty using a simple linear regression population model. Note that we have chosen to model the two variables such that poverty explains or predicts federal spending. This aligns with the choice to plot poverty on the x axis and federal spending on the y axis in Figure \ref{fig:povfedscatter}. This is a critical choice in every regression and one that computers still need humans to help with (more on that later).

\begin{equation}
FedSpend = \beta_0+\beta_1Poverty + \epsilon
\label{eq:simregexample}
\end{equation}

We are going to use observed values of poverty and federal spending to estimate \(\beta_0\) and \(beta_1\). Then, once we have those estimates, we can provide succinct answers regarding how federal spending tends to change given a change in poverty or a predicted level of federal spending given a particular level of poverty in a county.

The \(\epsilon\) represents all the other factors that explain or predict federal spending that are not in our model. If our world were such that the points in \ref{fig:povfedscatter} literally formed a straight line, we would not need an \(\epsilon\), but this is never the case with interesting questions of complex phenomena. This may or may not be a problem for whatever story we intend to tell about the the relationship between poverty and federal spending (more on that later).

Running the regression as represented in Equation \eqref{eq:simregexample} produces Table \ref{tab:simpregextab} of results.

\label{tab:simpregextab}Regression results of poverty on federal spending

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

7.950

0.219

36.294

0

7.520

8.379

poverty

0.108

0.013

8.265

0

0.082

0.134

With the exception of Chapter @\ref(causation), this section on regression models focuses on understanding the methods used to generate the values in the \texttt{estimate} column immediately to the right of the variable names as well as how to interpret and apply these values to any question. The remaining columns in the above table pertain to inference. The values in the \texttt{estimate} column are commonly referred to as \textbf{coefficients}, which were first mentioned in Chapter \ref{descriptive-statistics}. Regression coefficients measure the direction and magnitude of association between an explanatory variable and an outcome variable.

Now that we have our results, we can plug them into our sample regression equation like so

\begin{equation}
\hat{FedSpend}=7.95+0.11 \times Poverty
\label{eq:simregresults}
\end{equation}

and we are back to the first section of this chapter. Given a change in poverty we can predict the change in federal spending. Given any particular poverty level, we can predict a level of federal spending.

As for a standard template to interpret regression results, we generally say or write the following,

\begin{quote}
On average, as the percent of the population in poverty increases by 1 percentage point, federal spending per capita increases approximately 11 cents.
\end{quote}

A couple points about the above template:

\begin{itemize}
\tightlist
\item
  We always qualify using ``on average'' because that is exactly what regression does. Drawing a line through a scatter plot results in points above and below that line. The line drawn by regression traces how \(y\) responds to \(x\) \emph{on average}.
\item
  The standard change in \(x\) to use when reporting results is one unit. Poverty is in units of percent. Therefore, a one-unit change in a variable measured in percentages is one percentage point (e.g.~10\% to 11\%).
\end{itemize}

However, we could report these results using any change or particular value in poverty germane to our original research question. For example, if we expected a county's poverty rate to be 30\%, then we could report a predicted level of federal spending per capita equal to

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{7.95+0.11}\OperatorTok{*}\DecValTok{30}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 11.25
\end{verbatim}

dollars per capita. Similarly, if we expected a decrease in the poverty rate of 5 percentage points, then we could predict a change in the level of federal spending per capita equal to

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.11}\OperatorTok{*-}\DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.55
\end{verbatim}

dollars per capita.

\hypertarget{the-error-term}{%
\subsection{The error term}\label{the-error-term}}

Back to theory. We need to address this \(\epsilon\) that is present in the population regression model but disappears in the sample regression model and results. What gives?

The \(\epsilon\) term is commonly referred to as the \textbf{error term} or, for those who don't like to insinuate some error was made in the regression, \textbf{statistical noise}. I prefer error term if for no other reason than to remind us to consider the myriad of errors we \emph{may} be making in our regression model.

As mentioned, the error term represents the inherent uncertainty of modeling an outcome based on a necessarily finite number of explanatory factors. Other factors affect our outcome. As a matter of principle, this does not prohibit our attempt to estimate the effect of a variable we care about or can alter with policy or programs on the outcome.

Could we account for multiple factors (i.e.~multiple regression)? Absolutely. Can we control for \emph{everything} that affects our outcome? Definitely not if you subscribe to chaos theory or David Hume's thoughts on causality. Even if not so extreme as to say our world is too complex to ever make decisions concerning one variable's effect on another, the plausibility for us to collect data on every relevant factor is highly unlikely.

So, where did the error term go? It never really left; it simply is not used when calculating the predicted outcome based on our regression results. Like our \(\beta\) terms, the error term is a population parameter. However, unlike the \(\beta\)s, we do not have observed data that corresponds to its estimation. In fact, the concept of the error term exists on the basis that we do not observe it. Therefore, it is necessarily excluded when we predict an outcome based on observed data, all the while we are careful to remind readers that the numbers we report are estimates subject to error.

If the error term never left, where is it? Its sample analog exists as the difference between our estimated regression line and the observed data. Figure \ref{fig:povfedscatter2} below is the same as Figure \ref{fig:povfedscatter} except that it displays the line based on our regression results in Table \ref{tab:simpregextab}.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/povfedscatter2-1.pdf}
\caption{\label{fig:povfedscatter2}Federal spending and poverty among U.S. counties}
\end{figure}

Surely, it is apparent that our regression line does not intersect all points perfectly; many points lie above and others below it. Pick any point in Figure \ref{fig:povfedscatter2} and draw a vertical line to the regression line. The length of that line, which in this case is in units of dollars of federal spending per capita, and all other vertical distances between observed points and the regression line, is our sample version of the error term.

Table \ref{tab:simregexresid} quantifies the error for each of a select few observations.

\label{tab:simregexresid}Comparing observed and predicted federal spending

ID

fed\_spend

poverty

fed\_spend\_hat

residual

1961

7.592

16.9

9.773

-2.181

2440

10.188

17.1

9.795

0.393

1738

15.784

8.8

8.899

6.885

2641

23.503

0.0

7.950

15.554

1471

8.152

25.0

10.647

-2.495

1362

7.649

14.0

9.460

-1.811

2792

7.933

13.5

9.406

-1.474

Our regression model uses observed values of poverty and federal spending to estimate the parameters of the regression line, which produced Equation \eqref{eq:simregresults}. We can then plug the observed values of poverty into the equation to compute a predicted level of federal spending--\texttt{fed\_spend\_hat}. But, we know this prediction is not perfect in most cases. The right-most column of Table \ref{tab:simregexresid} shows the difference between \emph{predicted} federal spending and \emph{observed} federal spending. This difference is called the \textbf{residual}.

Using the first row of Table \ref{tab:simregexresid} as an example, our regression predicts a county with 16.9 percent poverty to receive 7.59 dollars per capita in federal spending. However, this county actually received 9.78 dollars per capita. Our regression underestimates federal spending for this county by 2.18 dollars. Thus, the residual for this county is -2.18.

The residual is represented mathematically by Equation \eqref{eq:simregresid}

\begin{equation}
e = y - \hat{y}
\label{eq:simregresid}
\end{equation}

where \(e\) is the sample analog of \(\epsilon\). This is simply the equation behind the process of differencing the observed and predicted values of our outcome just described.

\hypertarget{goodness-of-fit}{%
\subsection{Goodness of fit}\label{goodness-of-fit}}

Armed with an understanding of error and its sample analog, the residual, we can now consider goodness-of-fit. We must accept there will be error in our regression, but that does not mean we do not seek to minimize that error as much as possible.

\hypertarget{assessing-fit}{%
\subsubsection*{Assessing fit}\label{assessing-fit}}
\addcontentsline{toc}{subsubsection}{Assessing fit}

Table \ref{tab:simregexfit} provides a standard set of three goodness-of-fit measures often used to assess regression.

\label{tab:simregexfit}Goodness-of-fit measures

r\_squared

adj\_r\_squared

rmse

0.021

0.021

4.654482

The first column titled \texttt{r\_squared} refers to the measure \(R^2\), also known as the \textbf{coefficient of determination} defined in \ref{descriptive-statistics}. The \(R^2\) measures the strength of association between a set of one or more explanatory variables and an outcome variable. Specifically, it quantifies the percent of total variation in the outcome explained by our regression model. In this case, our regression using poverty explains 2.1\% of the total variation in federal spending.

The column titled \texttt{rmse} refers to \textbf{root mean squared error} (RMSE). The RMSE quantifies the typical deviation of the observed data points from the regression line and is particularly useful when predicting a value for our outcome. For example, if after predicting that a county with 30\% poverty will receive 11.25 dollars of federal spending per capita, someone asks us how far off that prediction is likely to be, the RMSE suggests our prediction will tend to be off by plus or minus 4.65 dollars.

Regression involves choices. We choose which variables to use to explain or predict an outcome and how to model their effect on the outcome. This menu of choices will become increasingly evident as we build our regression toolbox. As we make choices, competing regression models emerge from which we must choose the one we prefer to report for decision-making.

The \(R^2\) and RMSE provide us the basis for choosing our preferred model. In general, \textbf{we prefer the model with a \emph{higher} \(R^2\) and/or a lower \emph{RMSE}}. In virtually all cases, these two measures will agree with each other; the model with the higher \(R^2\) will also have the lower RMSE.

\hypertarget{understanding-fit}{%
\subsubsection*{Understanding fit}\label{understanding-fit}}
\addcontentsline{toc}{subsubsection}{Understanding fit}

\begin{announcement}
The material under this header is nonessential but is included because a
deeper understanding of the mechanics of fit can be helpful.
\end{announcement}

Regression draws the \emph{best} line through a set of data points of two or more variables. The best line in this case is the line with a slope and y-intercept that \textbf{minimizes the sum of squared residual} between the set of data points and said line. The procedure used to achieve such a line is called \textbf{ordinary least squares} (OLS). The type of regression covered in this book is sometimes referred to as OLS regression.

Recall that the variance of a variable is the sum of squared deviations from the mean, as depicted in Equation \eqref{eq:variance}. This should sound familiar. Instead of deviations from the mean, fitting the best line in regression concerns the deviations from the regression line, which by definition are the residuals. As with variance, we square the deviations (i.e.~residuals) for the data used to estimate the regression line, then we add these squared deviations together to obtain the sum of squared residual (SSR). Equation \eqref{eq:ssr} shows this process mathematically.

\begin{equation}
SSR=\sum _{i=1}^{n}(y_{i}-\hat{y})^2= (y_{1}-\hat{y})^2+(y_{2}-\hat{y})^2+\cdots +(y_{n}-\hat{y})^2
\label{eq:ssr}
\end{equation}

SSR quantifies the error in our regression and is what regression minimizes when predicting an outcome given the explanatory variables we have chosen to include.

The SSR also provides us what we need to compute the root mean squared error (RMSE). Recall that in order to compute the variance and standard deviation of a variable in Equations \eqref{eq:variance} and \eqref{eq:sd}, respectively, we divide the sum of squared deviations by the number of observations (or \(n-1\)) then take the square root. The SSR is a sum of squared deviations. The deviations in this case represent error. If we divide SSR by the number of observations, we now have the mean of the sum of squared error. Then, if we take the square root, we have the root mean squared error. Note that this is the same process used to obtain the standard deviation. Thus, the RMSE is the regression version of a standard deviation. Just as the standard deviation tells us the average deviation from the mean, the RMSE tells us the average deviation from the regression line, or the average error in our regression.

We can also quantify the extent to which our regression \emph{explains} the outcome. To do so, we need a benchmark against which to compare the reduction in error achieved by our regression. This benchmark is simply the average value of the outcome. If we had no explanatory variables to predict an outcome, the mean provides the typical value of the outcome. If we had to draw a random observation from a variable's distribution, the mean is our best guess of what that observation's value would be if we have no explanatory variables.

Figure \ref{fig:povfedscatter3} adds a reference line of average federal spending to our scatter plot. Note that because average federal spending is a constant number, it does not change as poverty changes; the red line has no slope. Also, note that the red line does slightly worse fitting the data, particularly toward the left and right extremes of poverty. Compared to the red line representing the mean, the data appear to be more centered around our regression line. As a result, our regression line has less error than the mean.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/povfedscatter3-1.pdf}
\caption{\label{fig:povfedscatter3}Federal spending and poverty among U.S. counties}
\end{figure}

The mean of the outcome is our benchmark for assessing how much the explanatory variables included in our regression model explains the total variation in the outcome. The difference, if any, between the values our regression predicts, \(\hat{y_i}\), and the mean, \(\bar{y}\), serves as the basis for quantifying the extent to which our regression model explains the total variation in the outcome. Just like with the SSR, we square the difference between each predicted value and the mean, then add them together. The result is called the \textbf{sum of squared explained} (SSE) and is represented mathematically in Equation \eqref{eq:sse}.

\begin{equation}
SSE=\sum _{i=1}^{n}(\hat{y}_{i}-\bar{y})^2= (\hat{y}_{1}-\bar{y})^2+(\hat{y}_{2}-\bar{y})^2+\cdots +(\hat{y}_{n}-\bar{y})^2
\label{eq:sse}
\end{equation}

We now have the sum of squared residuals (SSR) and sum of squared explained (SSE). Together, the SSR and SSE represent the \textbf{sum of squared total} (SST) variation in the outcome \(y\).

\begin{equation}
SST = SSR + SSE
\label{eq:sst}
\end{equation}

Recall that the \(R^2\) measures the percent of total variation in the outcome that is explained by our regression. To calculate any percent we take divide a proportion of the whole divided by the whole (e.g.~\(5/10 = 0.5\) or 50\%). Thus, to obtain the percent of variation in the outcome explained by our regression, we divide the SSE by SST.

\begin{equation}
R^2 = {\frac{SSE}{SST}}
\label{eq:r2}
\end{equation}

The better you understand the mechanics of simple linear regression, the easier it will be to understand the next section and subsequent chapters on regression models because they are mere extensions of this basic model.

\hypertarget{multiple-regression}{%
\section{Multiple regression}\label{multiple-regression}}

Of course, we are not limited to using only one variable to explain or predict an outcome. In fact, it is rather uncommon to use only one variable, but simple linear regression is useful for introducing the method of regression. Now, we can consider more realistic modeling method where we use multiple explanatory variables in our regression, which is aptly named multiple regression.

Equation \eqref{eq:multreg} provides the population model for multiple regression

\begin{equation}
y=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_kx_k+\epsilon
\label{eq:multreg}
\end{equation}

The only difference in this equation compared to Equation \eqref{eq:simreg} is the inclusion of multiple explanatory variables. Each explanatory is numbered and has a corresponding parameter \(\beta\) representing the marginal effect it has on the outcome. In theory, we can add however many explanatory variables we deem worth including, represented by the arbitrary \(k\).

Equation \eqref{eq:multregsample} presents the sample equation for multiple regression.

\begin{equation}
\hat{y}=b_0+b_1x_1+b_2x_2+\cdots +b_kx_k
\label{eq:multregsample}
\end{equation}

Again, nothing is different from before except for more explanatory variables and sample estimates of the parameters.

\hypertarget{using-multiple-regression}{%
\subsection{Using multiple regression}\label{using-multiple-regression}}

Let's return to our example of federal spending per capita in U.S. counties. Previously, we used only the percent of the population in poverty to explain or predict federal spending per capita. Let's add the percent of the population that owns a home and per capita income to our model. Thus, our model can be written as such

\begin{equation}
FedSpend = \beta_0 + \beta_1Poverty + \beta_2HomeOwn + \beta_3Income + \epsilon
\label{eq:multregex}
\end{equation}

which generates the following results

\label{tab:multregextab}Multiple regression results

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

23.519

1.333

17.645

0.000

20.905

26.132

poverty

-0.056

0.021

-2.674

0.008

-0.097

-0.015

homeownership

-0.126

0.012

-10.736

0.000

-0.149

-0.103

income

0.000

0.000

-7.723

0.000

0.000

0.000

and the following goodness-of-fit measures

\label{tab:multregexfit}Fit of multiple regression

r\_squared

adj\_r\_squared

rmse

0.064

0.063

4.55216

Now we can discuss what is different with multiple regression. First, note that our coefficient or estimate for poverty has changed from 0.108 to 0.105. A small difference to be sure, but that is specific to the example used; sometimes the estimate can change dramatically. Why the change? Because we are \textbf{controlling for other factors}. A slight amount of the marginal effect we reported poverty had on federal spending in our simple regression model was misattributed from the marginal effects of homeownership and/or income on federal spending.

This is a key feature of multiple regression: it estimates the marginal effect of a variable on an outcome, holding all other explanatory variables equal to their respective means. In other words, if we were omnipotent beings who could take each county in our data and set homeownership and income to the mean of homeownership and income according to the observed data, then pull some lever that makes poverty change and nothing else, the estimate for poverty in our multiple regression reports how much each percentage point in poverty changes federal spending. This is how we isolate the effect of one variable on an outcome despite knowing other variables simultaneously affect our outcome.

The interpretation of multiple regression estimates is essentially the same as simple regression. In our example, we can interpret the homeownership estimate like so:

\begin{quote}
On average, our results indicate that a one percentage point increase in the percent of the population that owns a home is associated with a decrease in federal spending per capita of approximately 9 cents, \textbf{holding other factors constant}.
\end{quote}

The part in bold is to point out the small difference between the two interpretations. Here, we are simply reminding a reader that we have controlled for other factors that presumably we have already explained, and our estimate for poverty accounts for those factors by holding them constant. Other common word choices for this part of the interpretation include ``all else equal'' or its Latin translation ``ceteris paribus.''

Again, we can answer any sort of question relevant to our original research question concerning the predicted change or level of federal spending by plugging in the numbers to our regression equation.

\begin{equation}
\hat{FedSpend} = 13.50 + 0.105\times Poverty - 0.093\times HomeOwn + 0Income
\end{equation}

If we wanted to predict the change in federal spending given an 3 percentage point increase in poverty and a decline in home ownership of 4 percentage points, the our answer would be

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.105}\OperatorTok{*}\DecValTok{3}\OperatorTok{+}\NormalTok{(}\OperatorTok{-}\FloatTok{0.093}\NormalTok{)}\OperatorTok{*}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.687
\end{verbatim}

dollars per capita (on average and all else equal, of course). If we wanted to predict the level of federal spending per capita for a county with 12\% poverty, a 80\% home ownership rate, and \$31,000 income per capita, then we would predict

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{13.50+0.105}\OperatorTok{*}\DecValTok{12}\FloatTok{-0.093}\OperatorTok{*}\DecValTok{80}\OperatorTok{+}\DecValTok{0}\OperatorTok{*}\DecValTok{31000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7.32
\end{verbatim}

dollars per capita.

Not so fast! This example provides a good opportunity to consider another aspect of the units our variables are in. Per capita income is in dollars. This means the estimate for income represents the effect of a \emph{one dollar} change in per capita income on federal spending per capita. That's a very small change that we would expect to have a very small effect on federal spending. This effect is so small that statistical software may round to 0. But what if we changed the units of income to \emph{thousands} of dollars per capita instead of dollars per capita? Then we get the following results.

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

23.519

1.333

17.645

0.000

20.905

26.132

poverty

-0.056

0.021

-2.674

0.008

-0.097

-0.015

homeownership

-0.126

0.012

-10.736

0.000

-0.149

-0.103

income

-0.086

0.011

-7.723

0.000

-0.108

-0.064

Now we see the effect of a \emph{one thousand} dollar change in per capita income on federal spending per capita. Note that the estimates for poverty and homeownership are the same. Therefore, the predicted level of federal spending for our county is actually

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{13.50+0.105}\OperatorTok{*}\DecValTok{12}\FloatTok{-0.093}\OperatorTok{*}\DecValTok{80}\FloatTok{+0.057}\OperatorTok{*}\DecValTok{31}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9.087
\end{verbatim}

\hypertarget{fit-and-adjusted-r-squared}{%
\subsection{Fit and adjusted R squared}\label{fit-and-adjusted-r-squared}}

In addition to doing a better job isolating the marginal effect of one variable on an outcome, including additional explanatory variables can reduce the error in our regression, thus achieve more accurate and/or precise predictions of the outcome.

We can assess this improvement in fit by comparing the results in Table \ref{tab:multregexfit} to those in Table \ref{tab:simregexfit}. We have gone from an RMSE of 4.65 dollars to an RMSE 4.59 dollars. This means our predictions from the multiple regression model tend to be off by 6 cents fewer than the predictions of our simple regression model.

The previous discussion on fit conspicuously skipped over the column titled \texttt{adj\_r\_square} because \textbf{adjusted-\(R^2\)} applies when comparing two or more models with a different number of explanatory variables. One caveat to using \(R^2\) to choose a preferred model is that it mechanically increases as the number of explanatory variables increases whether those additional variables improve the extent to which our regression explains the total variation in the outcome or not. Therefore, it is unfair to compare a model with one explanatory variable to a model with more than one explanatory variable.

The adjusted-\(R^2\) accounts for this unfairness by applying a penalty to each additional explanatory variable. We can fairly compare models with different numbers of explanatory variables using their respective adjusted-\(R^2\). In our example, we have gone from explaining 2.1\% of the total variation in federal spending to explaining 4.7\% of its total variation. Adding home ownership and income has more than doubled the explanatory power of our model of federal spending.

\hypertarget{explanatory-penalty}{%
\subsection{Explanatory penalty}\label{explanatory-penalty}}

Each explanatory variable we add to our regression model imposes a type of penalty on our results. Basically, for each explanatory variable included, we lose an observation in our data (not literally). This will be discussed further in the section on inference, but we need at least 33 observations to make valid inferences about a population based on sample estimates. If we had, say 50 observations in a dataset, and wanted to run a regression with 25 explanatory variables, then it is as though our regression model is based on only 25 observations (50 observations - 25 variables = 25 degrees of freedom). We will obtain results from such a model, but we should not use those results to make inferences.

In case you were wondering why not simply add all the variables we can to a model rather than carefully consider which variables to include and exclude in a model, this penalty is one of the primary reasons. Fewer degrees of freedom jeopardizes our ability to make valid inference. It can also reduce the precision of our predictions. The goal is to maximize the explanatory or predictive power of our regression model at minimal cost (i.e.~excluding superfluous variables). Choosing good regression models is where subject matter expertise plays a crucial role. Experience and knowledge within the context of the research question informs our choices. Statistics is the method by which we apply our expertise to data to make evidence-based decisions.

\begin{quote}
\textbf{To learn how to run regression in R, proceed to Chapter \ref{r-regression}.}
\end{quote}

\hypertarget{kt6}{%
\section{Key terms and concepts}\label{kt6}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Line concepts

  \begin{itemize}
  \tightlist
  \item
    y-intercept
  \item
    slope
  \item
    change in y versus value of y
  \end{itemize}
\item
  Regression model components

  \begin{itemize}
  \tightlist
  \item
    outcome/dependent/response variable
  \item
    independent/explanatory variable
  \item
    error term/statistical noise
  \item
    residual
  \item
    population parameter
  \item
    sample coefficients/estimates
  \end{itemize}
\item
  Goodness of fit

  \begin{itemize}
  \tightlist
  \item
    R-squared
  \item
    Adjusted R-squared
  \item
    root mean squared error (RMSE)
  \end{itemize}
\item
  Controlling for other factors in multiple regression
\end{itemize}
\end{learncheck}

\hypertarget{categorical-variables-and-interactions}{%
\chapter{Categorical Variables and Interactions}\label{categorical-variables-and-interactions}}

\begin{quote}
\emph{``For how can one know color in perpetual green, and what good is warmth without the cold to give it sweetness?''}

---John Steinbeck, Travels with Charley
\end{quote}

\hypertarget{lo7}{%
\section{Learning objectives}\label{lo7}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Explain why and how to extend simple or multiple regression models to
  a parallel slopes model
\item
  Interpret results of a parallel slopes model
\item
  Explain why and how to extend regression models to an interaction
  model
\item
  Interpret results of an interaction model
\item
  Provide advice on choosing between parallel slopes and interaction
  model
\item
  Explain why and how to extend regression models to a linear
  probability model
\item
  Interpret results of a linear probability model
\end{itemize}
\end{learncheck}

Chapter \ref{simple-and-multiple-regression} introduced regression using models containing quantitative variables only. In this chapter, we build our regression toolbox to include models that contain qualitative variables. We will cover three models in particular:

\begin{itemize}
\tightlist
\item
  Parallel slopes model: including a categorical explanatory variable
\item
  Interaction model: allowing the slope of the regression line for each level of a categorical variable to differ (i.e.~not parallel)
\item
  Linear probability model: including a two-level (binary) categorical outcome
\end{itemize}

\hypertarget{parallel-slopes-model}{%
\section{Parallel slopes model}\label{parallel-slopes-model}}

To introduce the inclusion of categorical variables in regression, the simplest type of categorical variable will be used. The simplest categorical variable is commonly referred to as a \textbf{dummy variable}. A dummy variable is a two-level or binary categorical variable. It takes on values of either 1 or 0, where 1 corresponds to ``yes'' or ``true'' and 0 corresponds to ``no'' or ``false''.

For example, a common way to represent biological sex in data is to use a dummy variable where either male or female is coded as 1 and the other sex is coded as 0. It is common to name such a variable as the level coded as 1. For example, a dummy variable coded as 1 for male and 0 for female will often be named ``male'' in a dataset. A variable coded as 1 to denote a person attained a college degree and 0 to denote they did not might be named ``college.''

The parallel slopes model using a dummy variable is represented in Equation \eqref{eq:pslope}.

\begin{equation}
y = \beta_0 + \beta_1x + \beta_2d + \epsilon
\label{eq:pslope}
\end{equation}

where \(d\) is simply used to distinguish the variable as a dummy variable from the quantitative \(x\) variable. The sample regression equation for the parallel slopes model is represented by Equation \eqref{eq:pslopesamp}.

\begin{equation}
\hat{y} = b_0 + b_1x + b_2d
\label{eq:pslopesamp}
\end{equation}

Knowing that \(d\) can equal only 1 or 0, we can plug these values into Equation \eqref{eq:pslopesamp} to understand the logic of the parallel slopes model before considering the results from an example. If \(d=0\), then \(b_2\) drops out of the equation because anything multiplied by 0 equals 0. In that case, our regression equation is,

\begin{equation}
\hat{y} = b_0 + b_1x
\label{eq:pslopesamp0}
\end{equation}

and we can plug in our results to predict changes or values of \(y\) just like before. If \(d=1\), then \(b_2\) stays in the model. Anything multiplied by 1 is equal to itself, and since \(d\) can only equal 1 if not equal to 0, we can drop \(d\) out of the equation like so

\begin{equation}
\hat{y} = b_0 + b_1x + b_2
\label{eq:pslopesamp1}
\end{equation}

Note that \(b_2\) is not multiplied by the value of another variable like \(b_1\) is multiplied by some change or value of \(x\). Instead, it is a constant like the y-intercept \(b_0\). In fact, combining \(b_0\) and \(b_2\) gives us a new y-intercept as shown in Equation \eqref{eq:pslopesamp1alt}.

\begin{equation}
\hat{y} = (b_0 + b_2) + b_1x
\label{eq:pslopesamp1alt}
\end{equation}

This volley of equations is not to meant to repulse the math averse. Rather, it is meant to show you that the logic of the parallel slopes model is quite simple. Including a dummy variable \(d\) is analogous to drawing two separate regression lines--one line through the observations for which \(d=0\) and another line through the observations for which \(d=1\). The second line will be either above or below the first regression line by a constant amount equal to \(b_2\), resulting in two regression lines running parallel to each other.

\hypertarget{using-parallel-slopes}{%
\subsection{Using parallel slopes}\label{using-parallel-slopes}}

Suppose we are interested in whether state laws mandating a jail sentence for drunk driving affects traffic fatalities, presumably by deterring drunk driving. To investigate, we collect the following data, some of which is previewed in Table \ref{tab:trdeath}.

\label{tab:trdeath}Sample of state traffic data

state

year

mrall

jaild

vmiles

mlda

unrate

region

41

1987

2.27606

yes

8.565328

21

6.2

West

22

1982

2.48916

yes

6.137799

18

10.3

South

4

1985

2.80201

yes

6.771263

21

6.5

West

23

1982

1.46127

yes

6.733286

20

8.6

N. East

27

1982

1.38156

no

7.059264

19

7.8

Midwest

8

1984

1.90596

no

7.707853

21

5.6

West

23

1984

2.00692

yes

8.083908

20

6.1

N. East

where \texttt{mrall} is number of traffic deaths per 10,000 population, \texttt{jaild} is the dummy variable for whether the state has a mandatory jail sentence for drunk driving, \texttt{vmiles} is the average miles driven per driver in a state, \texttt{mlda} is the minimum legal drinking age at the time, and \texttt{unrate} is the state's unemployment rate. There are 336 observations in this dataset (48 states from 1982 to 1988; panel data but used like a pooled cross-sectional for this example).

Suppose we choose to use the following model

\begin{equation}
mrall = \beta_0 + \beta_1vmiles + \beta_2jaild + \epsilon
\label{eq:pslopeexamp}
\end{equation}

First, let's visualize the relationship between these variables using a scatter plot with \texttt{vmiles} on the x axis and using color to differentiate states with and without mandatory jail sentences for drunk driving. Note in Figure \ref{fig:pslopescatter} below there appears to be a positive relationship between the average miles people drive and the traffic fatalities. That makes sense.

Now, imagine drawing a straight line through the red points that denote states with no mandatory jail for drunk driving and a separate line through the blue dots denoting states with mandatory jail sentencing. Do not force your imaginary lines to be parallel just yet. How do your two lines compare? Based on the data, our blue line should be above our red line, as the group of blue dots appear to be systematically higher than the group of red dots. The slopes are less obvious. It \emph{may} be the case that our red line should have a steeper slope than our \emph{blue} line, but it is difficult to tell.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/pslopescatter-1.pdf}
\caption{\label{fig:pslopescatter}Relationship between miles driven and traffic deaths by whether state has mandatory jail for drunk driving}
\end{figure}

The imaginary exercise you just went through is critical. Considering whether the slopes of our regression line to differ between the two groups is the difference between the parallel slopes model and the interaction model. Why not just add the interaction and if they are the same slope, so be it? Again, because we pay a penalty for adding superfluous explanatory variables. Moreover, an interaction model is more difficult to interpret and communicate to an audience. Most importantly, we should choose the model that reflects our theory that is based on our subject matter expertise. \textbf{Choosing to use a parallel slopes model forces the slopes between the two groups to be drawn (i.e.~estimated) as if they are parallel whether they actually are or not}.

Now having a sense of the relationship, we run our parallel slopes model, which generates the following results

\label{tab:psloperesults}Parallel slopes results

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

-0.238

0.182

-1.304

0.193

-0.597

0.121

vmiles

0.281

0.023

12.107

0.000

0.236

0.327

jaildyes

0.265

0.056

4.726

0.000

0.155

0.376

Now we can plug our results into the sample regression equation to answer whatever questions we may have or encounter.

\begin{equation}
\hat{mrall} = -0.238 + 0.281\times vmiles + 0.265 \times jaild
\end{equation}

I strongly recommend that you compare this equation to Equations 7.2-7.5. Otherwise, the suffering was in vain. The \texttt{jaild} variable is the \(d\) variable. It equals either 0 or 1. If a state does \textbf{not} have a mandatory jail sentence (jaild = 0), then we have

\begin{equation}
\hat{mrall} = -0.238 + 0.281\times vmiles
\end{equation}

and if a state does have a mandatory jail sentence, then we have

\begin{equation}
\hat{mrall} = -0.238 + 0.281\times vmiles + 0.265\\
\hat{mrall} = (-0.238 + 0.265) + 0.281\times vmiles\\
\hat{mrall} = 0.027 + 0.281\times vmiles
\end{equation}

The results tell us that, on average, states with mandatory jail sentencing have a higher traffic fatality rate of 0.265 per 10,000, all else equal.

We already know how to interpret the estimate for \texttt{vmiles} in Table \ref{tab:psloperesults}: The results suggest that, on average, as the average miles driven per driver in a state increases 1 mile, traffic fatalities per 10,000 increase 0.281, all else equal.

To help make this concrete, Figure \ref{fig:pslopescatter2} visualizes the results of our parallel slopes model. Note that, as expected, the blue line is above the red line. Based on the results, we know the blue line is above the red line by 0.265. We also know that the slope for both lines is 0.281.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/pslopescatter2-1.pdf}
\caption{\label{fig:pslopescatter2}Parallel slopes visualization}
\end{figure}

\hypertarget{variations}{%
\subsection{Variations}\label{variations}}

We can include a categorical variable with more than two levels. Suppose we were interested in whether traffic fatalities differ across U.S. regions. From the variables in Table \ref{tab:trdeath}, we might choose to construct the following model.

\begin{equation}
mrall = \beta_0 + \beta_1vmiles + \beta_2region + \epsilon
\label{eq:pslopeexamp2}
\end{equation}

where region is a four-level categorical variable containing South, West, N. East, and Midwest. Figure \ref{fig:pslopescatter2} visualizes this model. Note that there are now four regression lines, each corresponding to one of the four regions. There are clear differences between the regions. It appears states in the West and South are somehow different than states in the Midwest and Northeast with respect to traffic fatality rates.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/pslopescatter3-1.pdf}
\caption{\label{fig:pslopescatter3}Parallel slopes for 4 groups}
\end{figure}

Running this model produces the following results.

\label{tab:psloperesults2}Parallel slopes for regions

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

0.260

0.167

1.553

0.121

-0.069

0.589

vmiles

0.188

0.021

8.895

0.000

0.147

0.230

regionN. East

-0.076

0.065

-1.166

0.245

-0.204

0.052

regionSouth

0.519

0.056

9.283

0.000

0.409

0.630

regionWest

0.641

0.062

10.264

0.000

0.518

0.763

Note that Table \ref{tab:psloperesults2} provides estimates for three of the four regions. This is no different from our previous model; \texttt{jaild} has two levels, yes and no, but Table \ref{tab:psloperesults} provides only one estimate for \texttt{jaild=yes}. No matter how many levels in a categorical variable, one of the levels is set such that \(d=0\) and so that level drops out of the equation. Just like with the previous model where the estimate for \texttt{jaild=yes}
indicates how far above or below the regression line is relative to the line for \texttt{jaild=no}, the estimates for whatever levels remain in the equation indicate how far above or below the regression lines are relative to the excluded level.

In Table \ref{tab:psloperesults2}, Midwest is excluded. Look at Figure \ref{fig:pslopescatter2}, noting where the Midwest line is relative to the other regions. Northeast is below Midwest, while South and West are above it. The estimates for the three included regions in Table \ref{tab:psloperesults2} tell us how far the regions are above and below Midwest. Therefore, we could report something like

\begin{quote}
On average and controlling for average miles driven per driver, states in the Northeast region experience fewer traffic fatalities by approximately 0.08 per 10,000, while states in the South and West experience higher traffic fatality rates by 0.52 and 0.64, respectively.
\end{quote}

\hypertarget{interaction-model}{%
\section{Interaction model}\label{interaction-model}}

But what if we allowed the two slopes in Figure \ref{fig:pslopescatter2} to differ? If our expertise leads us to theorize the two slopes should differ between states with and without mandatory jail sentencing for drunk driving, and/or if the visualizing the data suggests they do, then we can choose to use an interaction model.

To be clear, allowing the two slopes to differ means we allow the marginal effect of the average miles driven per driver to differ between the two groups of states. Figure \ref{fig:pslopescatter3} visualizes this added flexibility. Note that the slope of the red line is indeed slightly steeper than the blue line. Thus, this visualization suggests that average miles driven per mile in states without mandatory jail sentences for drunk driving increases the traffic fatality rate by slightly more than it does in states with such laws.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/interactscatter-1.pdf}
\caption{\label{fig:interactscatter}Interaction model visualization}
\end{figure}

This version of an interaction model involves interacting a categorical variable with a numerical variable. Equation \eqref{eq:interaction} represents this version of the interaction model.

\begin{equation}
y = \beta_0 + \beta_1x + \beta_2d + \beta_3xd + \epsilon
\label{eq:interaction}
\end{equation}

Equation \eqref{eq:interaction} is similar to Equation \eqref{eq:pslope} for the parallel slopes model. The difference in \(\beta_3xd\). This is the interaction--multiplying two of the explanatory variables in our regression model. Equation \eqref{eq:interactionsamp} provides the sample equation of this interaction model.

\begin{equation}
\hat{y} = b_0 + b_1x + b_2d + b_3xd
\label{eq:interactionsamp}
\end{equation}

Following the same process as with the parallel slopes model, we can rearrange Equation \eqref{eq:interactionsamp} to examine the logic of this interaction model. If \(d=0\), then \(b_2\) and \(b_3x\) drop out of the model because they are multiplied by 0. Thus, we have the same sample regression equation as Equation \eqref{eq:pslopesamp0}.

\begin{equation}
\hat{y} = b_0 + b_1x
\label{eq:interactionsamp0}
\end{equation}

If \(d=1\), then \(b_2\) and \(b_3x\) remain in our model. As with the parallel slopes model, \(b_2\) combines with \(b_0\). This shifts the y-intercept for the group for which \(d=1\) either above or below the group for which \(d=0\) by the amount equal to \(b_2\). Because the lines are not parallel, just because a line starts above or below another does not mean it will stay above or below. It depends on the value of the \(b_3x\). The term \(b_3x\) is combined with \(b_1x\). Thus, if \(d=1\), we have the following sample regression equation

\begin{equation}
\hat{y} = b_0 + b_1x + b_2d + b_3xd\\
\hat{y} = b_0 + b_1x + b_2 + b_3x\\
\hat{y} = (b_0 + b_2) + (b_1 + b_3)x
\label{eq:interactionsamp1}
\end{equation}

In addition to the regression line for which \(d=1\) having an intercept above or below the regression line for which \(d=0\) by the amount \(b_2\), it will have a slope greater or lesser by the amount \(b_3\). Hopefully, it is clear how these equations correspond with the lines in Figure \ref{fig:interactscatter}.

\hypertarget{using-an-interaction}{%
\subsection{Using an interaction}\label{using-an-interaction}}

Running this interaction model on these data produces the following results

\label{tab:interactionmod}Interaction model results

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

-0.384

0.221

-1.741

0.083

-0.819

0.050

vmiles

0.300

0.028

10.634

0.000

0.245

0.356

jaildyes

0.731

0.399

1.831

0.068

-0.054

1.516

vmiles:jaildyes

-0.058

0.050

-1.178

0.240

-0.156

0.039

Once again, we can plug these values into our generic sample regression equation to obtain the following equation.

\begin{equation}
\hat{mrall} = -0.38 + 0.3\times vmiles + 0.73\times jaild - 0.06\times vmiles \times jaild
\label{eq:interactionex}
\end{equation}

For states without mandatory jail sentencing (jaild = 0), the equation is

\begin{equation}
\hat{mrall} = -0.38 + 0.3\times vmiles
\label{eq:interactionex0}
\end{equation}

and for states with mandatory jail sentencing (jaild = 1), the equation is

\begin{equation}
\hat{mrall} = -0.38 + 0.3\times vmiles + 0.73\times jaild - 0.06\times vmiles \times jaild\\
\hat{mrall} = 0.35 + 0.24\times vmiles
\label{eq:interactionex1}
\end{equation}

\hypertarget{variations-1}{%
\subsection{Variations}\label{variations-1}}

Variations on interaction models are beyond the scope of this book, but suffice it to say we can interact any two variables we deem necessary (or more). If you suspect that the effect of a variable on an outcome \emph{depends} on the value of another variable, then an interaction is how to model such a relationship.

\hypertarget{linear-probability-model}{%
\section{Linear probability model}\label{linear-probability-model}}

We have now covered the inclusion of categorical variables on the explanatory side of a regression model. We can also include categorical variables as an outcome. In fact, many interesting question involve outcomes of a categorical nature, particularly binary. For example,

\begin{itemize}
\tightlist
\item
  Did the person graduate from college (yes or no)?
\item
  Did the government default on its bond payments?
\item
  Did the program participant get a job afterward?
\item
  Did the nonprofit receive the grant it applied for?
\end{itemize}

As before, we may want to explain or predict such outcomes based on a set of explanatory variables.

A \textbf{linear probability model} (LPM) is just a special name we give the kind of regression we have already covered when the outcome is a dummy variable. The key difference between an LPM and what we have already done concerns probability. Whereas regression with a numerical outcome explains or predicts changes or values of the outcome in terms of the outcome's units, \textbf{the LPM explains or predicts changes or values in the \emph{probability} that the dummy outcome equals 1. If the dummy outcome is coded such that \(d=1\) means the event did occur, then the LPM estimates the probability that the event in question occurs}.

Equation \eqref{eq:lpm} shows the generic population LPM, which is the same as the generic multiple regression population model except for the left-hand side. All this equation is trying to denote is that our estimates on the right-hand side pertain to the \emph{probability} (Pr) that \(y=1\). Equation \eqref{eq:lpmsample} shows the sample LMP equation.

\begin{equation}
Pr(y=1)=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_kx_k+\epsilon
\label{eq:lpm}
\end{equation}

\begin{equation}
\hat{Pr(y=1)}=b_0+b_1x_1+b_2x_2+\cdots +b_kx_k
\label{eq:lpmsample}
\end{equation}

Again, nothing is different with respect to how we use the above equations to answer questions concerning the predicted change or value of the outcome. We simply need to remember that those changes or values will be expressed as probabilities that \(y=1\) and what it means for \(y\) to equal 1 in the context of our particular question.

\hypertarget{using-lpm}{%
\subsection{Using LPM}\label{using-lpm}}

What if instead of modeling traffic fatality rates as an outcome dependent on miles driven and mandatory jail sentencing for drunk driving, we modeled whether a state has mandatory sentencing as an outcome dependent on traffic fatality rates and region? Perhaps states passed such laws because they have high traffic fatality rates. Equation \eqref{eq:lpmex} represents our model in this case.

\begin{equation}
Pr(jaild=1)=\beta_0+\beta_1vmiles+\beta_2region+\epsilon
\label{eq:lpmex}
\end{equation}

Scatter plots for LPMs are not particularly useful for communicating to an audience, but they can provide insight to what it is we are trying to do with an LPM, if it is not yet clear. Figure \ref{lpmscatter} changes the coding of \texttt{jaild} from yes/no to 1/0 and plots it on the y axis against traffic fatality rates on the x axis. Regions are excluded for simplicity.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/lpmscatter-1.pdf}
\caption{\label{fig:lpmscatter}LPM visualization}
\end{figure}

Immediately, we should notice Figure \ref{fig:lpmscatter} does not look like the typical scatter plots we have viewed thus far. This is because all observations fall into one of two values for \texttt{jaild}. It is also difficult to tell how our regression line is being drawn through the data.

The points along the x axis are states for which \texttt{jaild\ =\ 0}, meaning they do not impose mandatory jail sentencing for drunk driving. The points at the top are states for which \texttt{jaild\ =\ 1}. Compared to the points at the bottom, note the slight shift to the right the points at the top seem to have made. This shift is what informs the regression line to slope upward. The pattern of these data suggests there is a positive association between traffic fatality rate and passing a mandatory jail sentencing for drunk driving.

The values of \(y\) along the regression line are the predicted probabilities that a state has a mandatory jail law given the corresponding values for traffic fatality rate. For example, states with a rate of 3 appear to have a probability of 0.5 or 50\% of passing a mandatory jail law.

Running this model produces the following results

\label{tab:lpmresults}LPM results

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

-0.066

0.102

-0.645

0.520

-0.267

0.135

mrall

0.117

0.054

2.171

0.031

0.011

0.222

regionN. East

0.064

0.070

0.913

0.362

-0.074

0.202

regionSouth

0.040

0.068

0.580

0.562

-0.094

0.173

regionWest

0.361

0.078

4.634

0.000

0.208

0.514

Plugging these results into our sample regression equation gives us

\begin{equation}
\hat{Pr(jaild=1)}= -0.07 + 0.12 \times vmiles + 0.06 \times neast + 0.04 \times south + 0.36 \times west
\label{eq:lpmexsample}
\end{equation}

Once again, we are back to plug-and-chug. What is the predicted probability that state in the Midwest with a traffic fatality rate of 2.5 has a mandatory jail sentence for drunk driving? Answer:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{-0.07} \OperatorTok{+}\StringTok{ }\FloatTok{0.12}\OperatorTok{*}\FloatTok{2.5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.23
\end{verbatim}

there is a 23\% likelihood that such a state has such a law. How would an increase of 2 fatalities per 10,000 affect the probability that a state imposes a mandatory jail law? Answer:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.12}\OperatorTok{*}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.24
\end{verbatim}

an increase in probability by about 24\%. Furthermore, our results suggest states in the West are substantially more likely to have this law. Compared to the Midwest, the West is 36\% more likely to have a law and about 30\% more likely than states in the South and Northeast.

\hypertarget{fit}{%
\subsection{Fit}\label{fit}}

Because our outcome is a dummy variable, it does not have the same kind of variation we need to assess model fit like before using \(R^2\) or RMSE. Since we are explaining or predicting whether or not an outcome occurs, we can assess the fit of the model based on how well it predicts the observed outcomes.

We could change this threshold, but suppose we decide that if our model predicts the likelihood an outcome at 50\% or greater, then we say that our model predicts the outcome will occur \(y=1\) and so \(y=0\) otherwise. Table @ref(tab: lpmpointstab) shows a few rows applying this logic. Note that each row shows the observed data for each variable in our model, then the predicted probability in the \texttt{jaild\_hat} column, then the rounding of that probability to 0 or 1 in the \texttt{prediction} column. Note the similarities and differences between the observed outcomes in \texttt{jaild} and the predicted outcomes \texttt{prediction}. Sometimes our model predicts correctly, and sometimes it does not.

\label{tab:lpmpointstab}Binary predictions from LPM

ID

jaild

mrall

region

jaild\_hat

prediction

29

0

2.174

West

0.549

1

113

1

1.461

N. East

0.169

0

254

0

0.821

N. East

0.094

0

98

1

1.936

Midwest

0.160

0

68

0

2.575

West

0.596

1

241

1

2.080

West

0.538

1

182

0

1.825

N. East

0.211

0

Table \ref{tab:confumat} below is referred to as a \textbf{confusion matrix}. It is simply a cross-tabulation of the observed outcomes and the predicted outcomes with the predictions along the top as columns.

\label{tab:confumat}Confusion matrix for LPM

0

1

0

210

31

1

56

38

We can see that there are 248 cases where our model correctly predicts the outcome (210 + 38). There are 87 cases where our model incorrectly predicts the outcome. Specifically, there are 31 cases where our model predicts a state has a law but doesn't and 56 cases where our model predicts a state does not have a law but does. We can also convert these to percentages like the table below. These confusion matrices help us assess and communicate how accurate our model is.

\label{tab:unnamed-chunk-49}Confusion matrix for LPM (in proportions)

0

1

0

0.7894737

0.4492754

1

0.2105263

0.5507246

\begin{quote}
\textbf{To Learn how to include interactions in a regression using R, proceed to Chapter \ref{r-interactions}.}
\end{quote}

\hypertarget{kt7}{%
\section{Key terms and concepts}\label{kt7}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Dummy variable
\item
  Parallel slopes
\item
  Interaction
\item
  Difference between parallel slopes and interaction models
\item
  Linear probability model (LPM)
\item
  Confusion matrix
\end{itemize}
\end{learncheck}

\hypertarget{nonlinear-variables}{%
\chapter{Nonlinear Variables}\label{nonlinear-variables}}

\begin{quote}
\emph{``The shortest distance between two points is often unbearable.''}

---Charles Bukowski
\end{quote}

So far, we have repeatedly drawn straight lines through points. But, we know not all relationships are linear. Our income tends to rise and fall with age. Those in charge of the purchasing or production of something should know that average and marginal costs fall and then rise with quantity. Happiness tends to rise sharply with income but then plateaus at around \$70,000 per year. If our goal is to draw the line that fits data best, why draw a straight line through data that is evidently nonlinear?

In this chapter, we will cover two ways to incorporate nonlinear relationships:

\begin{itemize}
\tightlist
\item
  Include a quadratic
\item
  Include a logarithmic transformation
\end{itemize}

\hypertarget{lo8}{%
\section{Learning objectives}\label{lo8}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Explain why and how to extend a regression model to include a
  quadratic relationship
\item
  Interpret the coefficients associated with a quadratic term in a
  regression model
\item
  Compute the value of the quadratic explanatory variable at which the
  outcome is at its maximum or minimum
\item
  Explain the difference between percent change and percentage point
  change
\item
  Explain why to log-transform variables in a regression model
\item
  Interpret results from log-log, log-level, and level-log models
\end{itemize}
\end{learncheck}

\hypertarget{quadratic}{%
\section{Quadratic}\label{quadratic}}

If we theorize or see visual evidence that the association between an explanatory variable and an outcome is such that the outcome initially increases or decreases as the explanatory variable increases, then, at some value of the explanatory variable, the outcome decreases, then we may want to include a quadratic term of that explanatory variable in our regression model. That long-winded statement warrants an immediate visualization provided by Figure \ref{fig:quadscatter} below.

Note that wage appears to initially increase with age, then decreases. The data present a pattern that resembles an inverted U, also known as a concave parabola. Age and wage is a classic example of a quadratic relationship. We should not force ourselves to fit a straight line to these data; we can estimate a better line.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/quadscatter-1.pdf}
\caption{\label{fig:quadscatter}Wages by age}
\end{figure}

Equation \eqref{eq:quadratic} presents a generic population regression model with a quadratic term. With respect to the math, the only difference between this and previous models is the choice to square one of the explanatory variables. This is just an example. Any number of explanatory variables can be squared if theory warrants it.

\begin{equation}
y = \beta_0 + \beta_1x_1 + \beta_2x_1^2 + \beta_3x_2 + \cdots + \beta_kx_k + \epsilon
\label{eq:quadratic}
\end{equation}

Thus, the sample equation is as follows

\begin{equation}
\hat{y} = b_0 + b_1x_1 + b_2x_1^2 + b_3x_2 + \cdots + b_kx_k
\label{eq:quadraticsamp}
\end{equation}

Fully understanding the logic of the above equations to answer questions we may encounter as we have done before involves calculus that this book will spare you. In order to report the marginal effect of a variable that has been squared on an outcome in regression, we use Equation \eqref{eq:quadraticsampmarg} below. The result of this equation provides the predicted change in \(y\) given a one-unit change in \(x_1\).

\begin{equation}
b_1 + 2b_2x_1
\label{eq:quadraticsampmarg}
\end{equation}

Note that had we not squared \(x_1\) the predicted change in \(y\) from a one-unit change in \(x_1\) would be \(b_1\), which is exactly the same as in previous models. However, now that we are drawing a curved line, the effect of a one-unit change in \(x\) on \(y\) is not constant; it changes depending on the value of \(x\).

Another important question when a quadratic relationship is involved is at what value of \(x\) is \(y\) maximized or minimized. This can help decision-making, such as how to minimize costs or maximize profit, or maximize the probability of some desirable outcome. In order to report the value of \(x\) at which \(y\) reaches its maximum or minimum, we use Equation \eqref{eq:quadraticsampopt} below. The result of the equation gives us the optimal value of \(x\).

\begin{equation}
x = {\frac{-b_1}{2b_2}}
\label{eq:quadraticsampopt}
\end{equation}

\hypertarget{using-quadratics}{%
\subsection{Using quadratics}\label{using-quadratics}}

Suppose we collect the following data.

\label{tab:wagestab}Preview of wages, age, and education

Wage

Educ

Age

19.13

9

35

25.67

12

37

38.77

21

41

32.37

17

53

19.34

6

38

18.76

18

73

18.11

14

21

Therefore, our regression equation is as follows

\begin{equation}
Wage = \beta_0 + \beta_1Age + \beta_2Age^2 + \beta_3Educ + \epsilon
\label{eq:quadraticex}
\end{equation}

Running this regression generates the following results

\label{tab:quadextab}Quadratic model results

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

-22.722

3.023

-7.517

0

-28.742

-16.701

Age

1.350

0.134

10.077

0

1.083

1.617

I(Age\^{}2)

-0.013

0.001

-9.840

0

-0.016

-0.011

Educ

1.254

0.090

13.990

0

1.075

1.432

In Table \ref{tab:quadextab}, note that there are two rows for Age--one for the linear or level term and a second for the quadratic term. When the quadratic relationship is an inverted U, or concave, the linear term will be positive and the quadratic will be negative. This corresponds with an initial positive relationship that eventually turns negative once the negative quadratic term overcomes the positive linear term.

Plugging our results from Table \ref{tab:quadextab} into the regression equation, we obtain the following equation

\begin{equation}
\hat{Wage} = -22.7 + 1.4\times Age - 0.01\times Age^2 + 1.3\times Educ
\label{eq:quadraticexsamp}
\end{equation}

We can answer questions regarding the predicted \emph{value} of Wage the same way as before. For example, the predicted wage of an individual who is 40 years old with 16 years of education is

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{-22.7} \OperatorTok{+}\StringTok{ }\FloatTok{1.4}\OperatorTok{*}\DecValTok{40} \OperatorTok{-}\StringTok{ }\FloatTok{0.01}\OperatorTok{*}\DecValTok{40}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\FloatTok{1.3}\OperatorTok{*}\DecValTok{16}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 38.1
\end{verbatim}

dollars per hour.

To predict the \emph{change} in wage given a change in age, we need to know the beginning point for age. For example, if we were asked what is the predicted change is wage for a 24-year old two years later who consequently increases their education from 16 to 18 to get their masters degree, we plug this scenario into Equation \eqref{eq:quadraticsampmarg} like so

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\OperatorTok{*}\NormalTok{(}\FloatTok{1.4} \OperatorTok{-}\StringTok{ }\DecValTok{2}\OperatorTok{*}\FloatTok{0.01}\OperatorTok{*}\DecValTok{24}\NormalTok{) }\OperatorTok{+}\StringTok{ }\FloatTok{1.25}\OperatorTok{*}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.34
\end{verbatim}

providing us the answer of a predicted increase of \$4.34.

Controlling for education, at what age do wages tend to reach their maximum? To answer, we plug the results into \eqref{eq:quadraticsampopt} like so

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{-1.35}\OperatorTok{/}\NormalTok{(}\DecValTok{2}\OperatorTok{*-}\FloatTok{0.013}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 51.92308
\end{verbatim}

According to our results, wages reach their maximum at around 52 years of age.

\hypertarget{log-models}{%
\section{Log models}\label{log-models}}

Once again, we will forego the math of logarithms and instead focus on why we may want to use them in a regression model and how to interpret the results. In short, logarithms are used to express rates of change in a variable (i.e.~percent change) rather than absolute change in a variable (i.e.~unit change).

\hypertarget{logarithmic-scales}{%
\subsection{Logarithmic scales}\label{logarithmic-scales}}

Graphs like Figure \ref{fig:logcovid} below were commonplace during the initial COVID-19 spread. Take a look at the small note at the bottom explaining to readers the purpose of a logarithmic scale. Note how the values on the y axis are evenly dispersed, but each tick mark increases by a factor of 10 (i.e.~the previous value multiplied by 10). The y-axis is in a \textbf{log10} scale. Another common log scale for visualization is a \textbf{log2} scale which increases each interval by a factor of 2.

The use of a log scale allows us to compare states like New York and Wyoming by taking into account large differences in absolute numbers. It would be unfair to compare the absolute number of COVID cases in New York to the absolute number of cases in Wyoming. It would also be unfair to compare the absolute number of new cases each day between the two states. A non-trivial portion of those numbers is a result of the size of the population in each state.

However, it is fair to compare the \emph{rate} of growth between the two states. While it is obviously concerning that New York has over 300,000 deaths, the key feature of this graph is that we can compare the slopes of each state's growth path because population size has been accounted for by the y-axis. Given New York's population and population density, it was likely to have the most cases, but the state also had the fastest growth rate in cases from about day 5 to day 10.

\begin{figure}
\includegraphics[width=14.11in]{images/logs_covid} \caption{Growth in COVID-19 cases by state}\label{fig:logcovid}
\end{figure}

\hypertarget{percent-v-percentage-point-change}{%
\subsection{Percent v percentage point change}\label{percent-v-percentage-point-change}}

Rate of change typically refers to percent change. The equation for percent change is shown below.

\begin{equation}
PctChange = {\frac{NewValue-OldValue}{OldValue}} \times 100
\label{eq:pctchange}
\end{equation}

For example, if the number of COVID cases increase from 1,000 to 10,000 over the course of a week, the absolute change is 9,000. The percent change is 900\%.

A common cause of confusion is the difference between a percent change and a percentage point change. This occurs when we discuss the change in a variable that is already expressed as a percent. For example, if the U.S. unemployment rate increases from 4\% to 15\% during the pandemic, that's an absolute change of 11 percentage points. The unemployment rate is expressed units of percentage points, so a unit change is a percentage point change. An increase from 4\% to 15\% is also a 275\% percent change.

As we have seen in previous examples of regression, when we include a variable that is not log-transformed, regression estimates the \textbf{unit change} in \(y\) given a \textbf{unit change} in \(x\). If a variable is expressed in units of percentages like unemployment or poverty, then a unit change for those variables is a percentage point change. Including a log-transformed variable in regression estimates percent changes in the variable(s) we transformed.

One reason we may prefer to use percent change is if the variable in question has some underlying impact that differs depending on the initial value from which it changed. This too applies to measures of wealth or income. Suppose we estimate that a policy will, on average, increase peoples' incomes by 12,000 dollars. This average unit change does not quite capture the benefit of the policy. Imagine a society of two people. One person earns 20,000 dollars per year and the other earns 80,000 dollars. That 12,000 likely has a greater positive impact on the low-income individual than it does the high-income individual. Consequently, this can be expressed in percent change. The 12,000 represents a 60\% increase in income for the low-income individual and 15\% for the high-income individual.

\hypertarget{why-logs-in-regression}{%
\subsection{Why logs in regression}\label{why-logs-in-regression}}

To summarize, we may want to use logs in regression if

\begin{itemize}
\tightlist
\item
  it is preferable to express change in percentages rather than units
\item
  a variable we intend to include has a skewed distribution
\item
  we theorize the relationship between two variables follows a logarithmic path
\end{itemize}

Let's consider these last two reasons further. As was mentioned, log scales allow us to compare numbers that are very far apart, as seen in Figure \ref{fig:logcovid}. If the scale for COVID cases were left in constant units, New York and a few other states would be so far above most other states that it would be difficult to fit in a sensible graph. Using logs condensed or pulled those extreme numbers back to a more compact distribution.

As will become clear in the next section on inference, using a sample to make valid conclusions about a population relies heavily on the normal distribution, which was introduced in Chapter \ref{descriptive-statistics}. In a similar sense, we want the variables we use for inference to be approximately normally distributed because extreme values of a skewed distribution can impose undue influence on our results. Log-transformations can transform a skewed distribution to be more normal.

For instance, variables that measure income or wealth tend to be right-skewed. Figure \ref{fig:gapskew} shows the distribution of GDP per capita across most countries in multiple years. Clearly this distribution is not normal and skewed to the right. It is difficult to see because there are so few cases, but some countries have GDP per capita near or more than \$120,000.

Figure \ref{fig:gaplog} shows the distribution if we convert GDP per capita to a log scale (log10 was used but any log scale will achieve the same normalization). Now we have a more normal distribution. This is desirable in statistics.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gapskew-1.pdf}
\caption{\label{fig:gapskew}Distribution of GDP per capita}
\end{figure}

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gaplog-1.pdf}
\caption{\label{fig:gaplog}Distribution of log10 GDP per capita}
\end{figure}

The third reason for using logs concerns theory, which should always inform the choices we make in statistics. When choosing how to model the relationship between an outcome and an explanatory variable, if past research, experience, visualization of data, or intuition tells us that the outcome changes dramatically at first, then begins to flatten, a logarithmic transformation should be used.

For example, suppose we wish to examine the relationship between national wealth and life expectancy. Intuitively, we expect this relationship to be positive--as wealth increases, life expectancy should increase. Also, life expectancy has some natural ceiling, so it cannot increase indefinitely, and we may expect relatively small increases from low levels of wealth to have much greater impacts on life expectancy then similar increases from high levels of wealth.

Figure \ref{fig:gapskewscatter} provides a scatter plot of GDP per capita and life expectancy in their original units. Note the rapid increase then plateau in life expectancy. A regression line would not fit these data well.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gapskewscatter-1.pdf}
\caption{\label{fig:gapskewscatter}Relationship between wealth and life expectancy using unit scale}
\end{figure}

Using a log transformation in an association between two variables does not change the underlying data or relationship, but it \emph{does} transform the pattern of points to be more linear, thus allowing a linear regression line to do a better job modeling the relationship. Figure \ref{fig:gaplogscatter} uses the same data but GDP per capita has been transformed to log scale. This simple change makes a big difference for the validity of any conclusions we may make regarding the relationship between wealth and life expectancy.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gaplogscatter-1.pdf}
\caption{\label{fig:gaplogscatter}Relationship between wealth and life expectancy using log scale}
\end{figure}

\hypertarget{using-log-models}{%
\subsection{Using log models}\label{using-log-models}}

There are three variations of the log model:

\begin{itemize}
\tightlist
\item
  Level-log: log transforming one or more explanatory variables but not the outcome
\item
  Log-level: log transforming the outcome but not an explanatory variable
\item
  Log-log: log transforming the outcome and an explanatory variable
\end{itemize}

Each model fits slightly different patterns of association best but they share the general pattern of a pronounced initial increase or decrease followed by a plateau. If past research, visuals, or theory does not lead us to choose one model over the other, one option is to compare the goodness-of-fit between the three, choosing the one with the highest \(R^2\) or lowest RMSE.

One last point before presenting each of the models and how to interpret: using the logarithmic transformation uses a special log scale called the \textbf{natural log}, often denoted as \textbf{\(ln\)}, as opposed to, say, \(log_{10}\) or \(log_2\). You do not need to concern yourself with the mathematical properties of the natural log. Just know that the natural log is what is used in regression to transform unit changes to percent changes.

\hypertarget{log-log}{%
\subsubsection*{Log-log}\label{log-log}}
\addcontentsline{toc}{subsubsection}{Log-log}

The log-log model is somewhat special among the three variations because it estimates a commonly used measure in economic or policy analyses--the \textbf{elasticity}. You may have already learned in policy analysis courses that the \textbf{elasticity is the percent change in an outcome given a one percent change in the explanatory variable}.

Equation \eqref{eq:loglog} presents a generic log-log model. Log-log is simply meant to convey that we logged our outcome and logged at least one explanatory variable.

\begin{equation}
ln(y)=\beta_0 + \beta_1ln(x_1) + \beta_2x_2 + \cdots + \beta_kx_k + \epsilon
\label{eq:loglog}
\end{equation}

Thus, the sample equation is

\begin{equation}
\hat{ln(y)}=b_0 + b_1ln(x_1) + b_2x_2 + \cdots + b_kx_k
\label{eq:loglogsamp}
\end{equation}

When we obtain an estimate for \(b_1\) we can plug it into the following template

\begin{quote}
On average, a one percent change in \(x_1\) is associated with a \(b_1\) percent change in \(y\), all else equal.
\end{quote}

Or, if we wanted to report using elasticity language, assuming our audience understands what we are talking about:

\begin{quote}
According to the results, the \(x_1\) elasiticy of \(y\) is \(b_1\).
\end{quote}

\hypertarget{level-log}{%
\subsubsection*{Level-log}\label{level-log}}
\addcontentsline{toc}{subsubsection}{Level-log}

Equation \eqref{eq:levlog} presents a generic level-log model. Level-log is simply meant to convey that we logged at least one explanatory variable.

\begin{equation}
y=\beta_0 + \beta_1ln(x_1) + \beta_2x_2 + \cdots + \beta_kx_k + \epsilon
\label{eq:levlog}
\end{equation}

Thus, the sample equation is

\begin{equation}
\hat{y}=b_0 + b_1ln(x_1) + b_2x_2 + \cdots + b_kx_k
\label{eq:levlogsamp}
\end{equation}

When we obtain an estimate for \(b_1\) we can plug it into the following template

\begin{quote}
On average, a one percent change in \(x_1\) is associated with a \(\frac{b_1}{100}\) unit change in \(y\), all else equal.
\end{quote}

Or, if dividing our estimate by 100 results in too small of a number to report, we can say the following

\begin{quote}
On average, a doubling of \(x_1\) is associated with a \(b_1\) unit change in \(y\), all else equal.
\end{quote}

because a doubling is equal to a 100 percent increase. Multiplying \(\frac{b_1}{100}\) by 100 cancels out the 100 in the denominator, leaving us with just \(b_1\).

\hypertarget{log-level}{%
\subsubsection*{Log-level}\label{log-level}}
\addcontentsline{toc}{subsubsection}{Log-level}

Equation \eqref{eq:loglev} presents a generic log-level model. Log-level is simply meant to convey that we logged our outcome.

\begin{equation}
ln(y)=\beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_kx_k + \epsilon
\label{eq:loglev}
\end{equation}

Thus, the sample equation is

\begin{equation}
\hat{ln(y)}=b_0 + b_1x_1 + b_2x_2 + \cdots + b_kx_k
\label{eq:loglevsamp}
\end{equation}

When we obtain an estimate for \(b_1\) we can plug it into the following template

\begin{quote}
On average, a one unit change in \(x_1\) is associated with a \(b_1 \times 100\) percent change in \(y\), all else equal.
\end{quote}

\hypertarget{example}{%
\subsubsection*{Example}\label{example}}
\addcontentsline{toc}{subsubsection}{Example}

Let's continue our investigation of national life expectancy using the various log models. Suppose we are interested in using the following base model for the three varieties of log models. Continent is included because perhaps we think it will capture some geographical, social, and/or cultural differences that impact life expectancy.

\begin{equation}
LifeExp=\beta_0 + \beta_1GDPpercap + \beta_2Continent + \epsilon
\label{eq:logex}
\end{equation}

The following tables present results for each of the three log models.

\label{tab:loglogtab}Log-log results

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

3.062

0.026

117.692

0

3.011

3.113

log(gdpPercap)

0.112

0.004

31.843

0

0.105

0.119

continentAmericas

0.133

0.011

12.519

0

0.112

0.154

continentAsia

0.110

0.009

12.037

0

0.092

0.128

continentEurope

0.166

0.012

14.357

0

0.143

0.189

continentOceania

0.152

0.029

5.187

0

0.095

0.210

\label{tab:levlogtab}Level-log results

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

2.317

1.359

1.704

0.088

-0.349

4.983

log(gdpPercap)

6.422

0.183

35.003

0.000

6.062

6.782

continentAmericas

7.015

0.554

12.652

0.000

5.927

8.102

continentAsia

5.912

0.477

12.400

0.000

4.977

6.847

continentEurope

9.577

0.604

15.855

0.000

8.392

10.762

continentOceania

9.213

1.536

5.999

0.000

6.201

12.226

\label{tab:loglevtab}Log-level results

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

3.856

0.006

601.881

0

3.843

3.869

gdpPercap

0.000

0.000

16.374

0

0.000

0.000

continentAmericas

0.250

0.011

22.054

0

0.228

0.272

continentAsia

0.160

0.010

15.322

0

0.140

0.181

continentEurope

0.311

0.012

26.383

0

0.288

0.334

continentOceania

0.316

0.034

9.381

0

0.250

0.382

Our log-log results indicate that a one percent increase in GDP per capita is associated with a 0.11 percent increase in life expectancy, on average and controlling for continent.

Our level-log results indicate that a one percent increase in GDP per capita is associated with an increase in life expectancy of 0.06 years.

Our log-level results indicate that a one dollar increase in GDP per capita is associated with a indiscernible percent increase in life expectancy. Changing GDP per capita from dollars to something like thousands of dollars would probably give an estimate that doesn't round to 0.

The continent estimates can be interpreted in similar fashion, remembering that with categorical variables, the estimate of each level of the variable is relative to the base comparison excluded from the equation. In this example, Africa is the base comparison. Let's focus on Asia for interpretation.

Our log-log results indicate that life expectancy in Asia is 110\% greater than life expectancy in Africa. Since a dummy variable can only change from 0 to 1, this is equivalent to a 100 percent change. Therefore, we must multiply our estimate by 100.

Our level-log results indicate that life expectancy in Asia is 5.9 years greater than life expectancy in Africa. Lastly, our log-level results indicate that life expectancy in Asia is 160\% greater than life expectancy in Africa.

\begin{quote}
\textbf{To learn how to include nonlinear variables in regression using R, proceed to Chapter \ref{r-nonlinear-regression}.}
\end{quote}

\hypertarget{kt8}{%
\section{Key terms and concepts}\label{kt8}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Marginal effect
\item
  Logarithmic scale
\item
  Percent change
\item
  Percentage point change
\item
  Natural log
\item
  Elasticity
\end{itemize}
\end{learncheck}

\hypertarget{causation-and-bias}{%
\chapter{Causation and Bias}\label{causation-and-bias}}

\begin{quote}
\emph{``All models are wrong but some are useful.''}

---George Box
\end{quote}

Our regression toolbox has grown considerably over the previous three chapters. We can now set out to answer a multitude of research questions that require us to accommodate different types of variables that may share linear or nonlinear relationships. Nevertheless, a model is a simplified version of our complex world. In this sense, all models are wrong. The goal is to make modeling choices that prevent our model from being \emph{so} wrong that they are useless for decision-making.

\hypertarget{lo9}{%
\section{Learning objectives}\label{lo9}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Explain the difference between using regression to predict an outcome
  versus explain an outcome and the consequences each has on model
  choices
\item
  Explain internal and external validity
\item
  Explain the criteria to validly claim a causal relationship
\item
  Use a directed acyclical graph (DAG) to represent a regression model
\item
  Identify confounders and colliders in a DAG
\item
  Identify the number of backdoor paths in a DAG and each are open or
  closed
\item
  Given a DAG, identify the variables one would need to control for to
  close any backdoor paths
\item
  Determine whether a regression model plausibly eliminates omitted
  variable bias
\item
  Predict the direction of omitted variable bias
\end{itemize}
\end{learncheck}

First, let's consider the two possible goals of regression:

\begin{itemize}
\tightlist
\item
  \textbf{Explain} the change in an outcome due to a change in a set of explanatory variables
\item
  \textbf{Predict} the value of an outcome given values for a set of explanatory variables
\end{itemize}

The usefulness of a model with the sole goal of prediction is how well it predicts the outcome. That may sound obvious and cyclical but it is an important point. Which variables we choose to include and their linear or nonlinear relationships is a secondary concern of prediction, if at all. If we don't care \emph{how} variables affect the outcome and only care about predicting the outcome with the greatest accuracy and precision possible, then we can throw together whatever model we want to achieve that without much concern for what the model actually means.

Models with the goal of prediction are common within the field of forecasting, which will be introduced in Chapter \ref{forecasting}. Sometimes, we care about good prediction and \emph{how} some explanatory variables impact the outcome. Then we are back in the realm of explanation where we have to take special care about which variables are included and excluded from our model as well as how they relate with each other.

The focus of this chapter is explanation using regression. Many scenarios within program evaluation or policy analysis involve explaining whether and to what extent one variable impacts another we care about changing. Ultimately, our concern is causality. It is one thing to conclude two variables are associated with each other; it is an entirely different thing to conclude that a change in one variable \emph{causes} the other to change. If we propose to spend millions of dollars on a program to help people, or decide to cut a program that does not, we should be as certain as about this causal claim as statistics allows us to be.

Sufficient understanding of causality or causal inference warrants its own course. It does not involve regression models that much different from what you have learned so far or will learn by the end of this book, but it does involve a broader and deeper understanding of research design and knowing how to identify threats to internal and external validity. Let us revisit the figure of credible analysis first shown in Chapter \ref{measurement-and-missing}.

\begin{figure}
\includegraphics[width=13.58in]{images/credible} \caption{Components of credible analysis}\label{fig:credfigrepeat}
\end{figure}

Chapter \ref{measurement-and-missing} covered measurement validity and measurement reliability. Now, let us define internal validity and external reliability.

\begin{itemize}
\tightlist
\item
  \textbf{Internal validity:} the credibility of the theoretical assumptions applied to the causal connection established between the explanatory variable(s) and its (their) effect on the outcome.
\item
  \textbf{External validity:} can results of the analysis be applied beyond the subjects included or context involved?
\end{itemize}

In other words, is there reason to believe our results are critically mistaken and are those affected or the context in which they were affected so unique or limited that we can not generalize to other potential targets or contexts?

\hypertarget{causality}{%
\section{Causality}\label{causality}}

Three conditions must be met to credibly claim a causal relationship. We will address each in turn.

\begin{itemize}
\tightlist
\item
  The explanatory variable is correlated with the outcome
\item
  The change in the explanatory variable occurred prior to the change in the outcome
\item
  No alternative explanation exists to which the change in the outcome could be attributed instead of the explanatory variable
\end{itemize}

Correlation between the explanatory and outcome variables is perhaps the most straightforward condition to satisfy. We have not yet covered inference and how to identify statistically significant results. For now, suffice it to say that if we run a regression and the estimate for our explanatory variable is statistically significant, then we have established correlation between the explanatory and outcome variables that is unlikely to be random.

The second condition is succinctly referred to as \textbf{temporal precedence}. In order for something to be a cause, it must occur prior to its alleged effect. Otherwise, perhaps it is our supposed outcome that is having an effect on the cause, similar to what was considered in Chapter \ref{categorical-variables-and-interactions} with the mandatory jail for drunk driving. This is also known as \textbf{reverse causality}.

Reverse causality could still be argued even when it seems clear the cause occurred prior to the effect. For example, the mere availability of a scholarship may cause students to reach higher levels of academic achievement. If we were to claim receipt of the scholarship caused a rise in the likelihood of completing college, which obviously occurred prior to graduation, this may not be accurate. Perhaps by virtue of motivating oneself to perform better in school would result in a higher likelihood of graduation whether the scholarship was received or not. Then again, the student would not have been as motivated if not for the scholarship. Perhaps the \emph{availability} of the scholarship would meet temporal precedence more convincingly.

As may be evident by now, something as seemingly simple as before-and-after can become complex if the causal pathway is considered carefully. Credible causal claims require subject matter expertise as much as quantitative skills. For temporal precedence, do your best to ensure your explanatory variable was measured or occurred prior to when your outcome was measured or occurred.

The remainder of this chapter concerns the third and most difficult condition to satisfy: no plausible rival or alternative explanation for our causal claim. Since we do not have the luxury of delving deep into causal modeling, what can MPA students learn that will serve them well when they need to consider whether the possibility of alternative explanations has been reasonably eliminated? My answer to this is the \textbf{directed acyclical graph}.

\hypertarget{directed-acyclical-graphs}{%
\section{Directed acyclical graphs}\label{directed-acyclical-graphs}}

Directed acyclical graphs (DAGs) are visual representations of causal pathways. Constructing a DAG involves theory, existing research, or theory. A DAG requires us to state our assumptions clearly, thus allowing us and others to evaluate the internal validity of our model.

Figure \ref{fig:dagbasic} below shows a basic DAG. This model involves three variables, X, Y, and Z. Y denotes the outcome and X denotes the variable of primary interest for which we intend to estimate a causal effect. Z denotes any other variables in the model. The arrow from X to Y indicates our claim that X causes changes in Y. X also causes Z to change, and Z causes Y to change. In this example, the claim is that X has a direct effect on Y and an indirect (mediated) effect on Y through Z.

\begin{figure}
\includegraphics[width=4.42in]{images/dag_basic} \caption{A basic DAG}\label{fig:dagbasic}
\end{figure}

Any DAG follows a few rules and conventions:

\begin{itemize}
\tightlist
\item
  Only one-directional arrows (directed)
\item
  No arrow from Y back to X (acyclical)
\item
  Solid arrows used for relationships between observable variables or variables specifically observed in our data
\item
  Dashed arrows used for relationships between unobservable variables (e.g.~ability, attitudes, propensities for certain behaviors) or variables unobserved in our data
\end{itemize}

Underlying every regression with the goal to explain a causal relationship is a DAG. Consider the following regression model

\begin{equation}
BMI = \beta_0 + \beta_1Exercise + \epsilon
\label{eq:dagreg}
\end{equation}

Figure \ref{fig:dagreg} below shows the DAG that corresponds with Equation \eqref{eq:dagreg}. The central claim is that exercise causes BMI to change. Therefore, there is a solid arrow from exercise to BMI. Also, note that because \(\epsilon\), by definition, represents all other unobserved factors that affect BMI, there is a dashed line from \(\epsilon\) to BMI. Lastly, this DAG makes the assumption that no variables contained in \(\epsilon\) affects exercise, nor does exercise affect any variables contained in \(\epsilon\)

\begin{figure}
\includegraphics[width=7.19in]{images/dag_reg} \caption{DAG representation of a regression model}\label{fig:dagreg}
\end{figure}

With this basic set-up, we can begin to learn how to evaluate a DAG in order to determine if our regression model credibly eliminates alternative explanations of our causal claim.

\hypertarget{evaluationg-dags}{%
\subsection{Evaluationg DAGs}\label{evaluationg-dags}}

We will review two aspects of evaluating DAGs for causal claims:

\begin{itemize}
\tightlist
\item
  Identifying backdoor paths
\item
  Adjusting regression models based on the presence of confounding or colliding variables
\end{itemize}

A backdoor path is any indirect path from X to Y no matter the direction of the arrows that connect it. For example, Figure \ref{fig:dagbasic} has one backdoor path: X --\textgreater{} Z --\textgreater{} Y. Figure \ref{fig:dagreg} has no backdoor paths between exercise and BMI.

\hypertarget{confounders}{%
\subsubsection*{Confounders}\label{confounders}}
\addcontentsline{toc}{subsubsection}{Confounders}

Identifying backdoor paths allow us to then identify whether confounding or colliding variables are present in our model. Figure \ref{fig:dagconf} below shows a variation on the simple DAG with one backdoor path. The backdoor path still runs from X to Z to Y, but now the direction of the arrows connecting this path are different. Specifically, the backdoor path is X \textless-- Z --\textgreater{} Y.

\begin{figure}
\includegraphics[width=5.74in]{images/dag_confound} \caption{Example of confounder variable}\label{fig:dagconf}
\end{figure}

The variable Z in this case is a \textbf{confounder}. Any variable in a backdoor path with arrows directed away from it toward X and Y is a confounder. This representation means Z affects X and Y. If as Z changes, X and Y change, then we may incorrectly attribute the effect of Z on Y to the effect of X on Y because it appears to us that as X changes, Y changes. And it does, but it is really Z that is causing changes.

This example of a confounder is sometimes referred to as spurious correlation. For example, ice cream sales and crime are spuriously correlated due to both increasing because of temperature. It would be mistake to claim an increase in ice cream sales causes an increase in crime.

\hypertarget{colliders}{%
\subsubsection*{Colliders}\label{colliders}}
\addcontentsline{toc}{subsubsection}{Colliders}

Figure \ref{fig:dagcoll} below shows another variation of the simple DAG with one backdoor path between X and Y that goes through Z. The direction of the arrows are such that this backdoor path can be written as X --\textgreater{} Z \textless-- Y. In this case, Z is a \textbf{collider}. Any variable on which the arrows connecting X and Y converge is a collider.

\begin{figure}
\includegraphics[width=5.42in]{images/dag_collide} \caption{Example of collider variable}\label{fig:dagcoll}
\end{figure}

Z is just some variable that is affected by both X and Y. Changes in Z do not cause changes in X or Y. Therefore, if we estimate how much Y changes in response to a change in X, Z has nothing to do with that causal estimate.

\hypertarget{backdoor-criterion}{%
\subsection{Backdoor criterion}\label{backdoor-criterion}}

For any theoretical model we intend to estimate via regression, we can now identify backdoor paths and whether there are confounding or colliding variables along those backdoor paths. How do we use this information to eliminate plausible alternative explanations and confidently claim a one-unit change in X causes Y to change by \(\beta\)? We need to satisfy the \textbf{backdoor criterion}.

The backdoor criterion is satisfied if all backdoor paths between X and Y are closed. We know if a backdoor path is open or closed depending on the presence of confounding or colliding variables.

\begin{itemize}
\tightlist
\item
  A confounder variable opens a backdoor path
\item
  A collider variable closes a backdoor path
\end{itemize}

If a backdoor path is open, we can close it by controlling for the confounder variable or any other variable along the backdoor path between X and Y. For instance if were were to identify a backdoor path of the following form

X \textless-- Z --\textgreater{} A --\textgreater{} Y

where A is some fourth variable in our model, then controlling for Z or A will close this backdoor path. If a set of control variables in our regression model closes all backdoor paths, then we have satisfied the backdoor criterion and we can consider our estimate of X on Y to be a causal estimate.

Consider the simple backdoor path again where Z is a collider variable.

X --\textgreater{} Z \textless-- Y

This backdoor path is already closed. We don't need to control for anything in our model because of this backdoor path. In fact, \textbf{controlling for a collider opens a backdoor path}. Therefore, we should not control for Z in our model, as doing so opens a backdoor that was already closed.

This is a valuable insight provided by the use of DAGs. If we already have the data, it costs us virtually no time or effort to include a variable in our model we think may affect our outcome Y, and it can be quite tempting to throw variables into a regression model for not much more reason than we have it in our data. However, a DAG forces us to consider all of the relationships between the variables in our model. If an explanatory variable is a collider, then including it may threaten our ability to make causal claims. Sometimes, deliberately excluding a variable from a regression model is the right choice and DAGs give us a fairly simple way to make and explain that choice.

Consider another backdoor path where Z is a collider and A is a confounder

X --\textgreater{} Z \textless-- A --\textgreater{} Y

A opens this backdoor path but Z blocks A's confounding. Controlling for Z would open this backdoor path. We can control for A in our regression without reopening the backdoor path, but it is not necessary.

\hypertarget{dags-and-regression}{%
\subsection{DAGs and regression}\label{dags-and-regression}}

Let us relate this new information to making choices about regression models. Referring back to Equation \eqref{eq:dagreg} and Figure \ref{fig:dagreg}, recall that \(\epsilon\) represents all other variables we do not observe or cannot include in our regression model but affect our outcome, BMI. Based on the DAG for this regression model, there are no backdoor paths. Therefore, whatever estimate we get for \(\beta_1\) is causal estimate.

However, Figure \ref{fig:dagreg} is probably incorrect. There are likely variables contained in \(\epsilon\) that affect exercise. If that is the case, then our DAG should be drawn like Figure \ref{fig:dagregovb} below. Now we have a backdoor path of the form

Exercise \textless-- \(\epsilon\) --\textgreater{} BMI

where \(\epsilon\) is a confounder. Therefore, this backdoor path is open, and because we do not observe the variables in \(\epsilon\), we do not currently have the means to close it.

\begin{figure}
\includegraphics[width=6.65in]{images/dag_regovb} \caption{Counfounding error term}\label{fig:dagregovb}
\end{figure}

The issue illustrated by Figure \ref{fig:dagregovb} is commonly referred to as \textbf{omitted variable bias} (OVB) and it is the bane of analysts' attempts to estimate causal relationships. To have omitted variable bias means we have failed to satisfy the third criterion of causality. There is a variable out there we have not controlled for which causes our explanatory variable of interest and our outcome to change. Therefore, we cannot trust our estimate of the effect of our explanatory variable on our outcome because it may be due to the omitted variable. In other words, an omitted variable is biasing our estimate, \(b_1\) to be systematically above or below the population parameter \(\beta_1\).

Our next task then is to identify the set of variables that would eliminate the arrow from \(\epsilon\) to exercise. If we can credibly break the link between \(\epsilon\) and our explanatory variables, then we can credibly claim there is no omitted variable bias in our model.

For the sake of this example, suppose healthy eating is the key omitted variable. Healthy nutrition gives us the energy to exercise and obviously affects our BMI. Suppose we collect a variable that measures the extent to which a person's diet is healthy. Now, we have a new DAG, as depicted in Figure \ref{fig:dagregnut}.

\begin{figure}
\includegraphics[width=6.42in]{images/dag_regnut} \caption{Eliminating OVB}\label{fig:dagregnut}
\end{figure}

To close the backdoor path in this DAG, we need to control for nutrition in our regression model. Equation \eqref{eq:dagreg2} shows our new regression model. If our theory informing our new DAG is correct, then our estimates of \(\beta_1\) and \(\beta_2\) are unbiased.

\begin{equation}
BMI = \beta_0 + \beta_1Exercise + \beta_2Nutrition + \epsilon
\label{eq:dagreg2}
\end{equation}

The DAG in Figure \ref{fig:dagregnut} may still be fail to be sufficiently convincing to those who have expertise in public health or related fields. In that case, we would continue the process of identifying and controlling for variables until we credibly break the link between \(\epsilon\) and all explanatory variables in our model.

Isolating causal effects is hard, and careers can be made by successfully doing so. Regardless of whether an unbiased causal estimate can be obtained for a particular question, knowing whether or not threats exist and what could be done about it is valuable, especially for managers or consumers of statistical analyses who have expertise concerning the potential causal pathways involved.

\hypertarget{direction-of-ovb}{%
\section{Direction of OVB}\label{direction-of-ovb}}

Fortunately, we can salvage estimates that suffer from omitted variable bias to make causal conclusions in some cases. Again, doing so requires us to have knowledge about the variables involved and their causal pathways.

We will cover this more thoroughly in the following chapters, but a statistically significant result means we can confidently conclude the association between X and Y is not equal to zero. In other words, our regression results provide an estimate so much less or greater than zero that it would be highly unlikely to see these results if the true association were equal to zero.

That is the issue with OVB: it causes our estimate to be lower or higher than what it should be. Therefore, OVB may lead us to conclude statistically significant results when otherwise there would not be statistically significant results in the absence of OVB; our estimate would not be sufficiently far from zero to confidently conclude a relationship.

However, if we can predict the direction of the OVB--whether it is pushing our estimate below or above what it should be--then we may be able to salvage our results. For instance, suppose we obtain a statistically significant estimate of 10 that we suspect is biased due to an omitted variable. If we can credibly claim the OVB causes our estimate to be lower than what it should be, then we still have useful results; our result would be even greater than 10 if not for OVB. Similarly, if our estimate were -10 and we suspect the OVB causes our estimate to be greater than what is should be, then we would have an even lower estimate in the absence of OVB.

The moral of this section is that when you or someone identifies a variable that may cause OVB, all is not lost. If OVB works against the estimate's value relative to zero, then it actually lowers the likelihood of significant results that you still obtained. However, if OVB works with the estimate's value relative to zero, then it increases the likelihood of finding the significant results you found when there may not be a significant relationship.

How can we postulate the direction of OVB? Figure \ref{fig:dagovbdirect} below shows a simple confounding scenario where our estimate of the effect of X on Y is biased due to an omitted variable Z. If X and Y move in the same direction because of a change in Z, then OVB is positive. If X and Y move in opposite direction because of a change if Z, then OVB is negative.

\begin{figure}
\includegraphics[width=6.35in]{images/dag_ovbdirect} \caption{Predicting direction of OVB}\label{fig:dagovbdirect}
\end{figure}

\hypertarget{usefulness}{%
\section{Usefulness}\label{usefulness}}

Just because you cannot claim a causal relationship does not mean you do not have useful results. It is worth reiterating the value of prediction. Regardless of whether we have a model that eliminates OVB, if it accurately predicts an outcome we would like to preempt, then it could be useful. Sure, we would like to know the underlying causes of an outcome, but the ability to accurately predict the likelihood of an outcome still allows policy or programs to intervene.

If I have a model that is biased but accurately predicts students who will drop out of college, I will use that model to provide assistance to those likely to drop out. In addition to making a difference by helping my target population, perhaps I will gain insights into the underlying causes I have failed to include in my model.

\hypertarget{kt9}{%
\section{Key terms and concepts}\label{kt9}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Internal validity
\item
  External validity
\item
  Establishing correlation
\item
  Temporal precedence
\item
  Reverse causality
\item
  DAG

  \begin{itemize}
  \tightlist
  \item
    confounder
  \item
    collider
  \item
    backdoor path
  \item
    backdoor criterion
  \end{itemize}
\item
  Omitted variable bias
\end{itemize}
\end{learncheck}

\hypertarget{part-inference}{%
\part{Inference}\label{part-inference}}

\hypertarget{sampling}{%
\chapter{Sampling}\label{sampling}}

\begin{quote}
\emph{``This is what I'm learning, at 82 years old: the main thing is to be in love with the search for truth.''}

---Maya Angelou
\end{quote}

We now turn our attention to inference, which involves taking a sample from a population to make conclusions about the population with some degree of certainty. At its foundation, inference is about the search for truth. Specifically, when we calculate some estimate using a sample of data, we use inference to say whether that estimate is a good guess of the unobserved population parameter. Rather than focus on sampling techniques (e.g.~random, clustered, stratified, convenience), this chapter focuses on the theory that allows us to use samples for inference as well as the potential limitations of doing so.

\hypertarget{lo10}{%
\section{Learning objectives}\label{lo10}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Explain the 68-95-99 rule and apply it given the mean and standard
  deviation of a normal distribution
\item
  Explain how a sampling distribution is constructed
\item
  Explain why the Central Limit Theorem is needed to conduct inferential
  statistics and how it allows us to do so
\item
  Given an estimate and standard error, construct 95 and 99 percent
  confidence intervals
\item
  Interpret confidence intervals
\item
  Explain the effect of random or biased sampling on the accuracy and
  precision of a sample estimate and sampling distribution
\item
  Explain the effect of sample size on the accuracy and precision of a
  sample estimate and sampling distribution
\end{itemize}
\end{learncheck}

\hypertarget{normal-distribution}{%
\section{Normal distribution}\label{normal-distribution}}

When we calculate an estimate, the estimate is highly unlikely to be exactly equal to the population parameter it is intended to represent. But, given a sample and an estimate, we can calculate the range in which the parameter falls with a certain degree of confidence. If that range does not include zero, then we have reason to believe the parameter is positive or negative. A non-zero parameter may be cause for action. Or, depending on the situation, a parameter of zero may be cause for action. We are able to calculate a confidence interval because of the normal distribution.

Inference relies on the normal distribution introduced in Chapter \ref{descriptive-statistics}. The normal distribution has a unique and useful quality such that wherever the mean of a variable's distribution lies, if the distribution of the variable is normal, then 68\% of the values lie within one standard deviation above and below that mean, 95\% of the values lie within two standard deviations above and below, and 99\% lie within three standard deviation. This quality of the normal distribution is sometimes called the \textbf{68-95-99 rule}, which is shown in Figure \ref{fig:normdist} below. The Greek symbol \(\mu\) (mû) denotes the mean and the symbol \(\sigma\) (sigma) denotes the standard deviation.

\begin{figure}
\includegraphics[width=9.56in]{images/normdist} \caption{68-95-99 rule of normal distribution}\label{fig:normdist}
\end{figure}

Suppose we take a sample of 397 professor salaries in order to estimate the average salary of all professors at the university. From our sample, we calculate a mean of 113706 dollars and a standard deviation of 30289 dollars. Figure \ref{fig:profsaldist} below shows the distribution of this sample of salaries.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/profsaldist-1.pdf}
\caption{\label{fig:profsaldist}Distribution of professor salaries}
\end{figure}

Note that the distribution of salaries is roughly normal-looking, though it is skewed somewhat to the right. If the distribution of 397 salaries were perfectly normal, then 68\% of those salaries fall between 83417 and 143995, 95\% percent of the 397 salaries fall within 53128 and 174284, and 99\% of the 397 salaries fall within 22839 and 204573. However, since we can observe the entire distribution, we can calculate the range in which 95\% of values fall or any percentage of values. The exact 95\% range for Figure \ref{fig:profsaldist} is 70,761 to 181,511.

These ranges are merely descriptions of our sample just like the measures used in Chapter \ref{descriptive-statistics}. We have not yet made any inference about the salaries of all professors at the university.

Again, it is highly unlikely that the average salary of all professors at the university equals 113706 dollars exactly. Our estimate is a guess of something we do not directly observe. Naturally, we want to know a range in which we can be reasonably confident the average salary of all professors falls. This range is called a \textbf{confidence interval}. However, unlike the distribution of 397 salaries, we only have one estimate from our one sample of salaries. How can we calculate a confidence interval of something we do not observe? To construct a confidence interval, \textbf{we \emph{assume} the \emph{sampling} distribution of the mean of salaries is normal}.

\hypertarget{sampling-distribution}{%
\section{Sampling Distribution}\label{sampling-distribution}}

Distributions were covered in Chapter \ref{descriptive-statistics}. A variable is comprised of multiple values that can be plotted along a number line or axis to form a distribution. This distribution can be described in terms of its center via the mean and its spread via the standard deviation.

A sampling distribution is simply a distribution comprised of multiple estimates, each taken from a separate sample, instead of multiple values, each taken from a separate unit of analysis. Imagine if the distribution in Figure \ref{fig:profsaldist} were made of 397 averages from 397 samples of salaries. If we have no reason to suspect our estimates are systematically above or below the population mean (i.e.~unbiased), then we have a distribution of guesses for the population mean, the center of which should approach the population mean given enough sample estimates. Assuming this sampling distribution is normally distributed, then we can construct an interval in which 95\% of the estimates fall as a plausible range in which the unobserved population mean falls.

This tends to be a large theoretical leap for many to make. To reiterate, we have only one sample, not 397. How do we construct a 95\% confidence interval off of a sampling distribution we do not have the data to observe? We do so using theory and assumptions. Most importantly, we use the \textbf{Central Limit Theorem}.

\hypertarget{central-limit-theorem}{%
\section{Central Limit Theorem}\label{central-limit-theorem}}

The Central Limit Theorem may seem like magic more than anything else in statistics, though it is scientifically sound. Given a sufficient sample size, the Central Limit Theorem allows us to assume sampling distributions are normally distributed even though we do not have data to observe the sampling distribution. Without it, we could not construct confidence intervals. Thus, we could not make inferences about a population.

Seeing the Central Limit Theorem work is believing, especially when circumstances are set that would seem to work against it. To do this, let us revisit the distribution of a six-sided die discussed in Chapter \ref{descriptive-statistics}. With each of the six values having an equal probability of occurring, we know each value has about a 17\% chance of occurring. If we were to roll the die some number of times divisible by 6, then all values should occur the same number of times, resulting in a distribution like that depicted in Figure \ref{fig:dieuniform}.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/dieuniform-1.pdf}
\caption{\label{fig:dieuniform}Probability distribution of a six-sided die}
\end{figure}

The distribution in Figure \ref{fig:dieuniform} is decidedly not normal. Yet, the Central Limit Theorem states that if we took numerous samples from this distribution, each of sufficient sample size, then the sampling distribution will be normal. We typically do not know the distribution of a variable like we do with a six-sided die, thus we do not know where an important measure like the mean is in that distribution. However, if any estimate regarding any variable--no matter the distribution of the variable--is normally distributed, then we do not need to know the distribution of the variable. This is the power and importance of the Central Limit Theorem.

To see how the Central Limit Theorem works, we need a population we can observe but would not be able to in usual circumstances. Suppose we had a population comprised of 10,000 observations. Each observation is the result of rolling a six-sided die. This is obviously a play example for the sake of instruction, but one could imagine the six values of the die to be something more interesting and important, such as levels of education. Table \ref{tab:diepop} below shows a preview of our simulated population.

\label{tab:diepop}Preview of simulated population from uniform distribution

ID

education

1

2

2

6

3

4

9998

4

9999

6

10000

4

Since we have the entire population, we can calculate the population mean, which would typically be a population parameter we cannot calculate. The mean ``education level'' of this population is 3.518. This is almost exactly equal to the mean we should expect from many rolls of a six-sided die. The distribution of the population's education is shown in Figure \ref{fig:diepophist}. The solid red line represents the population mean.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/diepophist-1.pdf}
\caption{\label{fig:diepophist}Distribution of simulated population}
\end{figure}

Suppose we were to draw one random sample of 20 from this population. The distribution of this sample is shown in Figure \ref{fig:diesamp201}. The mean of this sample is 3.65, represented by the purple dashed line. The red solid line represents the population mean of 3.518.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/diesamp201-1.pdf}
\caption{\label{fig:diesamp201}Distribution of sample of 20 from simulated population}
\end{figure}

The sample mean of 3.65 may or may not be a good guess of the population mean of 3.518; such judgments depend on the context of the research question or the decision needing made. Suppose we take a second random sample of 20 from the population, as shown in Figure \ref{fig:diesamp202}. The mean of this second sample is 3.95.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/diesamp202-1.pdf}
\caption{\label{fig:diesamp202}Distribution of a second sample of 20 from simulated population}
\end{figure}

Note that the distribution of the two samples are different and neither are uniform like the population distribution. This is the manifestation of randomness. Each sample gives us a different estimate of the population mean, both of which are too high. Estimates of other samples would fall below the population mean. With enough samples and sample estimates of the mean, we can construct a sampling distribution.

Suppose we take 100 samples of 20 from the population, estimating the mean of each sample to construct a \emph{sampling} distribution of the mean. This sampling distribution is shown in Figure \ref{fig:diesampdist20100}. Note that with only 100 samples of only 20 observations each, the sampling distribution roughly resembles the normal distribution. Also, the mean, or center, of the sampling distribution represented by the yellow line is very close to the population mean.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/diesampdist20100-1.pdf}
\caption{\label{fig:diesampdist20100}Sampling distribution of 100 sample means from samples of size 20}
\end{figure}

The centering of the sampling distribution at the population mean is due to having an unbiased estimate from random sampling. If our estimate is unbiased, then we expect it to equal the population parameter, \emph{on average}. This applies to any estimate and its potential bias, including the estimates in regression. If our regression model is unbiased, then our estimate comes from a sampling distribution that centers at the population parameter.

Now let's take 10,000 samples with a size of 33 observations each, calculating the mean of each sample. The sampling distribution of the 10,000 sample means is shown in Figure \ref{fig:diesampdist33} below. Note that it looks very similar to the normal distribution with a mean equal to 3.157. From a variable that is not normally distributed, we obtain a sampling distribution that is normal. No matter the distribution of the variable, the Central Limit Theorem assures us its sampling distribution will be normal, provided we have a large enough sample.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/diesampdist33-1.pdf}
\caption{\label{fig:diesampdist33}Sampling distribution of 10,000 sample means from samples of size 33}
\end{figure}

In reality, we cannot draw 10,000 samples from the population. If we could, and calculated the mean of the sampling distribution, then we could be \emph{extremely} confident that we have estimated the population parameter with virtually perfect accuracy. Alas, we have only one sample and no sampling distribution to observe. Though we can safely assume our one estimate comes from a normal sampling distribution that, if there is no bias, is centered at the population mean, we do not know where \emph{our} sample falls within that distribution. Our one sample could be one with an estimate in or near the left or right tails of the sampling distribution in Figure \ref{fig:diesampdist33}. Therefore, we need a range of plausible values for the population parameter. For this range we need to measure the spread of the sampling distribution in terms of its standard deviation.

\hypertarget{confidence-intervals}{%
\section{Confidence intervals}\label{confidence-intervals}}

The standard error is used to construct confidence intervals. The standard error is essentially the same as the standard deviation. It is the name given to the standard deviation of a sampling distribution instead of a variable's distribution. Now that we can safely assume our sample estimate comes from a normal sampling distribution, and given that we need to account for the randomness of any one sample, we can calculate the range of values within which 95\% or 99\% of the estimates in the sampling distribution falls. In short, we have returned to applying the 68-95-99 rule, only this time we use it to construct a confidence interval.

The 95\% confidence interval for any estimate is 2 standard errors (1.96, technically) below and above the estimate. The 99\% confidence interval is about 3 standard errors below and above the estimate. Referring back to the sampling distribution in Figure \ref{fig:diesampdist33}, the standard deviation is equal to 0.26. Suppose we drew one sample that gave us an estimate of 4. Suppose the standard error of that estimate happened to equal 0.26. In that case, the 95\% confidence interval is approximately 3.48 to 4.52. Since we know the population parameter equals 3.518, we know our confidence interval captures it, which is what we hope to be the case but cannot confirm in typical circumstances.

How do we calculate the standard error to construct the 95\% confidence interval or any confidence interval without observing the sampling distribution? Again, theory and assumptions. Throughout this running example, we have been trying to estimate the mean of the population. The standard error (SE) of a sample mean is calculated using the following equation

\begin{equation}
SE = \frac{s}{\sqrt{n}}
\label{eq:semean}
\end{equation}

where \(s\) is the standard deviation of the variable in our sample data and \(n\) is the number of observations in our sample.

Equation \eqref{eq:semean} highlights one of the reasons sample size is a point of interest in analysis. In addition to needing at least 33 observations for the Central Limit Theorem to work reliably, sample size affects the precision of our confidence interval. As \(n\) increases, the denominator in Equation \eqref{eq:semean} increases. Given a standard deviation, greater denominator results in a smaller SE than a lesser denominator. That is, as our sample size increases, the range of our confidence interval decreases.

To demonstrate the effect of sample size on precision, suppose we drew 10,000 samples of size 1,000 instead of 33, as was done for the sampling distribution in Figure \ref{fig:diesampdist33}. Figure \ref{fig:diesampdist1000} depicts the sampling distribution of this hypothetical scenario. Not that the distribution is virtually identical to normal. Of most importance is the spread of the sampling distribution. The standard deviation of the sampling distribution (or standard error) in Figure \ref{fig:diesampdist33} is 0.26. The standard error of the sampling distribution in Figure \ref{fig:diesampdist1000} is equal to 0.05.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/diesampdist1000-1.pdf}
\caption{\label{fig:diesampdist1000}Sampling distribution of 10,000 sample means from samples of size 100}
\end{figure}

From a sample of size 1,000 it is much less likely that we would obtain a sample estimate as far from the parameter of 3.518 as 4. Moreover, whatever our estimate, we can construct a confidence interval with the same level of confidence that will be much smaller. A more precise confidence interval may allow for more confident decision-making.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Though in reality we obtain only one estimate from one sample, we assume that estimate is one value of a normally distributed sampling distribution that is centered at the population parameter we intended to estimate. Our one sample estimate is highly unlikely to equal the population parameter because of randomness. Therefore, in addition to our specific estimate of the parameter, we construct a range of plausible values that captures that population parameter.

To walk through the full process of estimation using one sample in this simulated data example, one of the 10,000 samples of size 1,000 (sample 379) had a mean education level of 3.552. The standard deviation of education in this sample was 1.53. Given 1,000 observations, then the standard error is equal to

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{1.53}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.04838285
\end{verbatim}

Therefore, assuming a normal sampling distribution and absence of bias, the 95\% confidence interval for our estimate of the population mean of education is

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{3.552}\OperatorTok{-}\NormalTok{(}\FloatTok{1.96}\OperatorTok{*}\FloatTok{0.048}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.45792
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{3.552}\OperatorTok{+}\NormalTok{(}\FloatTok{1.96}\OperatorTok{*}\FloatTok{0.048}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.64608
\end{verbatim}

The 95\% confidence interval of this particular sample captures the population parameter of 3.518.

A common interpretation of, say, a 95\% confidence interval is that our population parameter has a 95\% probability of falling within our confidence interval. This is incorrect. A confidence interval either contains the population parameter or it does not. There is no 95\% probability to speak of; only 0\% or 100\%. What a confidence interval conveys is that if we were to draw numerous samples from this population rather than just the one, then we would expect 95\% of the confidence intervals constructed from all of our samples to capture the population parameter and 5\% of the confidence intervals to fail. Our sample could be one of those 5\% of samples for which the confidence intervals fail to capture the population parameter.

One out of 20 samples are expected to fail to capture the population parameter that it was intended to estimate. This is why many have concerns regarding a crisis of replication in science. If only one study is published, and replications of a study are difficult to have published, then we do not know if the one that was published is the anomaly or not.

\hypertarget{kt10}{%
\section{Key terms and concepts}\label{kt10}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Normal distribution
\item
  68-95-99 rule
\item
  Sampling distribution
\item
  Sample estimate
\item
  Central Limit Theorem
\item
  Confidence intervals
\item
  Standard error
\item
  Accuracy of sampling distribution and estimate
\item
  Precision of sampling distribution and estimate
\end{itemize}
\end{learncheck}

\hypertarget{surveys-and-evaluations}{%
\chapter{Surveys and Evaluations}\label{surveys-and-evaluations}}

\begin{quote}
\emph{``A census taker once tried to test me. I ate his liver with some fava beans and a nice chianti.''}

---Hannibal Lecter
\end{quote}

Before considering inference with respect to regression, this chapter covers two common concerns in sampling beyond question design or sampling method: sample power and weighting. Then, hypothesis testing is introduced, which applies to any sample estimate from which we wish to make inference. Lastly, using hypothesis testing for evaluations in their simplest forms is considered.

\hypertarget{lo11}{%
\section{Learning objectives}\label{lo11}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Apply the margin or error or the confidence interval of an estimate
  when making conclusions
\item
  Compute the sample size required to achieve a desired margin of error
  or precision around an estimate given the necessary information for
  such a computation
\item
  Weight survey results given sufficient information
\item
  Identify or construct the null and alternative hypotheses of a
  research question
\item
  Interpret a p-value and apply it to determine the outcome of a
  hypothesis test
\item
  Distinguish between types I and II error in a research scenario
\item
  Explain when a chi-square test or t-test is appropriate
\end{itemize}
\end{learncheck}

\hypertarget{sample-size}{%
\section{Sample size}\label{sample-size}}

Now that we understand how a sample of relatively small size allows us to make inferences about the population with a reasonable degree of confidence, let us next consider how to determine the size of sample we need to achieve a confidence interval with a specific degree of precision.

When President Trump's impeachment inquiry was in progress, numerous polls were conducted to gauge the sentiment of the U.S. electorate as to whether Trump should be impeached. One poll conducted by Fox News, using a sample of 1,000 voters, reported that 51\% of voters support impeaching Trump with a margin of error of 3 percentage points. These results garnered national attention as the first time a majority of U.S. voters supported the impeachment of Trump. Regardless of one's opinion on the matter or whether a national majority persuades elected officials, the claim that a majority of voters supported impeachment based on this poll is dubious.

A sample of U.S. voters was taken. From this sample, the proportion of voters in support of impeachment was calculated to serve as an estimate of the population parameter. This sample produced an estimate of 51 percent. The margin or error in surveys or polls, unless noted otherwise, refers to one-half of the 95\% confidence interval, or roughly two standard errors. Therefore, the 95\% confidence interval of the polling results was 48 to 54 percent. The confidence interval used to capture the unobserved population parameter includes a minority of voters supporting impeachment.

A common mistake made when interpreting estimates and confidence intervals is that the estimate is the most likely value within the confidence interval for the population parameter. \textbf{The population parameter is no more likely to equal the estimate than any other value within the confidence interval}. A confidence interval either captures the population parameter or it does not. Therefore, it was just as likely that 48 percent of voters supported the impeachment as it was 54 percent did or any percentage in between.

As long as our estimate is unbiased, we cannot influence its value. Any attempt to do so would be bias by definition. Thus, the pollsters were stuck with an estimate of 51 percent. The estimate of 51 percent was not the issue for making the conclusion that a majority of voters supported impeachment. The issue was the critical lack of precision around the estimate. Unlike the estimate, we \emph{can} influence the precision of the confidence interval.

How many voters would the pollsters have had to survey in order to achieve a margin of error of 1 percentage point and conclude a majority of voters support impeachment?

If the outcome is dichotomous or binary, such as whether or not a respondent supports impeachment, then the equation for determining the desired sample size is as follows

\begin{equation}
n=p(1-p)({\frac{Z}{E}})^2
\label{eq:sampsizeprop}
\end{equation}

\begin{itemize}
\tightlist
\item
  n is the sample size
\item
  p is the proportion of yes/true/success
\item
  Z is the number of standard deviations we set according to what confidence interval is desired
\item
  E is the desired margin of error
\end{itemize}

It is important to point out that the calculation in Equation \eqref{eq:sampsizeprop} occurs \emph{prior} to the poll. Therefore, we do not know the value of \(p\). Unless there is reason to expect \(p\) to equal a particular proportion, it is customary to input 0.50. If we want to use a 95\% confidence interval then we input 2 for \(Z\). Lastly, we replace \(E\) with how far we want each side of our chosen confidence interval to be below and above our estimate.

Suppose the primary purpose of the Fox News poll was to conclude a majority opinion. Then, a margin of error equal to 1 percentage point would allow a valid conclusion that a majority of voters support or oppose impeachment if the result of the poll were slightly below 49\% or above 51\% in favor. Choosing to use a 95\% confidence interval, then the sample size necessary to achieve a margin of error equal to 1 percent (0.01) is

\begin{equation}
n=0.5(1-0.5)({\frac{1.96}{0.01}})^2\\
n=9,604
\end{equation}

which is likely impractical for the kind of flash or expedient polling that news organizations tend to conduct.

If we wish to determine the sample size needed for estimating a 95\% confidence interval within a certain distance from a continuous estimate, such as the mean, we can use the following equation

\begin{equation}
n=(\frac{sZ}{E})^2
\label{eq:sampsizemean}
\end{equation}

where \(s\) is the sample standard deviation. Again, we do not have the sample standard deviation prior to obtaining the sample. We must rely on past analyses of the variable in question or a pilot study with a small sample in order to input the sample standard deviation.

\hypertarget{survey-weights}{%
\section{Survey weights}\label{survey-weights}}

Ideally, the demographic composition of survey respondents should match the demographic composition of the survey's intended population. Thanks to Census data, we have a reasonably accurate understanding of population demographics such as age, sex, race, and ethnicity for multiple geographic areas and government jurisdictions. Other organizations like Pew Research Center and Gallup provide population proportions of other various ways to form groups, such as political party or religious affiliation.

Unfortunately, it is unlikely for the demographic composition of survey respondents to match the population. Recipients choose not to respond and surveys tend to reach some demographics disproportionately more than others. This results in over- or under-representation of certain demographic groups in our survey, which limits our ability to generalize survey results and threatens the internal validity of any estimate.

Weighting is a way to correct for a demographic mismatch between the composition of respondents and the intended population. There are multiple methods of weighting, some of which are complex, but the basic method described below can work for most cases where maximum correction is not necessary or feasible.

Suppose a poll targeted to the general U.S. public asked if workers who have illegally entered the U.S. should be 1) allowed to keep their jobs and apply for citizenship, 2) allowed to keep their jobs as temporary guest workers but not allowed to apply for citizenship, and 3) lose their jobs and have to leave the country. The poll also asked for political party affiliation. A total of 890 responses were collected, generating the following results.

\label{tab:unnamed-chunk-75}Illegal immigration poll results

response

party

Apply for citizenship:278

Republican :357

Guest worker :262

Democrat :174

Leave the country :350

Independent:359

Based on the results, a plurality of 39\% of the U.S. public believes illegal immigrants should leave the country and 31\% believe they should be allowed to apply for citizenship. The question these estimates are biased by the composition of political party affiliation. About 40\% of the respondents are Republican and Independent, while about 20\% are Democrat. Suppose we find a national survey reporting that the U.S. is 30\% Republican, 36\% Independent, and 31\% Democrat. Therefore, Republicans and Independents are over-represented in our survey, while Democrats are under-represented. We need to correct for this using weights.

To calculate weights, we can use the following formula.

\begin{equation}
Weight = \frac{Population}{Sample}
\label{eq:weight}
\end{equation}

Using Equation \eqref{eq:weight}, we obtain the following weights for our survey

\begin{itemize}
\tightlist
\item
  Republican: 30/40 = 0.75
\item
  Independent: 36/40 = 0.9
\item
  Democrats: 31/20 = 1.55
\end{itemize}

These weights mean that each Republican response counts as only three-quarters of a response and each Democrat response counts as about 1.5 responses.

Next, we need to tabulate how many of each response was made by the three parties.

\label{tab:pollparty}Response by political party

Republican

Democrat

Independent

Apply for citizenship

57

101

120

Guest worker

121

28

113

Leave the country

179

45

126

Then, we multiply the values by their corresponding weight. For example, applying the weight for Republicans results in 134 responses for ``Leave the country'' (\(179 \times 0.75\)). This process gives us the following counts

\label{tab:unnamed-chunk-77}Weighted survey counts

party

response

total

weight

w.total

Republican

Apply for citizenship

57

0.75

43

Republican

Guest worker

121

0.75

91

Republican

Leave the country

179

0.75

134

Democrat

Apply for citizenship

101

1.55

157

Democrat

Guest worker

28

1.55

43

Democrat

Leave the country

45

1.55

70

Independent

Apply for citizenship

120

0.90

108

Independent

Guest worker

113

0.90

102

Independent

Leave the country

126

0.90

113

According to our weighted counts, 36\% of the U.S. believes illegal immigrants should leave the country, while 35\% believe they should be allowed to apply for citizenship. Weighting has changed a 8 percentage point gap in these two responses to a 1 point gap.

In case it was not obvious in the example, we have to ask survey recipients to provide the demographic information upon which we plan to weight. Survey administrators must consider what variables might bias the response(s) of interest if there is a mismatch between the sample and population. Wisely, the designers of the survey above suspected if a disproportionate number of Republicans or any other political party responded, this would bias their estimates. Presumably, race and ethnicity are correlated with this response, but we do not have this information to construct a weight.

\hypertarget{hypothesis-testing}{%
\section{Hypothesis testing}\label{hypothesis-testing}}

At its foundation, hypothesis testing is a simple procedural process. It does involve some application of statistical theory, the most fascinating and misunderstood aspect of which is the p-value. This section covers how to set up and use a hypothesis test to determine if an estimate is statistically significant.

\hypertarget{null-and-alternative-hypos}{%
\subsection{Null and alternative hypos}\label{null-and-alternative-hypos}}

Setting up a hypothesis test first involves establishing two mutually exclusive, competing statements:

\begin{itemize}
\tightlist
\item
  Null hypothesis: the condition I intend to test for is not true, not the case
\item
  Alternative hypothesis: the condition I intend to test for is true, is the case
\end{itemize}

The condition is based on our research question that warranted the analysis in the first place. For example, is the average age of MPA students less than the average age of all graduate students? Are females more likely to enroll in an MPA program than males? Does obtaining an MPA increase earnings?

Based on our question, we make a hypothesis \emph{before} computing an estimate that would answer the question. For example, the average age of MPA students is less than the average age of all graduate students. Females are more likely to enroll in an MPA program than males. The effect of attaining an MPA increases earnings. These examples are alternative hypotheses; the affirmative of the condition we set out to test. The null hypothesis is the negative of the condition. For example, average age of MPA students is equal to graduate students. The likelihood of enrolling in an MPA program are equal between males and females. An MPA degree does not increase earnings.

Note that the examples of alternative hypotheses were all directional. They used words like less/more than or increase/decrease. An alternative hypothesis need not be directional even though we may expect one direction over the other. For example, our alternative hypotheses could be that average age differs between MPA students and other grad students, the likelihood of enrolling differs between males and females, and attaining an MPA effects income.

I encourage students to stick with non-directional hypotheses. In additional to simplifying the analysis, doing so reduces the likelihood that we report a statistically significant result when in fact there is not one (i.e.~false positive). A strong case can be and is made that the statistical analyses allow too high of a likelihood for false positives. Claiming a direction means we have theoretically ruled out half of the possible results of our analysis before we even conduct the test, thereby increasing the likelihood we get the results we expected. If we are able to do this, one might wonder why conduct the test in the first place.

Translating the above into more mathematical concepts, most research questions involve differences: the difference in mean age between MPA students and other grad students, the difference in the proportions of female and male MPA graduates, the difference in the slopes of regression lines drawn through data points for income and education level or degree type.

Thus, the null hypothesis states that any of these differences equals zero. The alternative hypothesis states that any of these differences does not equal zero. The null hypothesis is denoted by \(H_0\) and the alternative hypothesis is denoted by \(H_A\).

\begin{itemize}
\tightlist
\item
  \(H_0: \mu_{MPA}-\mu_{grad} = 0\) or \(H_A: \mu_{MPA}-\mu_{grad} \neq 0\)
\item
  \(H_0: \rho_{female}-\rho_{male} = 0\) or \(H_A: \rho_{female}-\rho_{male} \neq 0\)
\item
  \(H_0: \beta_{MPA} = 0\) or \(H_A: \beta_{MPA} \neq 0\)
\end{itemize}

Note the use of population parameters above. Again, inference computes a sample estimate to make inferences about a population. Therefore, our hypotheses include the unobserved population parameter. Our estimate and confidence interval represents our best guess of that population parameter. The purpose of the hypothesis test is to establish a threshold at which we are sufficiently confident in our results to make a conclusion concerning our hypotheses \emph{prior} to viewing the results.

\hypertarget{conclusion-and-error-type}{%
\subsection{Conclusion and error type}\label{conclusion-and-error-type}}

There are two possible outcomes of a hypothesis test. We either

\begin{itemize}
\tightlist
\item
  reject the null hypothesis, or
\item
  fail to reject the null hypothesis.
\end{itemize}

We never accept the null hypothesis or reject the alternative hypothesis. This may seem like semantics, but it actually has important implications for our conclusions.

Suppose our results do not meet the threshold to reject the hypothesis, thus leading us to fail to reject the null hypothesis. This does not mean the null hypothesis is true. Our null hypothesis states whatever parameter of interest in our study equals zero. We do not observe the population parameter. Therefore, we cannot say that our null hypothesis is true. Instead, we say that we do not have sufficient evidence to reject the null. It may be true or false, but we cannot determine which based on our particular estimate from our particular sample.

Conversely, if we reject the null, then our conclusion is that the null hypothesis is false. We can rule out with reasonable confidence that the population parameter does not equal zero because doing so does not require us to claim a particular value for the population parameter; just that it is not equal to zero.

A popular example of hypothesis testing is a jury decision in a court case. A defendant is accused of committing a crime. In truth, the defendant is either innocent or guilty of that crime. Ideally, the defendant is presumed innocent until proven guilty according to a jury of their peers. Therefore, the null hypothesis is that the defendant is innocent and the alternative hypothesis is that the defendant is guilty. The jury decides either that the defendant is guilty or not guilty. If guilty, then the jury has rejected the null hypothesis of innocence. If not guilty, then the jury has failed to reject the null hypothesis. Note that the jury does not decide the defendant is innocent, which would be equivalent to accepting the null or rejecting the alternative.

Despite our best efforts, there always remains some chance that we have reached the wrong conclusion with our hypothesis test. We can make one of two possible errors:

\begin{itemize}
\tightlist
\item
  Type I error or false positive: rejecting the null when the null is actually true
\item
  Type II error or false negative: failing to reject the null when the null is actually false
\end{itemize}

For instance, Type I error is finding an innocent defendant guilty, a healthy patient sick, or an ineffective program effective. Type II error is finding a guilty defendant not guilty, no evidence of illness in a sick patient, or no evidence of efficacy in an effective program. Again, the null hypothesis involves the population parameter, so we do not \emph{know} if it is actually true or not. The null must be true or false, and the outcome of our hypothesis test claims whether the null does not appear to be false or is false. Therefore, there is a probability that we have reached the incorrect conclusion.

It is impossible to eliminate the chance of Types I and II errors, though it is possible to increase or decrease their likelihoods. However, the two share an inverse relationship; as we reduce the chance of one type of error, we increase the chance of the other type of error. Depending on the context of our research question, we may be more or less concerned about Type II error, but the focus of a hypothesis test is placed on Type I error, which serves as the threshold for our decision.

\hypertarget{decision-rule}{%
\subsection{Decision rule}\label{decision-rule}}

Before testing our hypothesis, we ask ourselves the following question: ``What is the maximum probability of Type I error that I or others should be willing to tolerate?'' Actually, this question has been answered for us in most disciplines. The common threshold for this tolerance is 5\% or 1\% probability of rejecting the null hypothesis when the null is actually true. Social sciences typically use 5\%.

With our threshold set, we can now test our hypothesis. We calculate the sample estimate and the standard error of our estimate, which are used to calculate the confidence interval. The confidence level of our confidence interval depends on our chosen threshold for Type I error. If our threshold for Type I error is 5\%, then we calculate the 95\% confidence interval. If our threshold is 1\%, we use a 99\% confidence interval.

Our confidence interval is our best guess of the plausible range of values for the population parameter. We have decided to tolerate the chance that our confidence interval is one of the five out of 100 confidence intervals--or 1 out of 100--expected to fail to capture the parameter. Our null hypothesis states that the parameter equals zero. Therefore, if our confidence interval does not contain zero, we reject the null hypothesis. If our confidence interval does contain zero, then we have failed to reject the null hypothesis because the parameter \emph{might} equal zero with a higher probability than we decided to tolerate.

In most cases, we do not need more information than the estimate, standard error, and confidence interval to make a decision regarding our hypothesis test. However, an analysis provides us an additional piece of information that allows us to arrive at the same conclusion but from a different perspective called the \textbf{p-value}.

\begin{quote}
The p-value tells us the probability of obtaining the estimate we did, or an estimate further away from the null hypothesis, if the null hypothesis were actually true.
\end{quote}

The p-value provides us a concise decision rule. The tolerance threshold we set is often referred to as the significance level and denoted by the Greek letter \(\alpha\) (alpha).

\begin{itemize}
\tightlist
\item
  If \(p<\alpha\), reject the null hypothesis.
\item
  If \(p\geq \alpha\), fail to reject the null hypothesis.
\end{itemize}

Again, a typical value for \(\alpha\) is 5\%, or 0.05. Having chosen a 5\% significance level, if our results generate a p-value of 0.04, for instance, then the likelihood of obtaining our result in a world where the null is true is 4\%. Therefore, we reject the null hypothesis. If our p-value were equal to 0.06, then the likelihood of obtaining our result in a world where the null is true is 6\%. This exceeds our maximum tolerance, thus we fail to reject the null hypothesis.

\hypertarget{chi-square-test}{%
\section{Chi-square test}\label{chi-square-test}}

The Chi-square (\(\chi^2\)) test is a common choice for introducing the application of hypothesis testing. Chi-square is used to test whether two \emph{nominal} variables are associated to a statistically significant extent. A nominal variable, such as race, sex, or political party affiliation, has two or more of levels. If one wanted to test if, given the level for a unit of analysis in one nominal variable (e.g.~male), there is a higher likelihood for a particular level in another nominal variable to occur (e.g.~Republican), a Chi-square test is an appropriate choice.

For an example, let us return to the immigration survey. We saw in \ref{tab:pollparty} that 179 out of 357 Republicans (50\%) responded that illegal immigrants should be forced to leave the country, while 101 out of 174 Democrats (58\%) responded that illegal immigrants should be allowed to apply for citizenship. Is there a statistically significant pattern in responses conditional on political party affiliation, or are these differences due to random noise?

The null hypothesis for this question is that there is no association between the opinion on illegal immigration and political party affiliation. That is to say, if we chose any of the three political party levels in our data, the probability of an individual providing one of the three opinions is equal the other opinions, or

\(H_0: P_{leave} = P_{guest} = P_{citizen}\)

The alternative hypothesis is that there is an association between opinion on illegal immigration and political party affiliation, or

\(H_A\): at least one \(P\) is not equal to the others

Next, suppose we chose to use the customary 5\% statistical significance level, or \(\alpha=0.05\). Now we are ready to test our hypothesis using a Chi-square test. Doing so generates the following results.

\begin{verbatim}
## 
## 	Pearson's Chi-squared test
## 
## data:  immigration_poll
## X-squared = 100.95, df = 4, p-value < 0.00000000000000022
\end{verbatim}

This is one case where there are no confidence intervals to compute since our variables are not numerical. Instead, we rely on the p-value. Our p-value is less than 0.00000000000000022. More concisely, \(p<0.05\). Therefore, we reject the null hypothesis that there is no association between the three illegal immigration opinions in the survey and political party affiliation. Furthermore, the probability for us to get the values in Table \ref{tab:pollparty} or more extreme in a world where the null hypothesis is actually true is equal to an infinitesimal percent, not to suggest that such a small p-value is required to make inferences.

Our results allow us to make inferences such as Republicans are more likely to believe illegal immigrants should lose their jobs and have to leave the country, while Democrats are more likely to believe they should be allowed to keep their jobs and apply for citizenship. Not a particularly surprising inference, but perhaps that is because many such inferences in the past have been made using similar techniques and reported many times.

\hypertarget{t-test}{%
\section{T-test}\label{t-test}}

The t-test is another common introductory application of hypothesis testing. A t-test is used to test the association between a nominal variable with two levels and a numerical variable. It is frequently used in simple program evaluations with a pre/post or treatment/control design. Both involve a nominal variable with two levels. If one want to test if a numerical outcome is different between the two levels, then a t-test is an appropriation choice.

There are two varieties of the t-test. To test if an average of a numerical outcome is different between two groups, such as a treatment and control group, then we use an \textbf{independent t-test}. To test if an average numerical outcome is different before and after a treatment for the \emph{same} units of analysis, we use a \textbf{dependent t-test}. The difference between the two t-tests concerns how we approximate the sampling distributions and confidence intervals, but their use for hypothesis testing is essentially the same.

Suppose we work for a nonprofit that provides job training workshops and want to evaluate their effectiveness. One way to go about such a task is to compare the earnings of participants (i.e.~treatment group) to the earnings of non-participants (i.e.~control group). We have earnings data for 185 participants and 128 non-participants, some of which is previewed in the table below.

\label{tab:jobtraindata}Preview of job training data

treatment

earnings

1

66493.964

1

0.000

0

46651.829

1

10070.227

1

0.000

0

1211.736

Before constructing the null and alternative hypotheses, a note about the population in this example. In program evaluations or generally any analysis aimed toward testing whether some event caused an effect on an outcome of interest, there are two populations. There is the entire population of units of analysis (e.g.~all people, all nations, all dogs) and the subset of that population for whom/which the program, policy, or intervention is intended.

The choice of population leads to two slightly different questions. If the entire population is our research population, then our intent is to estimate the average effect of the program on a randomly chosen unit from the entire population. This is referred to as the \textbf{average treatment effect} (ATE). If the subset that the program targets is our research population, then our intent is to report the average effect of a randomly chosen targeted unit. This is referred to as the \textbf{average treatment on the treated} (ATT). The detailed differences between the two are beyond the scope of this book and more appropriate for a class in program evaluation or causal inference, but the existence of this difference is worth being aware of.

Given that job training programs are not intended for all people, the presumption is that we want to estimate the average effect on those the program targets. Of course, we could choose as our population only those who participated in the program. In that case, we need not bother with hypothesis tests, as we would be calculating the population parameters directly from the observed data. However, we would not be able to generalize the results.

With the population in mind, our null hypothesis is that the average earnings of participants is equal to the average earnings of non-participants, or

\(H_0: \mu_{treated} = \mu_{untreated}\)

Our alternative hypothesis is that the average earnings of participants is not equal to the average earnings of non-participants, or

\(H_A: \mu_{treated} \neq \mu_{untreated}\)

Choosing a significance level of 5\%, we are ready to test our hypothesis.

A simple computation of the average earnings between the two groups provides the following information.

\label{tab:jobtrainsum}Comparison of means between treated and untreated

treatment

Average Earnings

0

21645.10

1

26031.49

Participants have a higher average earnings than non-participants, but is this difference statistically significant? For that, we should use an \emph{independent} t-test because participants and non-participants are two different groups. Running the t-test provides the following results

\begin{verbatim}
## 
## 	Welch Two Sample t-test
## 
## data:  earnings by treatment
## t = -1.1921, df = 275.58, p-value = 0.2342
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -11629.708   2856.939
## sample estimates:
## mean in group 0 mean in group 1 
##        21645.10        26031.49
\end{verbatim}

Our p-value is greater than our significance level, \(0.23>0.05\). Therefore, we fail to reject the null hypothesis and conclude that there is not statistically significant evidence that participants earn more than non-participants, on average. Note that our 95\% confidence interval ranges between -11,630 and 2,856. It includes zero, thus we cannot claim the difference between the means is not equal to zero with reasonable confidence.

\begin{quote}
\textbf{To learn how to conduct chi-square and t-tests in R, proceed to Chapter \ref{r-evaluations}.}
\end{quote}

\hypertarget{kt11}{%
\section{Key terms and concepts}\label{kt11}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Margin of error
\item
  Survey weight
\item
  Null and alternative hypotheses
\item
  Rejecting the null
\item
  Failing to reject the null
\item
  Types I and II error
\item
  Confidence level
\item
  Significance level
\item
  P-value
\item
  Chi-square test
\item
  T-test
\end{itemize}
\end{learncheck}

\hypertarget{regression-inference}{%
\chapter{Regression Inference}\label{regression-inference}}

\begin{quote}
\emph{``One out of every four people is suffering from some form of mental illness. Check three friends. If they're OK, then it's you.''}

---Rita Mae Brown
\end{quote}

We can now apply our knowledge of inference to fully understand all of our regression results and extend our results to other questions. First, this chapter explains each column in a regression table as well as how to test additional hypotheses that standard regression results do not answer by default. Then, while inference is used to identify statistical significance, that does not necessarily mean our results are \emph{practically} significant. This chapter ends with how to determine the latter.

\hypertarget{lo12}{%
\section{Learning objectives}\label{lo12}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Explain and interpret the standard components in a table of regression
  results
\item
  Construct the null and alternative hypotheses of a variable in a
  regression model
\item
  Determine the outcome of the hypothesis test based on the regression
  results
\item
  Explain the consequence of choosing a significance level for a
  hypothesis test
\item
  Distinguish between statistical and practical significance
\item
  Determine whether results are practically significant
\end{itemize}
\end{learncheck}

\hypertarget{regression-table}{%
\section{Regression table}\label{regression-table}}

Chapters \ref{simple-and-multiple-regression}, \ref{categorical-variables-and-interactions}, and \ref{nonlinear-variables} presented numerous regression tables. These tables included the standard set of results that statistical programs provide by default. Below is one of the tables from Chapter \ref{categorical-variables-and-interactions} for a regression to explain traffic fatalities as a function of miles driven and U.S. region.

\label{tab:extable}Parallel slopes for regions

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

0.260

0.167

1.553

0.121

-0.069

0.589

vmiles

0.188

0.021

8.895

0.000

0.147

0.230

regionN. East

-0.076

0.065

-1.166

0.245

-0.204

0.052

regionSouth

0.519

0.056

9.283

0.000

0.409

0.630

regionWest

0.641

0.062

10.264

0.000

0.518

0.763

We have covered how to interpret the \texttt{estimate} column at length. This value is the sample estimate of our unobserved population parameter. Provided our model is unbiased, and because of the Central Limit Theorem, we assume that the value of our estimate was drawn from a sampling distribution that is approximately normal and a mean equal to the population parameter. We assume our estimate is the mean of that sampling distribution. For example, in Table \ref{tab:extable}, it is assumed the estimate for \texttt{vmiles} of 0.188 represents the mean of an unobserved sampling distribution comprised of numerous estimates for \texttt{vmiles} that would be obtained if we repeated the regression using numerous samples.

The \texttt{std-error} column is the standard error of the regression estimate (aka coefficient) in the same row. This value is an approximation of the standard deviation of the sampling distribution from which the estimate was drawn. For example, the standard error for \texttt{vmiles} of 0.021 represents the standard deviation of its sampling distribution.

We know the true population parameter is highly unlikely to exactly equal our estimate but is expected to fall somewhere within our sampling distribution. With our estimate assumed to be the center of the normal sampling distribution and the standard error its standard deviation, we can apply the 68-95-99 rule to construct a range of values that represent a percentage of the estimates within the sampling distribution. The common choices are 95\% and 99\%, with 95\% being the default in statistical programs.

The \texttt{lower\_ci} and \texttt{upper\_ci} columns provide the 95\% confidence interval. This range represents our best guess of the plausible values for the unobserved population parameter. The population parameter either falls within our confidence interval or it does not. There is \emph{not} a 95\% probability that the parameter falls within our confidence interval. Rather, it is one range that if we were to repeat the analysis many times using different samples to construct many confidence intervals, we expect 95\% of those ranges to successfully capture the population parameter. Therefore, the population parameter is no more likely to equal our estimate as it is to equal any value within our confidence interval.

The values for the confidence interval are obtained by subtracting and adding 1.96 standard errors from the estimate. In Table \ref{tab:extable}, the 95\% confidence interval for \texttt{vmiles} is 0.147 (\(0.188-2\times 0.021\)) to 0.230 (\(0.188+2\times 0.021\)). If we were to repeat this regression 20 or 100 times, we would expect 19 or 95 of the resulting confidence intervals to capture the population parameter for \texttt{vmiles}. Understanding this chosen rate of success/failure, 0.147-0.230 is our best guess of the range of plausible values for the true response in traffic fatalities to the average number of miles driven. It is just as likely that this true population parameter equals 0.147 or 0.230 as it is to equal 0.188.

The \texttt{statistic} and \texttt{p\_value} columns concern hypothesis testing. As a reminder, the regression model that produced Table \ref{tab:extable} was

\begin{equation}
mrall = \beta_0 + \beta_1vmiles + \beta_2region + \epsilon
\label{eq:exmodel}
\end{equation}

where \texttt{mrall} is the number of traffic fatalities in a state per 10,000 population.

If the purpose of our regression is to \emph{explain} traffic fatalities, then the inclusion of \texttt{vmiles} and \texttt{region} implies a research question along the lines of ``Do distances driven and region in a state affect traffic fatalities?'' Therefore, our regression model sets out to test whether \texttt{vmiles} and \texttt{region} has a statistically significant effect on \texttt{mrall}.

The standard null hypothesis for a regression model is no effect, or

\(H_0: \beta=0\)

and the alternative hypothesis is

\(H_A: \beta \neq 0\)

for each of the explanatory variables we include in the model. As we now know, our results will lead us to either reject the null hypothesis or fail to reject the null hypothesis for each explanatory variable.

If the null hypothesis were actually true, \(\beta=0\), for any of our explanatory variables, then its sampling distribution \emph{should} be centered at 0, not centered at the value of our estimate. For example, if \(\beta_1=0\) for \texttt{vmiles}, then its sampling distribution should have a mean of 0, not 0.188. This alternative distribution if the null were true is referred to as the \textbf{null distribution}. Just like the sampling distribution, we assume the null distribution is approximately normal.

The \texttt{statistic} and \texttt{p\_value} columns answer the following question: ``If the null for my explanatory variable were true, thus my estimate having a null distribution with a mean equal to zero and a standard deviation equal to the standard error of my estimate, how likely is it that I got the estimate I got?''

Specifically, the \texttt{statistic} column equals how many standard deviations or standard errors our estimate is away from the center of the null distribution, zero. The value is equal to the \texttt{estimate} divided by the \texttt{std\_error}. For example, the estimate for \texttt{vmiles} is 8.895 standard errors away from 0 (\(\frac {0.188}{0.021}\)). Assuming the null distribution is normal, how likely is it for us to get this estimate or one further away from 0 if the null were true? This is what the \texttt{p\_value} provides us. If only 5\% of the values in a normal distribution lie 3 standard deviations from the center, then an extremely small percentage of values must lie almost 9 standard deviations from the center. This is why the p-value for \texttt{vmiles} rounds to zero.

Supposing we chose a 5\% significance level prior to running the regression, our p-value for \texttt{vmiles} is statistically significant, meaning we reject the null hypothesis that \(\beta_1=0\). In other words, there is statistically significant evidence that the average number of miles driven by each driver in a state is associated (perhaps causes) with an increase in the state's traffic fatality rate.

Since \texttt{region} is a nominal variable with four categories, it results in three estimates as if each level was a separate dummy variable equal to 1 if a state is in that region and 0 otherwise. The three regions in Table \ref{tab:extable} are compared to the excluded region, Midwest. The null hypothesis is that there is no difference between the Midwest and another region of focus. Let us first focus on states in the West. On average, the traffic fatality rate for states in the West is 0.64 higher than states in the Midwest. The p-value is below 0.05, thus we can reject the null hypothesis and report this result as statistically significant.

By contrast, the traffic fatality rate for states in the Northeast is 0.08 less than states in the Midwest. However, the p-value is greater than 0.05. Therefore, we fail to reject the null hypothesis, meaning we cannot conclude with reasonable confidence that \(\beta_{Northeast} \neq 0\).

Note that every estimate for which the p-value is less than 0.05, its confidence interval does not contain zero. These two parts of the table will always agree because they answer the same question from slightly different angles. If we choose a 95\% confidence interval as our best guess of the plausible ranges for the population parameter, and that interval does not contain 0, then we must have obtained an estimate that, if the null were true, is so far away from 0 that the likelihood of getting it is less than 5\%. Had we chosen a 99\% confidence interval, or 1\% significance level, then for any interval that does not contain 0 the corresponding p-value is less than 0.01.

\hypertarget{other-hypotheses}{%
\section{Other hypotheses}\label{other-hypotheses}}

By default, the standard regression table addresses hypothesis tests of the form \(H_0: \beta=0\) and \(H_A: \beta \neq 0\). If our null hypothesis is \(\beta\) equals something other than 0, then we need to be careful because the \texttt{statistic} and \texttt{p\_value} columns do not apply. However, the confidence interval can still be used. If the confidence interval does not contain the value used for our null hypothesis, then we can conclude with our chosen level of confidence that the population parameter does not equal that value.

Comparing different levels within a categorical variable is slightly more complicated. In Table \ref{tab:extable}, we can conclude that states in the South and West are different than states in the Midwest, and we cannot conclude states in the Northeast are different than states in the Midwest. But, what if we wanted to compare states in the South to states in the West, or Northeast to South? Our regression and our results were not set-up to make such comparisons.

A quick and dirty way to make various comparisons across levels is to examine their confidence intervals. If the confidence intervals do not overlap or do not come close to overlapping, then we can be reasonably certain the population parameters for the two levels are not equal to each other. For example, the \texttt{upper\_ci} for Northeast is 0.052 and the \texttt{lower\_ci} for South is 0.4. The two intervals are separated by multiple standard errors. Therefore, it is probably safe to conclude they are different. By contrast, the confidence intervals for South and West overlap substantially. Therefore, it is not safe to conclude that South and West are different. Alternatively, we can tell our statistical software to exclude a specific level, thereby allowing us to test our hypothesis without the guesswork. Setting the reference level is covered in this chapter's corresponding R chapter.

Finally, every regression result includes a global hypothesis test of the form

\(H_0: \beta_0 = \beta_1 = \cdots = \beta_k = 0\)

\(H_A\): at least one \(\beta \neq 0\).

Keep in mind that all of our conclusions our probabilistic. There is always a chance of Type I and Type II error. Since the hypothesis test assumes a sampling distribution for each explanatory variable, each explanatory variable we add is like taking an additional sample from some underlying population relevant to our regression. At 95\% confidence, we \emph{expect} 1 out of every 20 intervals to fail at capturing the parameter, such as not including zero when the parameter is truly zero. The global hypothesis test above is a way of testing whether we got a significant result due to the random chance that 1 out of every 20 explanatory variables can be expected to be significant even if the null were true. This global hypothesis test is commonly referred to as an \textbf{F-test}. Its use and interpretation is covered in the R Chapter.

\hypertarget{practical-significance}{%
\section{Practical significance}\label{practical-significance}}

It is easy to lose sight of the forest for the trees when focusing on statistical significance. Just because we find a statistically significant relationship does not mean that relationship is practically significant or economically meaningful. Also, obtaining insignificant results does not necessarily mean you have results that are not important or worth reporting. Such distinctions between statistical and practical significance require an analyst or manager to have a broader sense of the underlying data and the context of the results.

After obtaining our results, asking ourselves three questions can help determine if our results are practically significant:

\begin{itemize}
\tightlist
\item
  What is the typical change in the explanatory variable associated with the statistically significant estimate?
\item
  Is the predicted change in the outcome due to a typical change in the explanatory variable negligible or meaningful?
\item
  If the explanatory variable is statistically significant, is its confidence interval so close to zero that using the upper or lower bound instead of the midpoint estimate would make the predicted change in the outcome negligible? If the explanatory variable is statistically insignificant, is its confidence interval so closely around zero that the entire range of plausible values of the parameter would lead to a negligible change in the outcome?
\end{itemize}

The estimate in our regression table conveys the predicted change in the outcome given a 1-unit or percent change in the explanatory variable. Referring back to Table \ref{tab:extable}, as the average number of miles driven per driver increases by one mile, traffic fatality rate increases 0.188 per 10,000. Is a one-mile increase a realistic change in the average distances driven per driver? Is one mile representative of its typical variation?

How do we get a sense of what is a typical change in the explanatory variable? The standard deviation of a variable tells us the average deviation from the variable's mean. For example, the standard deviation of \texttt{vmiles} is 1.1, so a typical change in \texttt{vmiles} is quite close to one unit. Based on the estimate for \texttt{vmiles}, a 1.1 unit change is predicted to change the traffic fatality rate by 0.21 (\(0.188 \times 1.1\)).

Next, is a predicted change of 0.21 in the traffic fatality rate negligible or meaningful? Again, we can use descriptive measures to answer this question. The mean traffic fatality rate is 2.0 and its standard deviation is 0.6. Thus, the predicted change in \texttt{mrall} from a typical change in \texttt{vmiles} is about 10\% of the mean and about one-third a standard deviation. Given the typical variation in traffic fatality rate is 0.6, is a change of 0.2 negligible or meaningful? This is where professional judgment and context plays a role, as there is no universal rule to determine what is a meaningful effect. Since the context is something as consequential as fatalities, perhaps any change is practically significant.

Lastly, since the population parameter is just as likely to equal any value in the confidence interval as it is the estimate, we should check if the lower or upper bound of the confidence interval changes our answer regarding practical significance. Since the result for \texttt{vmiles} is positive, we should focus on how close the lower bound is to zero. The lower bound for \texttt{vmiles} equals 0.147. Repeating the calculations above using the lower bound indicates that a typical change of 1.1 in \texttt{vmiles} predicts a change in \texttt{mrall} of 0.16, which is about 8\% of the mean fatality rate and one-fourth its standard deviation. Does this represent a negligible or meaningful change? Again, professional judgment and context is required.

Students of statistics are taught to focus so much on the estimate and statistical significance that they understandably get the impression that insignificance implies the results are useless. This is not necessarily the case. Once again, the confidence interval is helpful to determine whether statistically insignificant results are still practically significant.

Suppose the p-value for \texttt{vmiles} was equal to or greater than 0.05, thus leading us to fail to reject the null hypothesis. This would also mean that our 95\% confidence interval contains 0. Whether the results are still useful depends on the precision of the confidence interval around 0 relative to what we consider a meaningful change in the fatality rate given a typical change in \texttt{vmiles}. For instance, if the confidence interval ranged between -10 and 10, then our best guess for the effect ranges between substantially negative to positive or possibly no effect. This sort of imprecision is useless. However, what if the confidence interval was -0.01 to 0.01? Then, assuming a change of 0.01 in the fatality rate is negligible, we could conclude the effect of \texttt{vmiles} is negligible with a reasonable level of confidence despite failing to reject the null hypothesis.

\hypertarget{kt12}{%
\section{Key terms and concepts}\label{kt12}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Regression results

  \begin{itemize}
  \tightlist
  \item
    estimate
  \item
    standard error
  \item
    statistic or t-statistic
  \item
    p-value
  \item
    lower and upper confidence intervals
  \end{itemize}
\item
  Null distribution
\item
  Practical significance
\end{itemize}
\end{learncheck}

\hypertarget{regression-diagnostics}{%
\chapter{Regression Diagnostics}\label{regression-diagnostics}}

\begin{quote}
\emph{``The hardest assumption to challenge is the one you don't even know you are making.''}

---Douglas Adams
\end{quote}

As previous chapters explained, the regression model we choose to use for explaining or predicting an outcome and the inferences we make involve several assumptions based on sound statistical theory. However, this is not to suggest that those assumptions cannot be violated. Bad choices regarding the inclusion or exclusion of explanatory variables, small sample size, and statistical oddities in our data can cause necessary assumptions to break down. If so, we may make or accept invalid conclusions.

Recall the credible analysis figure depicted below. Whether one's role is a producer or consumer of a quantitative analysis, expertise on the subject in question can make significant contributions to every level of Figure \ref{fig:credfigrepeat2}. Understanding how variables are measured helps us evaluate measurement validity and reliability. Understanding the causal pathways between variables helps us evaluate internal and external validity. Understanding inference, probabilities of error, and the context of the results can help us make valid statistical conclusions like whether we have statistical and or practical significance. Analysts and managers alike can involve themselves in this process and work together to ensure an analysis is as credible as possible.

\begin{figure}
\includegraphics[width=13.58in]{images/credible} \caption{Components of credible analysis}\label{fig:credfigrepeat2}
\end{figure}

This chapter covers some remaining assumptions and diagnostics that a credible quantitative analysis should include. While running diagnostics may primarily fall within the role of an analyst, those managing an analysis can ask good questions or identify potential issues if they at least know what else can go wrong.

\hypertarget{lo13}{%
\section{Learning objectives}\label{lo13}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Determine whether and which classical regression assumptions may be
  violated based on a residual versus fitted plot (RVFP)
\item
  Explain why and when multicollinearity may be a problem and propose
  potential solutions
\item
  Distinguish between outlier, high-leverage, and high-influence
  observations in regression
\item
  Identify influential observations using a residual vs.~fitted plot
  (RVLP)
\end{itemize}
\end{learncheck}

\hypertarget{classical-assumptions}{%
\section{Classical assumptions}\label{classical-assumptions}}

The estimates we obtain from regression are the best linear unbiased estimates possible \emph{if} certain assumptions hold. If they do not, then our estimates could be biased or they could render our hypothesis tests invalid, creating a higher chance of Types I and II error than we chose that our significance level establishes. Fortunately, these assumptions can be remembered with an apt acronym: LINE.

For the assumptions of regression to hold, the relationship between the outcome and explanatory variables must be \textbf{Linear} (or modeled correctly as nonlinear), the observations must be \textbf{Independent} of each other, the data points must be \textbf{Normally} distributed around the regression line, and the data points should have \textbf{Equal} variation around the regression line. A key tool used to evaluate these assumptions is a \textbf{Residual vs.~Fitted Plot} (RVFP). An RVFP is a simple transformation of the regression line plot. Figure \ref{fig:genericreg} below shows a generic regression line fit to data with the outcome and predicted outcome on the y axis. An RVFP rotates the predicted outcome to the x axis, resulting in a horizontal line. This allows the distance between the observed and the fitted outcome to be vertical. Thus, the residuals of the regression are plotted on the y axis. Figure \ref{fig:genericrvfp} shows a generic RVFP.

\begin{figure}
\includegraphics[width=7.01in]{images/genericreg} \caption{Generic regression line through data}\label{fig:genericreg}
\end{figure}

\begin{figure}
\includegraphics[width=6.85in]{images/genericrvfp} \caption{Generic residual vs. fitted plot}\label{fig:genericrvfp}
\end{figure}

Note that the residuals in the RVFP above appear to be randomly positioned; there is no discernible pattern in the scatter plot. No pattern in the RVFP is a visual indication that the classic regression assumptions are not violated.

Certain patterns in the RVFP signal violations of certain assumptions. For example, Figure \ref{fig:rvfplinear} below shows a clear case that the linear assumptions is violated due to age and wage sharing a quadratic relationship.

\begin{figure}
\includegraphics[width=14.71in]{images/rvfp_linear} \caption{Fitting linear model to quadratic data}\label{fig:rvfplinear}
\end{figure}

The RVFP can also be used to check whether residuals are normally distributed around the regression line and whether the residuals have equal variance. Figures \ref{fig:rvfpnormal} and \ref{fig:rvfpequal} below show examples where each is clearly violated.

\begin{figure}
\includegraphics[width=14.53in]{images/rvfp_normal} \caption{Violation of normally distributed risiduals}\label{fig:rvfpnormal}
\end{figure}

\begin{figure}
\includegraphics[width=14.19in]{images/rvfp_equal} \caption{Violation of equal variation}\label{fig:rvfpequal}
\end{figure}

There is not a direct visual check for the assumption that observations are independent of each other. However, signs that normality or linearity have been violated could be due to violation of independence.

Independent observations is a very strong assumption. It states that the units in our data share absolutely no relationship with each other; the information pertaining to one unit has absolutely no bearing on the information gathered for another. Consider all the scenarios in which this assumption is likely violated: individuals in the same household or community, governments in the same state/province, states/provinces in the same country. Random sampling ensures independence, but random sampling is often unfeasible or not applicable to a research question.

Other than controlling for the variable(s) by which observations are related covered in Chapter \ref{causation-and-bias}, there are statistical methods to account for dependence across observations, but they are beyond the scope of this book. A competent analyst should know at least a few such methods. As with any of the above assumptions, a manager who is knowledgeable in statistics knows to ask questions regarding independence.

\hypertarget{multicollinearity}{%
\section{Multicollinearity}\label{multicollinearity}}

Multicollinearity involves whether two or more explanatory variables in our regression are strongly correlated. If the correlation between two or more explanatory variables is strong enough, it can result in Type II error (i.e.~false negative) for one or more of the variables sharing the strong correlation.

Recall that multiple regression isolates the effect of one variable on the outcome by holding all other explanatory variables constant at their mean. This requires variables to vary while holding others constant. If the values of two variables move in near perfect tandem, then regression will find it difficult to isolate the effect of one while another is held constant.

It is as if regression creates a traffic intersection with each variable having its own lane and stoplight. To investigate the isolated effect of one variable, regression turns the stoplight for that variable green and sets the stoplights for the other variables to red, letting them idle at their mean. But suppose two variables have decided not to move unless the other is allowed to move. Thus, when one gets the green light to go, it does not move, and regression estimates an effect that is less likely to be statistically significant than should be the case.

Calculating the correlation coefficient covered in Chapter \ref{descriptive-statistics} can give us a sense of whether multicollinearity may be an issue. As a general rule of thumb, if two variables have a correlation coefficient greater than 0.8 or less than -0.8, then multicollinearity could be a problem. Once a regression is run, if one or more variables that you thought should reject the null fail to do so, this could be due to multicollinearity with another explanatory variable in the model.

The solution to multicollinearity is somewhat subjective. If one variable is integral to the original purpose of your analysis, then consider dropping the other variable causing the problem. However, dropping a variable from your model should not be done lightly. The inclusion of a variable implies a theoretical claim that it affects the outcome. By dropping that variable because it is correlated with another explanatory variable, you may be introducing omitted variable bias because the dropped variable may be a confounder as discussed in Chapter \ref{causation-and-bias}. Instead, you could combine the collinear variables into a single index variable, which were discussed in Chapter \ref{data}. For instance, if the collinear variables are numerical, you calculate the average between them as a more holistic measure of the construct they both represent and include that in your regression model instead.

\hypertarget{influential-data}{%
\section{Influential Data}\label{influential-data}}

Regression is an extension of correlation, which is fundamentally based on the mean. As is any measure based on the mean, regression estimates are sensitive to extreme values in our data. Depending on our sample size, one or a few extreme values can substantially impact our regression estimates. We should be aware of influential observations and consider whether our conclusions or recommendations should differ depending on whether influential observations are included.

One must be more specific when communicating extreme values in regression, as there are three varieties:

\begin{itemize}
\tightlist
\item
  Regression Outlier: an observation with a extreme residual
\item
  High-leverage observation: an observation with an extreme value with respect to a particular variable; an outlier in the distribution of the explanatory variable
\item
  High-influence: a regression outlier with high leverage
\end{itemize}

Suppose we were interested in the relationship between poverty rate and murder rate among U.S. states and D.C. Figure \ref{fig:influential} below provides a visual example of an influential observation in regression. Note the plot point in the far upper-right corner in the left panel. This plot point has an extreme positive residual and it imposes high positive leverage because it is positioned far from the center of the poverty distribution. As a result, this plot point pulls the slope of the regression line upward. The right panel visualizes the same regression with the influential observation removed. The regression line is noticeably flatter and fits the data better.

\begin{figure}
\includegraphics[width=14.94in]{images/influential} \caption{Regression with and without a high-influence observation}\label{fig:influential}
\end{figure}

Figure \ref{fig:influential} provides an obvious case. The primary question is how do we decide an observation is high-influence? As is the case when identifying outliers of a single distribution, there is no definitive rule for identifying high-influence observations in regression. Furthermore, whether to exclude a high-influence observation is subjective and depends on the context. Either way, influential observations should be noted in a report.

A key tool used to investigate possible high-influence observations is a residual vs.~leverage plot (RVLP). This is similar to an RVFP in that it is a simple transformation of the standard regression scatter plot that allows us to identify outliers, high-leverage, and high-influence observations more effectively. Figure \ref{fig:rvlp} below shows an RVLP for the regression of poverty and murder rates.

\begin{figure}
\includegraphics[width=9.28in]{images/rvlp} \caption{Residual vs. leverage plot}\label{fig:rvlp}
\end{figure}

The software used to produce this RVLP also adds something called Cook's distance to the plot, denoted by the red dashed line. Cook's distance is a measure commonly used to identify influential observations. One rule of thumb is that any observation with a Cook's distance greater than 1 should be investigated. Here, we see that observation 51 in our data has a Cook's distance greater than 1. This observation is D.C.

\begin{quote}
\textbf{To learn how to run regression diagnostics in R, proceed to Chapter \ref{r-regression-diagnostics}.}
\end{quote}

\hypertarget{kt13}{%
\section{Key terms and concepts}\label{kt13}}

\begin{learncheck}
\begin{itemize}
\tightlist
\item
  Violations of regression assumptions
\item
  Multicollinearity
\item
  Regression outlier
\item
  High-leverage observation
\item
  High-influence observation
\item
  Excluding observations from a regression model
\end{itemize}
\end{learncheck}

\hypertarget{part-advanced-topics}{%
\part{Advanced Topics}\label{part-advanced-topics}}

\hypertarget{forecasting}{%
\chapter{Forecasting}\label{forecasting}}

\begin{quote}
\emph{``Forecasting is the art of saying what will happen, and then explaining why it didn't!''}

---Anonymous; Balaji Rajagopalan
\end{quote}

Previous chapters primarily used cross-sectional data to demonstrate various applications. Those applications fundamentally apply to time series and panel data as well. However, time series and panel data contain additional information, opening a vast array of additional methods that go far beyond the scope of this book.

This and the next chapter offer narrow coverage of two common, yet potentially advanced data applications in public administration: forecasting with time series data and fixed effects analysis with panel data. The intent is to provide the readers a few skills to conduct or understand basic analyses in each scenario.

\hypertarget{what-is-forecasting}{%
\section{What is forecasting}\label{what-is-forecasting}}

Recall in Chapter \ref{data} that time series measures one or more characteristics pertaining to the same subject over time. Therefore, the unit of analysis is the unit of time over which those characteristics are measured.

\label{tab:timeseriesrep}Time series example

country

continent

year

lifeExp

pop

gdpPercap

United States

Americas

1987

75.020

242803533

29884.35

United States

Americas

1992

76.090

256894189

32003.93

United States

Americas

1997

76.810

272911760

35767.43

United States

Americas

2002

77.310

287675526

39097.10

United States

Americas

2007

78.242

301139947

42951.65

Forecasting involves making out-of-sample predictions for a measure within a time series. Throughout the chapters on regression, we made out-of-sample predictions each time we computed the predicted value of the outcome in our regression, \(\hat{y}\), for a scenario not observed in our sample. Forecasting is no different in this regard. It is specific to predictions with time series data. Since the unit of analysis in time series data is a unit of time, an out-of-sample prediction involves a time period unobserved in our sample (i.e.~the future).

Analyses can seek to predict, to explain, or both. Keep in mind that forecasting is typically focused on prediction rather than explanation. Would it be helpful to know why an outcome is the value that it is in most cases? Certainly, but good decisions can be made by knowing what to expect regardless of why. Moreover, the benefits of modeling a valid explanatory model may not exceed the costs of delaying accurate predictions.

If the focus is solely prediction, then we do not need to concern ourselves with internal validity or omitted variable bias. Frankly, we do not care if our model makes theoretical sense as long as its predictions are accurate. While this frees us from many constraints, it makes goodness-of-fit even more important. Therefore, the primary focus of this chapter is how to identify a good forecast model and how to choose the best model among multiple good models.

Lastly, keep in mind that a forecast involves confidence intervals. Whereas explanatory regression produces confidence intervals around the estimated effect of an explanatory variable on an outcome, forecasts produce confidence intervals around the predicted value of a future outcome. These confidence intervals convey the range of values that our forecast model expects the future outcome to fall within some percentage of simulated futures.

Figure \ref{fig:futures} shows one forecast model of Australian tourism with 10 simulated futures based on resampling. This is not a typical way to show a forecast. Instead, forecasts are usually shown with a shaded confidence interval as in Figure \ref{fig:ci}. The darker region represents an 80\% confidence interval and the lighter region represents a 95\% confidence interval. These confidence intervals are based on the simulated futures.

\begin{figure}
\includegraphics[width=14.17in]{images/forecast_futures} \caption{Simulated futures of a forecast}\label{fig:futures}
\end{figure}

\begin{figure}
\includegraphics[width=13.99in]{images/forecast_ci} \caption{Confidence intervals based on simultated futures}\label{fig:ci}
\end{figure}

\hypertarget{patterns}{%
\section{Patterns}\label{patterns}}

We rely on patterns to make good forecasts. A time series that exhibits no patterns offers no information for predicting the future. Time series can exhibit the following three types of patterns:

\begin{itemize}
\tightlist
\item
  Trend: a long-term increase or decrease
\item
  Seasonal: a repeated pattern according to a calendar interval usually shorter than a year
\item
  Cyclic: irregular increases or decreases over unfixed periods of multiple years
\end{itemize}

With a time series of U.S. GDP in Figure \ref{fig:usgdp}, we can see two of the aforementioned patterns. First, there is an obvious upward trend. Secondly, there appear to be irregularly spaced plateaus or dips, most of which represent economic recessions. Recessions exhibit a cyclical pattern. Phenomena related to weather or holidays, such as energy production, consumption, and travel, are likely to exhibit seasonal patterns like the sales data shown in Figure \ref{fig:sales} below.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/usgdp-1.pdf}
\caption{\label{fig:usgdp}U.S. GDP 1975-2019}
\end{figure}

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/sales-1.pdf}
\caption{\label{fig:sales}Sales data}
\end{figure}

\hypertarget{autocorrelation}{%
\subsection{Autocorrelation}\label{autocorrelation}}

Again, it is useful for forecasts if a time series exhibits a pattern. Another way to think of a pattern is that past values provide some information for predicting future values.

Whereas correlation measures the linear association between two variables, autocorrelation measures the linear association between an outcome and past values of that outcome. We can use an autocorrelation plot to examine if past values appear to predict future values.

Figure \ref{fig:acfgdp} below is an autocorrelation plot of U.S. GDP. For all measurements along the time series of GDP, the autocorrelation plot quantifies the correlation between a chosen ``current'' GDP and past measurements of GDP called lags. Figure \ref{fig:acfgdp} goes as far as 22 lagged measures. The blue dashed line denotes the threshold at which the correlations are statistically significant at the 95\% confidence level.

We can see that the first lag of GDP is almost perfectly correlated with current GDP. In other words, last quarter's GDP is a very strong predictor of current GDP. The strength of the correlation decreases over time but remains statistically significant. This gradual decrease in autocorrelation is indicative of time series with a trend pattern.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/acfgdp-1.pdf}
\caption{\label{fig:acfgdp}Autocorrelation of U.S. GDP}
\end{figure}

Figure \ref{fig:acfgdp} below shows the autocorrelation from the quarterly sales time series that exhibited a seasonal pattern. The autocorrelation plot suggests that each even-numbered lag is correlated with the current sales measure, switching between negative and positive each time. This peak and valley pattern is common in seasonal data.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/acfsales-1.pdf}
\caption{\label{fig:acfsales}Autocorrelation of sales}
\end{figure}

In each of the examples above, we can use information from the past to predict the future. A time series that shows no autocorrelation is called \textbf{white noise}. White noise provides us no significant information about predicting the future. Figures \ref{fig:wn} and \ref{fig:acfwn} below is an example of white noise. Note there is no discernible pattern in the time series plot and no autocorrelations are statistically significant.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/wn-1.pdf}
\caption{\label{fig:wn}White noise time series}
\end{figure}

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/acfwn-1.pdf}
\caption{\label{fig:acfwn}Autocorrelation of white noise}
\end{figure}

\hypertarget{forecasting-basics}{%
\section{Forecasting basics}\label{forecasting-basics}}

Forecasts use past observed data to predict future unobserved data. If time series exhibits a pattern such that autocorrelation is present, we can use the past to improve predictions of the future.

\hypertarget{evaluation}{%
\subsection{Evaluation}\label{evaluation}}

The central goal of a forecast is to provide the most accurate prediction. How can we evaluate the accuracy of our predictions if the future events have not occurred? As was the case in previous chapters on regression, a forecast essentially draws a line through data. We can get a sense of how accurate our forecast model is by comparing its predictions to observed values. That is, we can use the residuals of a forecast model to evaluate its goodness-of-fit. A better fitting model is expected to generate more accurate predictions, on average.

\hypertarget{residuals}{%
\subsubsection*{Residuals}\label{residuals}}
\addcontentsline{toc}{subsubsection}{Residuals}

Figure \ref{fig:compare} shows a forecast model denoted by the red line that simply uses the previous GDP measure to predict current GDP, compared to observed GDP denoted by the blue line. Recall how strongly lagged GDP was correlated with current GDP. This results in a forecast that appears to fit the trend fairly well. Nevertheless, there is error for almost every year, and since GDP in this time window exhibits a consistent upward trend, using last year's GDP causes a consistent underestimation.

\includegraphics{data-apps-text_files/figure-latex/compare-1.pdf}

Figure \ref{fig:residcheck} below plots the residuals between observed and predicted GDP--the vertical distance between blue and red lines--in the top panel. The bottom-left panel is a autocorrelation plot for the residuals--computing the correlation between current residuals and lagged residuals--and the bottom-right panel shows the histogram of the residuals.

\includegraphics{data-apps-text_files/figure-latex/residcheck-1.pdf}

Figure \ref{fig:residcheck} provides a lot of useful information related to the central goal of forecasting. In order for us to conclude we have a good forecast, two goals must be met:

\begin{itemize}
\tightlist
\item
  The time series of residuals should be white noise, and
\item
  the residuals should have a mean approximately equal to zero.
\end{itemize}

It is difficult to tell from the top panel of Figure \ref{fig:residcheck} whether these goals are met. However, notice that the residuals are almost always positive, which we would expect since we know our forecast almost always underestimates GDP. Therefore, the mean is certainly greater than zero, as can be seen in the histogram.

The autocorrelation plot of the residuals suggests that residuals lagged up to six time periods is significantly correlated with current residuals. This is further evidence that the time series of our residuals is not white noise.

A good forecast extracts as much information from past data as possible to predict the future. If it fails to do so, then lagged residuals will be correlated with current residuals. Therefore, our simple forecast for GDP has not extracted all the information from the past that could inform future predictions, resulting in a sub-par forecast.

\hypertarget{root-mean-squared-error}{%
\subsubsection*{Root Mean Squared Error}\label{root-mean-squared-error}}
\addcontentsline{toc}{subsubsection}{Root Mean Squared Error}

Multiple models could achieve residuals that are white noise and have a mean equal to zero. We can further evaluate forecast models by comparing their root mean squared errors (RMSE). Recall from Chapter \ref{simple-and-multiple-regression} that the RMSE quantifies the typical deviation of the observed data points from the regression line and is analogous to the standard deviation or standard error measures. In fact, the 95\% confidence interval around a forecast is based on two RMSEs above and below the point forecast, just as two standard errors are used to construct a 95\% confidence interval around a point estimate in regression.

Table \ref{tab:frmse} shows a set of standard goodness-of-fit measures for our simple forecast of GDP. We will only concern ourselves with RMSE. According to the results, the point forecast of our model is off by plus-or-minus 137 billion dollars, on average. If we developed a model with a smaller RMSE, we would prefer it to this model, provided its residuals behave no worse.

\label{tab:frmse}Forecast goodness-of-fit measures

ME

RMSE

MAE

MPE

MAPE

MASE

ACF1

Training set

110.1394

137.2821

118.1553

1.421887

1.475215

0.2562912

0.4933519

\hypertarget{models}{%
\subsection{Models}\label{models}}

There are four basic forecasting models:

\begin{itemize}
\tightlist
\item
  Mean: future outcomes predicted to equal the average of the outcome over the entire time series
\item
  Naive: future outcomes predicted to equal the last observed outcome
\item
  Drift: draws a straight line connecting the first and last observed outcome and extrapolates it into the future
\item
  Seasonal naive: same as naive but predicts each future season to equal its last observed season
\end{itemize}

Figure \ref{fig:fcompare} below applies the mean, naive, and drift forecast models to U.S. GDP. It should be obvious that using the mean is a poor choice and will be for any time series with a strong trend pattern. Under normal circumstances absent of an impending economic shutdown, we would likely conclude that the drift model provides a more accurate forecast than the naive model.

\includegraphics{data-apps-text_files/figure-latex/fcompare-1.pdf} \includegraphics{data-apps-text_files/figure-latex/fcompare-2.pdf} \includegraphics{data-apps-text_files/figure-latex/fcompare-3.pdf}

According to the drift model, predicted GDP for the next ten time periods is shown in Table \ref{tab:gdpdrift}. Again, this is not a sophisticated model, and some may be alarmed by making predictions based on simply connecting the first and last observations, then extending the line into the future. It is important to keep in mind that the utility of a forecast is not the exact point forecasts in Table \ref{tab:gdpdrift}. In fact, it would be misleading to report GDP in Q2 of 2020 is predicted to be 21.65 trillion dollars. The utility of a forecast is the corresponding confidence interval. If this is our best model, then we can report that GDP in Q2 of 2020 is predicted to be between 21.48 and 21.81 trillion dollars with 95\% confidence.

\label{tab:unnamed-chunk-93}Forecast values

Point Forecast

Lo 80

Hi 80

Lo 95

Hi 95

2020 Q2

21645.05

21539.73

21750.36

21483.98

21806.11

2020 Q3

21755.19

21605.84

21904.53

21526.78

21983.59

2020 Q4

21865.33

21681.91

22048.74

21584.82

22145.83

2021 Q1

21975.46

21763.10

22187.83

21650.68

22300.25

2021 Q2

22085.60

21847.53

22323.68

21721.50

22449.71

2021 Q3

22195.74

21934.24

22457.25

21795.81

22595.68

2021 Q4

22305.88

22022.67

22589.10

21872.74

22739.02

2022 Q1

22416.02

22112.44

22719.60

21951.74

22880.30

2022 Q2

22526.16

22203.31

22849.01

22032.41

23019.91

2022 Q3

22636.30

22295.09

22977.51

22114.46

23158.14

Perhaps more sophisticated methods would provide a better forecast model. If so, then the model will fit observed data better, resulting in more precise confidence intervals. Greater precision could indeed be valuable depending on the context, as many decisions can be aided by considering best- and worst-case scenarios. Nevertheless, as long as our model achieves residuals that look like white noise with a mean approximately equal to zero, we can be fairly confident that our model is not wildly inaccurate though it may be less precise than an alternative model.

Let us check the residuals for our drift model. As can be seen in Figure \ref{fig:gdpdriftresid}, the mean of the residuals is approximately zero, but it appears that there is still information in past measures not extracted by our simple drift model. These results suggest we should try to improve our model.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gdpdriftresid-1.pdf}
\caption{\label{fig:gdpdriftresid}GDP drift residuals}
\end{figure}

The figures below compare mean, naive, and seasonal naive models using the seasonal sales data from earlier. Because this time series does not exhibit a clear trend, the mean model is not as obviously bad as it was with GDP, though it is highly imprecise. The same applies to the naive model. If we care about predicting specific seasons (i.e.~quarters), then clearly the seasonal naive model is the preferred choice.

\includegraphics{data-apps-text_files/figure-latex/fcompare2-1.pdf} \includegraphics{data-apps-text_files/figure-latex/fcompare2-2.pdf} \includegraphics{data-apps-text_files/figure-latex/fcompare2-3.pdf}

Let us check the residuals of the seasonal naive model. The residuals have a mean of zero, and with the exception of one significantly correlated residual for lag 4, it appears we have mostly white noise. This model may be sufficient in many cases. The fact that sales from a year ago still provide information for current sales suggests there may be an annual trend component to this time series that our seasonal naive model does not extract. Therefore, a better model is achievable.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/salesresid-1.pdf}
\caption{\label{fig:salesresid}Residual check}
\end{figure}

\hypertarget{recap}{%
\section{Recap}\label{recap}}

We have only scratched the surface of forecasting. The corresponding R Chapter covers how to implement the models and plots above as well as incorporating explanatory variables into a forecast model.

Here are the key takeaways from this chapter:

\begin{itemize}
\tightlist
\item
  Prediction does not care about the theory of a model.
\item
  Patterns in time series contain information that can be used to predict the future.
\item
  A good forecast model extracts all useful information from the past to predict the future. If this is achieved, the residuals from our forecast will look like white noise and have a mean equal to zero.
\item
  The best model among competing good models is the model with the smallest RMSE.
\end{itemize}

\hypertarget{panel-analysis}{%
\chapter{Panel Analysis}\label{panel-analysis}}

\begin{quote}
\emph{``The more things change, the more they are the same.''}

---Jean-Baptiste Alphonse Karr
\end{quote}

\hypertarget{panel-data}{%
\section{Panel data}\label{panel-data}}

Recall from Chapter \ref{data} that panel data measures the \emph{same} units over multiple time periods. Table \ref{tab:panelrep} below provides an example of panel data. Panels for geographic or political areas such as counties, school or voting districts, states, and countries are common and easy to obtain. Due to privacy protections and challenges of following people over time, panels of individual people are somewhat more difficult to obtain but are quite common.

\label{tab:panelrep}Panel example

country

continent

year

lifeExp

pop

gdpPercap

Argentina

Americas

1997

73.275

36203463

10967.282

Argentina

Americas

2002

74.340

38331121

8797.641

Argentina

Americas

2007

75.320

40301927

12779.380

Bolivia

Americas

1997

62.050

7693188

3326.143

Bolivia

Americas

2002

63.883

8445134

3413.263

Bolivia

Americas

2007

65.554

9119152

3822.137

Cross-sectional data is like having a single picture for each unit among multiple units. We use variation across those units with respect to some variable (e.g.~income, unemployment rates) to explain or predict outcomes of interest. Time series is like having a video of one subject, following that subject over multiple time periods. We use variation over time to explain or predict outcomes of interest for that subject. Panel data is like having videos for multiple subjects. Therefore, we have variation across units \emph{and} over time to use for explaining or predicting outcomes of interest.

\hypertarget{fixed-effects}{%
\section{Fixed effects}\label{fixed-effects}}

The additional information contained within panel data affords us a wide array of new analytic techniques that go far beyond the scope of this book. There is one technique or model, however, that is probably the most common and very easy to use: the fixed effects model.

Recall the standard multiple regression model shown below. This model is slightly different from what you have seen before because it uses \emph{indexing} to indicate that we have panel data. This indexing is done with the \emph{i} and \emph{t} subscripts, which represent subject and time, respectively. It is simply used to convey that we have multiple subjects \emph{i} over multiple time periods \emph{t}.

\begin{equation}
y_{it}=\beta_0+\beta_1x_{1it}+\beta_2x_{2it}+\cdots+\beta_kx_{kit}+\epsilon_{it}
\label{eq:multregpan}
\end{equation}

A fixed effects model is a slightly modified version of Equation \eqref{eq:multregpan} that represents an important conceptual leap. Recall that the \(\epsilon_{it}\) term represents all the factors that are associated with or affect the outcome \(y_{it}\) that we cannot include in our model for various reasons. This error term is inevitable and not a problem as long as there are no factors that also affect any of our explanatory variables. Otherwise, we have omitted variable bias in our model and may not be able to use our results.

In most cases, someone can probably think of an omitted variable that is related to one or more of the explanatory variables. In other words, it is really difficult to convincingly guard against claims of omitted variable bias. However, having panel data allows us to guard against an important source of potential OVB by using a fixed effects model. Using a fixed effects model allows us to control for all of the omitted factors that do not change over time.

The fixed effects model is represented in Equation \eqref{eq:femod} below. Note the new term (the Greek letter alpha) immediately to the right of the equal sign has replaced the usual y-intercept, \(\beta_0\), term. Also, note the index for this new term only includes \emph{i}. Because our data contains a time series for each subject \emph{i}, we can model a unique y-intercept for each subject. The unique y-intercepts represent all of the stuff that makes the subjects inherently different from each other and do not change over time, or at do not meaningfully change over the time span of our data.

\begin{equation}
y_{it}=\alpha_{i}+\beta_1x_{1it}+\beta_2x_{2it}+\cdots+\beta_kx_{kit}+\epsilon_{it}
\label{eq:femod}
\end{equation}

The fixed effect model is essentially identical to controlling for a categorical variable like we saw in Chapter \ref{categorical-variables-and-interactions}. Recall the graph below where we controlled for the region each state is in when modeling the relationship between miles driven and traffic fatalities. We controlled for region not because we thought being in the West literally causes drivers to have more fatal accidents, but rather because regions might capture unobserved geographic or infrastructure characteristics that affect traffic fatalities.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/pslopescatter3rep-1.pdf}
\caption{\label{fig:pslopescatter3rep}Parallel slopes for 4 groups}
\end{figure}

The fixed effect model takes this idea a little further, controlling for the unobserved characteristics of each unit--the state in this case--rather than some aggregated level like region. This produces a separate regression line for each unit, as seen in Figure \ref{fig:feviz}. For unobserved reasons, states seem to have inherent differences with respect to miles driven and fatalities. Note how flat the common regression slope is for all of the states compared to the slope of the regression without fixed effects in Figure \ref{fig:olsviz}. Ignoring the inherent differences between states leads to the conclusion that distance driven has a larger effect on the fatality rate than what the data actually suggests once we control for those differences. This is the primary reason for using a fixed effects model.

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/feviz-1.pdf}
\caption{\label{fig:feviz}Visualizing fixed effects}
\end{figure}

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/olsviz-1.pdf}
\caption{\label{fig:olsviz}Ignoring fixed effects}
\end{figure}

There is one trade-off of using fixed effects that casual users should be aware of: using a fixed effect absorbs all constant variables. Variables that tend not to change over time such as race, sex, geography, membership to some higher-level unit (e.g.~employee within an agency or union) all collapse into the fixed effect. This means that we will not obtain an estimate for these variables in a fixed effects model because they are also fixed. If we really care about getting an estimate from a time-invariant variable, then we cannot use a fixed effects model.

For example, recall the regression results we obtained in Chapter \ref{categorical-variables-and-interactions} for the following parallel slopes model.

\begin{equation}
mrall = \beta_0 + \beta_1vmiles + \beta_2region + \epsilon
\label{eq:pslopeexamp2rep}
\end{equation}

\label{tab:psloperesults2rep}Parallel slopes for regions

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

0.260

0.167

1.553

0.121

-0.069

0.589

vmiles

0.188

0.021

8.895

0.000

0.147

0.230

regionN. East

-0.076

0.065

-1.166

0.245

-0.204

0.052

regionSouth

0.519

0.056

9.283

0.000

0.409

0.630

regionWest

0.641

0.062

10.264

0.000

0.518

0.763

We got estimates for three of the regions, providing us average differences in the traffic fatality rate relative to the fourth excluded region.

Running a fixed effects model for Equation \eqref{eq:pslopeexamp2rep} generates the following results. Note the absence of a single intercept because there is a separate intercept for each state that typically is not included in a table. More importantly, there is no estimate for the regions because a state's region is absorbed by the state's fixed effect.

We are left with an estimate for the only variable in our model that differs across time within each state: \texttt{vmiles}. The estimate is statistically significant at the standard 5\% significance level and is substantially less than the estimate in the model ignoring fixed effects. Here, as the average miles driven per driver \emph{within} a state increases by 1 mile, the fatality rate increases by 0.057 deaths, all else equal.

\label{tab:unnamed-chunk-95}Fixed effects results

term

estimate

std.error

statistic

p.value

vmiles

0.057

0.019

2.956

0.003

\hypertarget{part-r-chapters}{%
\part{R Chapters}\label{part-r-chapters}}

\hypertarget{r-introduction}{%
\chapter{R Introduction}\label{r-introduction}}

This part of the book contains what are referred to as R chapters that correspond to chapters in previous parts of the book. Though previous chapters use R to present information, they focus on concepts that are applicable regardless of statistical software. R chapters take those concepts and present ways to practically apply them via a short series of exercises using R.

\hypertarget{r-chapter-structure}{%
\section{R chapter structure}\label{r-chapter-structure}}

Each R chapter begins with a list of learning objectives followed by a what you need to set up in terms of packages and datasets to complete the chapter.

Each chapter then guides you through a few exercises that require you to operate R. Periodically, they will ask you to interpret your results or connect what you have done to the concept it was meant to help you understand.

By the end of each chapter, you will have at least one document to save and submit. That document will contain code and answers to questions. Some chapters may ask you to generate an output document such as PDF, Word, or HTML.

\hypertarget{r-chapter-feedback}{%
\section{R chapter feedback}\label{r-chapter-feedback}}

Once you submit your work to the course site on eLC, a document will become available that contains answers prepared by your instructor. This document is meant to provide nearly immediate feedback.

You should compare your work to that of your instructor, making note of any differences and attempting to make sense of them. At the beginning of each class, we will review the corresponding R chapter if students have questions.

\hypertarget{what-is-r-and-rstudio}{%
\section{What is R and RStudio}\label{what-is-r-and-rstudio}}

R is a programming language for statistical computing. RStudio is a user interface for R. These two programs are analogous to a smart phone. Your phone has base code you never interact with directly but is what allows your phone to work. You interact with this code, doing all the cool things it allows you to do through what you see on the screen. R is like the base code for your phone. RStudio is like the screen.

\hypertarget{installing-r-and-rstudio}{%
\section{Installing R and RStudio}\label{installing-r-and-rstudio}}

First, download and install R \href{https://cloud.r-project.org/}{here}.

\begin{itemize}
\tightlist
\item
  \textbf{Windows user:} click on ``Download R for Windows'', then click on ``base'', then click on ``Download R \#.\#.\# for Windows.''
\item
  \textbf{MacOS user:}, click on ``Download R for (Mac) OS X.'' What you click on next depends on what version of macOS you are using. Under ``Latest release,'' you will see a link such as ``R-\#.\#.\#.pkg'' with a description to the right that indicates which versions of macOS it is compatible with, such as macOS 10.13 (High Sierra) and higher. If you are using an older version of macOS, scroll down to the header ``Binaries for legacy OS X systems'' where you can find the link that will work with your version. If you do not know which version of macOS you are using, click on the apple symbol in the top-left of your screen, then click on ``About This Mac.'' The resulting window will display your version of macOS.
\end{itemize}

Second, download and install RStudio \href{https://www.rstudio.com/products/rstudio/download/}{here}.

\begin{itemize}
\tightlist
\item
  Click on the download link beneath the ``RStudio Desktop'' version that is ``FREE.''
\item
  The website should automatically provide a link under step 2 to download the version of RStudio recommended for your computer.
\end{itemize}

\hypertarget{rstudio-orientation}{%
\section{RStudio orientation}\label{rstudio-orientation}}

\begin{learncheck}
\textbf{Exercise 1} Launch RStudio
\end{learncheck}

Upon launch, you should see three panes:

\includegraphics[width=26.4in]{images/rstudio_sshot}

\begin{itemize}
\tightlist
\item
  Console pane (left) is where you can tell R what to do. It also displays the results of commands. \textbf{Only use the console for installing packages.}
\item
  Environment pane (top right) displays all the data in your current R session. A session is the time between launching and closing R.
\item
  Files pane (bottom right) allows you to navigate your files, displays plots, provides a list of installed packages, allows you to search for help, and displays file exports.
\end{itemize}

You will usually see a fourth pane while working in RStudio -- the source editor pane.

\begin{learncheck}
\textbf{Exercise 2} In the RStudio menu bar at the top of your screen,
go to
\texttt{File\ -\textgreater{}\ New\ File\ -\textgreater{}\ R\ Script}. A
new pane will open. \textbf{This is the pane where you will tell R what
to do 99\% of the time because it allows you to do so while creating a
document you can save.}
\end{learncheck}

\hypertarget{r-packages}{%
\subsection{R Packages}\label{r-packages}}

Many tasks in R require you to install R packages that augment its functionality. Extending the smartphone analogy from above, your phone comes with base programs (e.g.~calendar, weather), but others create third-party applications to augment the functionality of your phone. This is also the case with R, which has an active user community that develops useful third-party apps called packages.

\hypertarget{install}{%
\subsubsection*{Install}\label{install}}
\addcontentsline{toc}{subsubsection}{Install}

Just like your phone, you have to first install a R package to use it. Install packages by typing the following code \textbf{into the console pane}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"name_of_package"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You only need to install a package once. The package is saved on your computer where R can find it.

\begin{learncheck}
\textbf{Exercise 3} We will almost always need to use a package called
\texttt{tidyverse}. In your console pane of RStudio, type
\texttt{install.packages("tidyverse")}, then click \texttt{Enter} on
your keyboard. This will begin the installation. Monitor the console
while the package installs. RStudio may ask you some Yes/No questions
during the process. Answer all questions in the affirmative by typing
\texttt{Yes} then clicking \texttt{Enter}.
\end{learncheck}

\hypertarget{load}{%
\subsubsection*{Load}\label{load}}
\addcontentsline{toc}{subsubsection}{Load}

When an app is installed on our phone, you still have to launch it to use it. The same goes for using a R package. Each time you launch RStudio, you need to load the package(s) whose functions you plan to use. Therefore, loading packages should be done in the source editor. The following generic code is used to load a package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(name_of_package)}
\end{Highlighting}
\end{Shaded}

Note that to load a package, we do not use quotation marks around the name like we do when installing.

\begin{learncheck}
\textbf{Exercise 4} You should have a script open in the source editor
(upper-left) pane. Type \texttt{library(tidyverse)} in the script. Click
\texttt{Run} or \texttt{Cmd+Enter} to execute this line of code. The
console pane will provide information about the loading.
\end{learncheck}

You must load a package before using any functions included in that package or else you will receive the following error message, \texttt{Error:\ could\ not\ find\ function}. Remember this crucial fact. I will tell you what packages are needed for certain tasks, but remember that you need to install and load the package to use it.

\hypertarget{submit}{%
\section{Submit}\label{submit}}

Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download.

\hypertarget{additional-resources}{%
\section{Additional Resources}\label{additional-resources}}

There are many resources that provide orientations to R. Below are a few that offer a thorough and accessible introduction.

\begin{itemize}
\tightlist
\item
  Chapters 1-3 of \href{https://rbasics.netlify.app/index.html}{Getting Used to R, RStudio, and R Markdown}
\item
  \href{https://rladiessydney.org/courses/ryouwithme/01-basicbasics-0/}{BasicBasics of RYouWithMe} by R-Ladies Sydney
\end{itemize}

\hypertarget{r-data}{%
\chapter{R Data}\label{r-data}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Examine datasets to determine their units of analysis and structures
\item
  Examine variables to determine their type
\item
  Print a preview of the first or last few rows of a dataset
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

You should by now know how to start a script and a notebook. For this chapter, use a script. As a reminder, everything you type in a script R will interpret as code to execute. You can add comments not meant as code by adding a hashtag \# before the line of text. Use comments to answer any questions.

To complete this R chapter, you need to load the following packages. Add this code to your script and run it.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(carData)  }\CommentTok{#if this fails, you need to install}
\end{Highlighting}
\end{Shaded}

Before we begin, go to the \texttt{Packages} tab in the bottom-right pane. Find \texttt{carData} in the list and click on the name. This should take you to the \texttt{Help} tab, which will contain the documentation for \texttt{carData}. This page serves as a directory to all of the datasets that come loaded with the package. We will be examining some of these datasets. If you want or need to learn more about a particular dataset, you can click on its name in this list.

\hypertarget{viewing-datasets}{%
\section{Viewing datasets}\label{viewing-datasets}}

It is assumed most readers are familiar with spreadsheets. Perhaps one of the first obstacles to using R is that you do not constantly stare at a spreadsheet, creating somewhat of a disconnect between what you do to data and seeing it done.

You can examine a dataset in spreadsheet form using the following command:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{View}\NormalTok{(dataset)}
\end{Highlighting}
\end{Shaded}

where you replace \texttt{dataset} with the actual name of the dataset.

\begin{learncheck}
\textbf{Exercise 1:} In your script, use the \texttt{View} command on
the \texttt{Arrests} dataset that should be available in your current R
session.
\end{learncheck}

A new tab should have appeared in the top-left pane containing the spreadsheet version of \texttt{Arrests}. This dataset contains information on arrests for possession of small amounts of marijuana in the city of Toronto.

\begin{learncheck}
\textbf{Exercise 2} Based on what you see, what is the structure of this
dataset? What is the unit of analysis?
\end{learncheck}

Let's examine a new dataset called \texttt{Florida} which contains county voting results for the 2000 presidential election.

\begin{learncheck}
\textbf{Exercise 3} What is the structure and unit of analysis of the
\texttt{Florida} dataset?
\end{learncheck}

\begin{learncheck}
\textbf{Exercise 4} Do the same thing one more time with the
\texttt{USPop} dataset. What is its structure and unit of analysis?
\end{learncheck}

\hypertarget{warning-about-view}{%
\subsection{Warning about View}\label{warning-about-view}}

\texttt{View} is useful but \textbf{should not} be included in a notebook that you plan to export to another document. This is because R will attempt to print the entire dataset to the export document. This is almost always a mistake.

To avoid making this mistake, I suggest you not use the \texttt{View} function as code, but rather use a point-and-click alternative.

Run the following code to save \texttt{Arrests} to your environment pane in the top-right.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arrests <-}\StringTok{ }\NormalTok{Arrests}
\end{Highlighting}
\end{Shaded}

Now click on \texttt{arrests}. This should do the same thing as running \texttt{View}.

\hypertarget{glimpsing-datasets}{%
\section{Glimpsing Datasets}\label{glimpsing-datasets}}

If your dataset is moderately large, \texttt{View} is an inefficient way to get a sense of your data. The \texttt{glimpse} function generates a compact printout providing key information about a dataset.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(dataset)}
\end{Highlighting}
\end{Shaded}

\begin{learncheck}
\textbf{Exercise 5} Use \texttt{glimpse} on \texttt{Arrests}.
\end{learncheck}

Notice that the results show you the dimensions of the dataset--the number of rows (observation) and columns (variables). Next, it provides a vertical list of variables, with several of their values listed horizontally, that can export to documents more easily.

\begin{learncheck}
\textbf{Exercise 6} Having now examined \texttt{Arrests} using
\texttt{View} and \texttt{glimpse}, what type are the following
variables based on the taxonomy used in the chapter on data.

\begin{itemize}
\tightlist
\item
  year
\item
  age
\item
  sex
\end{itemize}
\end{learncheck}

The column immediately to the right of the variable name is also informative. It tells you how each variable is stored in R. A variable can be stored in several ways:

\begin{itemize}
\tightlist
\item
  \textbf{Integers:} commonly used for discrete variables
\item
  \textbf{Doubles/numerics:} commonly used for continuous variabls but can also store discrete variables
\item
  \textbf{Logicals:} commonly used for categorical variables that are binary (i.e.~1 or 0). In R, logicals are assigned \texttt{TRUE}, if equal to 1, or \texttt{FALSE}, if equal to 0.
\item
  \textbf{Factors:} commonly used for categorical variables. Factors can store categorical variables with any number of levels. Therefore, a binary variable can be stored as a factor instead of a logical if you want the variable to be assigned different values like ``Yes'' or ``No.''
\item
  \textbf{Characters:} commonly used for strings of text that don't fit the other storage types well, such as open-ended responses in a survey. However, any variable can be stored as a character. A numerical variable can be stored as a character and R will not recognize its values as numbers. Character variables are often denoted with quotation marks "" around them.
\end{itemize}

\begin{learncheck}
\textbf{Exercise 7} Are the variables in exercise 6 stored in a way that
makes sense given your answers?
\end{learncheck}

Variables will not always be stored the way they should. Sometimes we have to tell R how to store a variable based on our own understanding of their type. This skill will be covered later.

\hypertarget{preview-first-or-last-few-rows}{%
\section{Preview first or last few rows}\label{preview-first-or-last-few-rows}}

Sometimes we want to show people what the data looks like in spreadsheet form, or we want to examine the first or last few rows in our dataset. As previously, mentioned the \texttt{View} function prints the entire dataset, which is almost always too much.

We can preview the top few rows of a dataset using the \texttt{head} function like so

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(dataset, }\DataTypeTok{n =}\NormalTok{ number_of_rows_you_want_to_print)}
\end{Highlighting}
\end{Shaded}

or the bottom few rows of a dataset with the \texttt{tail} function

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tail}\NormalTok{(dataset, }\DataTypeTok{n =}\NormalTok{ number_of_rows_you_want_to_print)}
\end{Highlighting}
\end{Shaded}

\begin{learncheck}
\textbf{Exercise 8} Print the first and last 4 rows of the
\texttt{USPop} dataset.
\end{learncheck}

Note that R prints the rows in descending order according to how the dataset is ordered. For \texttt{tail} R went up 4 rows from the bottom and printed the 4 rows in descending order.

\hypertarget{submit}{%
\section{Submit}\label{submit}}

Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download.

\hypertarget{r-missing-data}{%
\chapter{R Missing Data}\label{r-missing-data}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Determine if a dataset has missing values
\item
  Determine which variables in a dataset have missing values and how many values are missing
\item
  Run functions on variables that have missing values
\item
  Replace all missing values with a non-missing value, such as 0, if doing so is advisable
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Open a script
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(carData)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data}{%
\section{Data}\label{data}}

We will be using the \texttt{SLID} data from the \texttt{carData} package to learn how to deal with missing data. Per its documentation,

\begin{quote}
``The SLID data frame has 7425 rows and 5 columns. The data are from the 1994 wave of the Canadian Survey of Labour and Income Dynamics, for the province of Ontario. There are missing data, particularly for wages.''
\end{quote}

As is always the case when we begin working with new data, we want to get a sense of what it contains.

\begin{learncheck}
\textbf{Exercise 1:} Use \texttt{glimpse} to examine \texttt{SLID}.
\end{learncheck}

This is a moderately large dataset with 7,425 observations. Obviously, it would be crazy to look for missing values by scrolling through a spreadsheet. We can see from the glimpse results that wages definitely has missing values.

\hypertarget{checking-for-missing-data}{%
\section{Checking for missing data}\label{checking-for-missing-data}}

We can tell R to check if an entire dataset has \emph{any} missing data using the following function

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{anyNA}\NormalTok{(dataset)}
\end{Highlighting}
\end{Shaded}

where we \texttt{dataset} with the name of the dataset. If the dataset has at least one missing value, then \texttt{anyNA} will return \texttt{TRUE}.

\begin{learncheck}
\textbf{Exercise 2:} Use \texttt{anyNA} to confirm \texttt{SLID} has
missing values.
\end{learncheck}

The \texttt{anyNA} hasn't told us anything we didn't already know given the obvious \texttt{NA}s present in wages. Next, we may want to know which variables have missing values.

To determine which variables have missing values, we want to run \texttt{anyNA} repeatedly for each variable in our dataset. To run any function repeatedly on each row or column of a dataset, we can use the following function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(dataset, }\DecValTok{1}\NormalTok{ (}\ControlFlowTok{for}\NormalTok{ rows, or) }\DecValTok{2}\NormalTok{ (}\ControlFlowTok{for}\NormalTok{ columns), }\ControlFlowTok{function}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

where we replace \texttt{dataset} with the name of our dataset, include either \texttt{1} or \texttt{2}, and replace \texttt{function} with the name of the function we want to repeat.

\begin{learncheck}
\textbf{Exercise 3:} Use \texttt{apply} to run the \texttt{anyNA}
function repeatedly on each column.
\end{learncheck}

Your results should tell you that wages, education, and language contain missing values.

\hypertarget{counting-missing-values}{%
\section{Counting missing values}\label{counting-missing-values}}

Once we know a variable has missing values, we typically want to know how many values are missing or what percentage of total observations are missing for that variable.

The \texttt{is.na} function tests every value of a variable for whether it is missing. If a value is NA, \texttt{is.na} returns \texttt{TRUE}. To illustrate, the below code assigns a series of ten values to \texttt{v}, five of which are missing. This \texttt{v} object is no different from a variable in a dataset. Then, using the \texttt{is.na} function on \texttt{v} will return a list of values accordingly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{v <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DecValTok{5}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{11}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{, }\OtherTok{NA}\NormalTok{)}
\KeywordTok{is.na}\NormalTok{(v)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE
\end{verbatim}

Recall in Chapter \ref{r-data} that the logical value of \texttt{TRUE} equals 1 in R, while \texttt{FALSE} equals 0. This means we can do math on TRUE/FALSE values just like we would if they were coded as 1/0.

If \texttt{is.na} gives us \texttt{TRUE} for every \texttt{NA}, then adding all the \texttt{TRUE}s will give us the total count of missing values.

To sum all the values of any variable, we can use the \texttt{sum} function

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(v))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

The result tells us 5 of the 10 values in \texttt{v} are missing. We can easily determine that 50\% of the data for \texttt{v} is missing. But what if we have some denominator that is not as easy as 10? We can quickly to determine the percent of missing values by taking the average of \texttt{TRUE}s and \texttt{FALSE}s from the \texttt{is.na} function because the average sums the values of the variable and divides by the number of values.

We take the average of the \texttt{is.na} function using the \texttt{mean} function

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(v))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5
\end{verbatim}

As expected, we get 0.5 or 50\%. Building from this example, we can quantify the total and percent of missing values for wages like so

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(SLID}\OperatorTok{$}\NormalTok{wages))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3278
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(SLID}\OperatorTok{$}\NormalTok{wages))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4414815
\end{verbatim}

Wages is missing 3,278 observations, or about 44\% of all observations.

\begin{learncheck}
\textbf{Exercise 4:} Use the \texttt{sum} and \texttt{mean} function to
determine the count and percent of missing values for the
\texttt{education} and \texttt{language} variables.
\end{learncheck}

If we had, say, 10 variables with missing values, the process above would be rather tedious. Like before, we can tell R to repeatedly quantify missing values for each variable using a slightly different function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(SLID, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     wages education       age       sex  language 
##      3278       249         0         0       121
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(SLID, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{mean}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      wages  education        age        sex   language 
## 0.44148148 0.03353535 0.00000000 0.00000000 0.01629630
\end{verbatim}

\hypertarget{bypassing-missing-values}{%
\section{Bypassing missing values}\label{bypassing-missing-values}}

Many functions that execute some kind of computation (e.g.~sum, average) do not work if you execute them on variables that contain missing values. This is deliberate so users are notified of missing values.

For instance, let's try to calculate average years of education.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(SLID}\OperatorTok{$}\NormalTok{education)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

In order to have functions bypass missing values, we have to include the \texttt{na.rm=TRUE} option that tells R to skip \texttt{NA}s.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(SLID}\OperatorTok{$}\NormalTok{education, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12.49608
\end{verbatim}

Since education is missing only 3\% of its values, this is probably a good approximation of what the average would be if there were no missing values.

\begin{learncheck}
\textbf{Exercise 5:} Compute the average for \texttt{wages}.
\end{learncheck}

It is unclear what to do with average wages since almost half of its values is missing. At the very least, we can report something like, ``Only 56\% of respondents reported a wage. Of those who reported a wage, the average equals \$15.55.''

\hypertarget{replacing-missing-values}{%
\section{Replacing missing values}\label{replacing-missing-values}}

There are several reasons wages may be missing. The respondent could be unemployed, a student, or a stay-at-home parent. We don't know exactly why wages is missing values.

For the sake of this example, let's assume all missing wages means the respondent is unemployed (a bad assumption since this would suggest an unemployment rate of about 44\%). Suppose we wanted to know the average wages for our entire sample, regardless of whether they are employed. In that case, we want to replace the missing values with 0.

To replace \texttt{NA}s with some other value, we can use the \texttt{replace\_na} function like so

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset_nomissing <-}\StringTok{ }\KeywordTok{replace_na}\NormalTok{(dataset, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{variable1 =} \DecValTok{0}\NormalTok{, }\DataTypeTok{variable2 =} \DecValTok{0}\NormalTok{,...))}
\end{Highlighting}
\end{Shaded}

where we create a new dataset indicating we've replaced the missing values (we don't want to overwrite the original data). Inside the \texttt{replace\_na} function, we include the name of the \texttt{dataset} then list the names of the variables and the value we want to use to replace their missing values.

\begin{learncheck}
\textbf{Exercise 6:} Create a new dataset \texttt{SLID\_nomissing} that
replaces all missing values for \texttt{wages} with 0. Then, calculate
the average wage for this new data.
\end{learncheck}

\hypertarget{submit}{%
\section{Submit}\label{submit}}

Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download.

\hypertarget{r-description}{%
\chapter{R Description}\label{r-description}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Make a professional quality table of descriptive statistics
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Start a notebook
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(arsenal)}
\KeywordTok{library}\NormalTok{(knitr)}
\KeywordTok{library}\NormalTok{(carData)}
\end{Highlighting}
\end{Shaded}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Summary statistics tables are ubiquitous in reports and studies. Usually a project involves numerous variables that would require too many visualizations to summarize, though we should still consider visualizations for the most important variables. A standard summary stats table provides readers a reference for key measures pertaining to all our variables in a fairly compact form. They can even provide new insights in lieu of a visualization.

In this chapter, we set out to summarize variables within the \texttt{States} dataset of the \texttt{carData} package.

\begin{learncheck}
\textbf{Exercise 1:} Use the \texttt{glimpse} function to examine the
\texttt{States} dataset.
\end{learncheck}

\texttt{States} is a cross-section of the 50 states and D.C. containing education and related statistics. Be sure to read the help page for \texttt{States} to understand each variable. You can do that by typing \texttt{States} in the search bar of the bottom-right pane of RStudio or going to the \texttt{carData} package under the Packages tab and clicking on the \texttt{States} link.

\hypertarget{summary-table}{%
\section{Summary Table}\label{summary-table}}

Summary tables come in many styles, so there is no way to cover everything. In most cases, a summary table includes the following descriptive measures depending on the type of variable:

\begin{itemize}
\tightlist
\item
  Numerical variables

  \begin{itemize}
  \tightlist
  \item
    Mean
  \item
    Standard deviation
  \item
    Minimum
  \item
    Maximum
  \end{itemize}
\item
  Categorical variables

  \begin{itemize}
  \tightlist
  \item
    Counts for each level, and/or
  \item
    Percentages for each level
  \end{itemize}
\end{itemize}

If a variable is skewed, then it may be wise to replace the mean and standard deviation with the median, first quartile, and third quartile. We will learn how to do this.

\hypertarget{using-arsenal}{%
\section{Using Arsenal}\label{using-arsenal}}

Due to the many styles of summary tables, there are numerous R packages designed to produce summary tables. The best R package in terms of quickly getting the information to a nicely formatted table of which I am aware is Arsenal. Therefore, we will learn how to use Arsenal. I will demonstrate Arsenal using the \texttt{gapminder} data with which we are all familiar. Then, I will ask you to replicate those demonstrations using the \texttt{States} data.

Producing a summary table with Arsenal involves at least two, probably three, steps.

\begin{itemize}
\tightlist
\item
  Create a new object containing the summary statistics we want to include in a table
\item
  Relabel the variables to something appropriate for our audience
\item
  Generating the summary table based on the new object we just created
\end{itemize}

Here is an example using \texttt{gapminder} data without altering any of Aresenal's default options that we will want to know how to alter in most cases.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.gapminder <-}\StringTok{ }\KeywordTok{tableby}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{continent }\OperatorTok{+}\StringTok{ }\NormalTok{gdpPercap }\OperatorTok{+}\StringTok{ }\NormalTok{lifeExp }\OperatorTok{+}\StringTok{ }\NormalTok{pop, }\DataTypeTok{data =}\NormalTok{ gapminder)}

\KeywordTok{labels}\NormalTok{(sum.gapminder) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{continent =} \StringTok{"Continent"}\NormalTok{, }\DataTypeTok{gdpPercap =} \StringTok{"GDP Per Capita"}\NormalTok{, }\DataTypeTok{lifeExp =} \StringTok{"Life Expectancy"}\NormalTok{, }\DataTypeTok{pop =} \StringTok{"Population"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(sum.gapminder, }\DataTypeTok{title =} \StringTok{"Summary Stats for Gapminder Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lc@{}}
\caption{Summary Stats for Gapminder Data}\tabularnewline
\toprule
& Overall (N=1704)\tabularnewline
\midrule
\endfirsthead
\toprule
& Overall (N=1704)\tabularnewline
\midrule
\endhead
\textbf{Continent} &\tabularnewline
~~~Africa & 624 (36.6\%)\tabularnewline
~~~Americas & 300 (17.6\%)\tabularnewline
~~~Asia & 396 (23.2\%)\tabularnewline
~~~Europe & 360 (21.1\%)\tabularnewline
~~~Oceania & 24 (1.4\%)\tabularnewline
\textbf{GDP Per Capita} &\tabularnewline
~~~Mean (SD) & 7215.327 (9857.455)\tabularnewline
~~~Range & 241.166 - 113523.133\tabularnewline
\textbf{Life Expectancy} &\tabularnewline
~~~Mean (SD) & 59.474 (12.917)\tabularnewline
~~~Range & 23.599 - 82.603\tabularnewline
\textbf{Population} &\tabularnewline
~~~Mean (SD) & 29601212.325 (106157896.744)\tabularnewline
~~~Range & 60011.000 - 1318683096.000\tabularnewline
\bottomrule
\end{longtable}

The second code chunk above requires a specific code chunk option in order for the table to print correctly. The top line of a code chunk includes \texttt{\{r\}} by default. To print the summary table from the Arsenal package correctly using the \texttt{summary()} function, we need to add the code chunk option \texttt{results=\textquotesingle{}asis\textquotesingle{}}. Therefore, the top line of the code chunk should be \texttt{\{r,\ results=\textquotesingle{}asis\textquotesingle{}\}}.

\begin{learncheck}
\textbf{Exercise 2:} Replicate the code shown above to create a default
summary table for the \texttt{States} data using the Arsenal package. Be
sure to relabel the variables to something relatively understandable and
brief. Labeling is tedious but you only need to do it once. You can run
the code individually or knit your notebook to preview the table.
\end{learncheck}

In three relatively short bits of code, we already have a decent summary table that would have taken excruciatingly long to input manually. But it can be made better.

\hypertarget{adjustments}{%
\subsection{Adjustments}\label{adjustments}}

\hypertarget{decimal-digits}{%
\subsubsection*{Decimal digits}\label{decimal-digits}}
\addcontentsline{toc}{subsubsection}{Decimal digits}

The biggest aesthetic issue with this table is including so many decimals. None of these variables have such a small range that rounding to integers masks useful information. Obviously, if a variable only ranges between 0 and 1, we would not want to round to an integer.

Specifying the number of decimals is quite easy with Arsenal. Because arsenal tries to be as flexible as possible, we have to specify the number of decimals separately for numerical and percentage measures. The following code sets the number of decimals to zero for the gapminder data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.gapminder2 <-}\StringTok{ }\KeywordTok{tableby}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{continent }\OperatorTok{+}\StringTok{ }\NormalTok{gdpPercap }\OperatorTok{+}\StringTok{ }\NormalTok{lifeExp }\OperatorTok{+}\StringTok{ }\NormalTok{pop, }\DataTypeTok{data =}\NormalTok{ gapminder, }\DataTypeTok{digits =} \DecValTok{0}\NormalTok{, }\DataTypeTok{digits.pct =} \DecValTok{0}\NormalTok{)}

\KeywordTok{labels}\NormalTok{(sum.gapminder2) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{continent =} \StringTok{"Continent"}\NormalTok{, }\DataTypeTok{gdpPercap =} \StringTok{"GDP Per Capita"}\NormalTok{, }\DataTypeTok{lifeExp =} \StringTok{"Life Expectancy"}\NormalTok{, }\DataTypeTok{pop =} \StringTok{"Population"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(sum.gapminder2, }\DataTypeTok{title =} \StringTok{"Summary Stats for Gapminder Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lc@{}}
\caption{Summary Stats for Gapminder Data}\tabularnewline
\toprule
& Overall (N=1704)\tabularnewline
\midrule
\endfirsthead
\toprule
& Overall (N=1704)\tabularnewline
\midrule
\endhead
\textbf{Continent} &\tabularnewline
~~~Africa & 624 (37\%)\tabularnewline
~~~Americas & 300 (18\%)\tabularnewline
~~~Asia & 396 (23\%)\tabularnewline
~~~Europe & 360 (21\%)\tabularnewline
~~~Oceania & 24 (1\%)\tabularnewline
\textbf{GDP Per Capita} &\tabularnewline
~~~Mean (SD) & 7215 (9857)\tabularnewline
~~~Range & 241 - 113523\tabularnewline
\textbf{Life Expectancy} &\tabularnewline
~~~Mean (SD) & 59 (13)\tabularnewline
~~~Range & 24 - 83\tabularnewline
\textbf{Population} &\tabularnewline
~~~Mean (SD) & 29601212 (106157897)\tabularnewline
~~~Range & 60011 - 1318683096\tabularnewline
\bottomrule
\end{longtable}

\begin{learncheck}
\textbf{Exercise 3:} Replicate the code shown above to create a summary
table for the \texttt{States} data with no decimals. You will need to
copy-and-paste the labels code.
\end{learncheck}

\hypertarget{reporting-median-and-iqr}{%
\subsubsection*{Reporting median and IQR}\label{reporting-median-and-iqr}}
\addcontentsline{toc}{subsubsection}{Reporting median and IQR}

Instead of the mean and standard deviation, we may want to report the median, first quartile, and third quartile for our numerical variables. We can control the descriptive measures using the following code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.gapminder3 <-}\StringTok{ }\KeywordTok{tableby}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{continent }\OperatorTok{+}\StringTok{ }\NormalTok{gdpPercap }\OperatorTok{+}\StringTok{ }\NormalTok{lifeExp }\OperatorTok{+}\StringTok{ }\NormalTok{pop, }\DataTypeTok{data =}\NormalTok{ gapminder, }\DataTypeTok{digits =} \DecValTok{0}\NormalTok{, }\DataTypeTok{digits.pct =} \DecValTok{0}\NormalTok{, }\DataTypeTok{numeric.stats =} \KeywordTok{c}\NormalTok{(}\StringTok{"median"}\NormalTok{, }\StringTok{"q1q3"}\NormalTok{, }\StringTok{"range"}\NormalTok{))}

\KeywordTok{labels}\NormalTok{(sum.gapminder3) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{continent =} \StringTok{"Continent"}\NormalTok{, }\DataTypeTok{gdpPercap =} \StringTok{"GDP Per Capita"}\NormalTok{, }\DataTypeTok{lifeExp =} \StringTok{"Life Expectancy"}\NormalTok{, }\DataTypeTok{pop =} \StringTok{"Population"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(sum.gapminder3, }\DataTypeTok{title =} \StringTok{"Summary Stats for Gapminder Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lc@{}}
\caption{Summary Stats for Gapminder Data}\tabularnewline
\toprule
& Overall (N=1704)\tabularnewline
\midrule
\endfirsthead
\toprule
& Overall (N=1704)\tabularnewline
\midrule
\endhead
\textbf{Continent} &\tabularnewline
~~~Africa & 624 (37\%)\tabularnewline
~~~Americas & 300 (18\%)\tabularnewline
~~~Asia & 396 (23\%)\tabularnewline
~~~Europe & 360 (21\%)\tabularnewline
~~~Oceania & 24 (1\%)\tabularnewline
\textbf{GDP Per Capita} &\tabularnewline
~~~Median & 3532\tabularnewline
~~~Q1, Q3 & 1202, 9325\tabularnewline
~~~Range & 241 - 113523\tabularnewline
\textbf{Life Expectancy} &\tabularnewline
~~~Median & 61\tabularnewline
~~~Q1, Q3 & 48, 71\tabularnewline
~~~Range & 24 - 83\tabularnewline
\textbf{Population} &\tabularnewline
~~~Median & 7023596\tabularnewline
~~~Q1, Q3 & 2793664, 19585222\tabularnewline
~~~Range & 60011 - 1318683096\tabularnewline
\bottomrule
\end{longtable}

\begin{learncheck}
\textbf{Exercise 4:} Replicate the code shown above to create a summary
table for the \texttt{States} data that reports median and the first and
third quartiles.
\end{learncheck}

\hypertarget{across-groups}{%
\subsubsection*{Across groups}\label{across-groups}}
\addcontentsline{toc}{subsubsection}{Across groups}

Finally, instead of reporting summary statistics for the entire sample, we may want to report them separately for each level of a categorical variable. This is a common way to make comparisons.

We can have Arsenal report across groups by adding the categorical variable to the left side of the formula in the \texttt{tableby} code. The code below reports the \texttt{gapminder} data across continents. By default, Arsenal tests for correlations across groups and reports a p-value. This is not a common part of a summary table, so I turn this feature off with the \texttt{test\ =\ FALSE} within the code below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.gapminder4 <-}\StringTok{ }\KeywordTok{tableby}\NormalTok{(continent }\OperatorTok{~}\StringTok{ }\NormalTok{gdpPercap }\OperatorTok{+}\StringTok{ }\NormalTok{lifeExp }\OperatorTok{+}\StringTok{ }\NormalTok{pop, }\DataTypeTok{data =}\NormalTok{ gapminder, }\DataTypeTok{digits =} \DecValTok{0}\NormalTok{, }\DataTypeTok{digits.pct =} \DecValTok{0}\NormalTok{, }\DataTypeTok{test =} \OtherTok{FALSE}\NormalTok{)}

\KeywordTok{labels}\NormalTok{(sum.gapminder4) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{continent =} \StringTok{"Continent"}\NormalTok{, }\DataTypeTok{gdpPercap =} \StringTok{"GDP Per Capita"}\NormalTok{, }\DataTypeTok{lifeExp =} \StringTok{"Life Expectancy"}\NormalTok{, }\DataTypeTok{pop =} \StringTok{"Population"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(sum.gapminder4, }\DataTypeTok{title =} \StringTok{"Summary Stats for Gapminder Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lcccccc@{}}
\caption{Summary Stats for Gapminder Data}\tabularnewline
\toprule
\begin{minipage}[b]{0.15\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering
Africa (N=624)\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering
Americas (N=300)\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering
Asia (N=396)\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering
Europe (N=360)\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering
Oceania (N=24)\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering
Total (N=1704)\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.15\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering
Africa (N=624)\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering
Americas (N=300)\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering
Asia (N=396)\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering
Europe (N=360)\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering
Oceania (N=24)\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering
Total (N=1704)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.15\columnwidth}\raggedright
\textbf{GDP Per Capita}\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.15\columnwidth}\raggedright
~~~Mean (SD)\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
2194 (2828)\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
7136 (6397)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
7902 (14045)\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
14469 (9355)\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
18622 (6359)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
7215 (9857)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.15\columnwidth}\raggedright
~~~Range\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
241 - 21951\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
1202 - 42952\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
331 - 113523\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
974 - 49357\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
10040 - 34435\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
241 - 113523\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.15\columnwidth}\raggedright
\textbf{Life Expectancy}\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.15\columnwidth}\raggedright
~~~Mean (SD)\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
49 (9)\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
65 (9)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
60 (12)\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
72 (5)\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
74 (4)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
59 (13)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.15\columnwidth}\raggedright
~~~Range\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
24 - 76\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
38 - 81\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
29 - 83\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
44 - 82\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
69 - 81\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
24 - 83\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.15\columnwidth}\raggedright
\textbf{Population}\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.15\columnwidth}\raggedright
~~~Mean (SD)\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
9916003 (15490923)\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
24504795 (50979430)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
77038722 (206885205)\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
17169765 (20519438)\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
8874672 (6506342)\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
29601212 (106157897)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.15\columnwidth}\raggedright
~~~Range\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
60011 - 135031164\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
662850 - 301139947\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
120447 - 1318683096\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering
147962 - 82400996\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
1994794 - 20434176\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
60011 - 1318683096\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{learncheck}
\textbf{Exercise 5:} Replicate the code shown above to create a summary
table for the \texttt{States} data that reports across regions.
\end{learncheck}

\hypertarget{export-to-csv}{%
\subsection{Export to CSV}\label{export-to-csv}}

Knitting your notebook to HTML, Word, or PDF should produce a summary table in the appropriate format. Depending on our or others' workflow, we may want to export our summary table to CSV in order to easily open in Excel or other spreadsheet software. Arsenal can easily handle this.

To export my \texttt{gapminder} summary to CSV, I need to create a new object that contains the actual summary table. Below, I save the last summary to the object named \texttt{sum.table}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.table <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(sum.gapminder4, }\DataTypeTok{title =} \StringTok{"Summary Stats for Gapminder Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, I need to convert this table into a data frame using the \texttt{as.data.frame()} function like so.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.table <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(sum.table)}
\end{Highlighting}
\end{Shaded}

Lastly, I just need to save this data frame as a CSV file using the \texttt{write.csv()} function like so.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.csv}\NormalTok{(sum.table, }\DataTypeTok{file =} \StringTok{"sumtable.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

R will save the CSV file to my project folder. Otherwise, R will save the file to my current working directory.

\hypertarget{submit}{%
\section{Submit}\label{submit}}

Arsenal has many more options. The help page and links to tutorials can help you generate various types of summary tables.

Submit your notebook to eLC.

\hypertarget{r-visualization}{%
\chapter{R Visualization}\label{r-visualization}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter you will learn how to make the following visualizations:

\begin{itemize}
\tightlist
\item
  Histogram
\item
  Box plot
\item
  Bar chart
\item
  Line graph
\item
  Scatter plot
\end{itemize}

The code used to make the above visualizations in \ref{data-visualization} will be provided and explained. Then, you will be asked to replicate the visualization using different data.

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Start a notebook
\item
  Load the following packages and data
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(data.table) }\CommentTok{# contains fread function to import from URL}
\KeywordTok{library}\NormalTok{(fpp2)}
\NormalTok{countyComplete <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}\StringTok{'https://www.openintro.org/data/R/county_complete.R'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For everything but the line graph, we will use the \texttt{countyComplete} data within the \texttt{openintro} package. This dataset contains 3,143 counties and 53 variables. For the line graph, we will use the \texttt{prisonLF} data within the \texttt{fpp2} package. This dataset is a quarterly time series of prisoner numbers in Australia from 2005 to 2016, split by sex, state, and legal status.

\hypertarget{grammar-of-graphics}{%
\section{Grammar of graphics}\label{grammar-of-graphics}}

R uses the grammar of graphics to make visualizations. You need to define \textbf{three} essential elements to produce a graph:

\begin{itemize}
\tightlist
\item
  \texttt{data}: defines the dataset containing our variable(s) of interest
\item
  \texttt{aes}: defines the variables used to generate the plot and how they are used
\item
  \texttt{geom}: defines the kind of plot
\end{itemize}

We plot variables from \texttt{data} to \texttt{aes}thetic attributes of \texttt{geom}etric objects. The function we use to do this is called \texttt{ggplot}. The generic code below shows the essential elements that will produce a default plot. We replace \texttt{data} with the name of the dataset. Within the \texttt{aes} parentheses, we tell R to assign one or more variables to a variety of attributes, such as \texttt{x} or \texttt{y} or \texttt{color}. What we include within \texttt{aes} depends on the type of plot we want to make, which is determined by the second line which always includes \texttt{geom\_} followed by the type of plot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ variable, ...)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_type}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

For example, the below code takes the variable \texttt{gdpPercap} from the dataset \texttt{gapminder} and maps it to the horizontal x axis of a chosen \texttt{geom}etric object called a histogram.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gapminder, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ gdpPercap)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-154-1.pdf}

Data, aesthetics, and geometries are the essential elements needed to generate a graph. If you tell R these three elements correctly, it will produce a graph. There are additional elements that can be added to make graphs be more effective or look better that will be covered in class.

Aesthetics take variables in your data and assign them to attributes that correspond to the geometric object you intend to use. That is, \texttt{aes} and \texttt{geom} work together and must be compatible.

For example, you can't generate a scatter plot--\texttt{geom\_point}--if you only define an \texttt{x} aesthetic. You must define an \texttt{x} and \texttt{y} aesthetic for a scatter plot. The figure below lists various aesthetic attributes.

\includegraphics[width=10.24in]{images/ggaes}

The type of your variable also informs which aesthetic(s) to use. The following work well with visualizing continuous variables:

\begin{itemize}
\tightlist
\item
  x and y
\item
  size
\end{itemize}

The following aesthetics work well with visualizing categorical variables:

\begin{itemize}
\tightlist
\item
  labels
\item
  color and fill
\item
  shape
\item
  linetype
\item
  size
\end{itemize}

\hypertarget{histogram}{%
\section{Histogram}\label{histogram}}

Here is the code used to make the histogram from Chapter \ref{data-visualization}. The dataset is named \texttt{college\_grad\_students} and the variable assigned to the x axis aesthetic is named \texttt{grad\_median}, which is the median earnings of full-time employees with various graduate school majors. Then, a \texttt{geom\_histogram} is added. The rest of the code inside the histogram parentheses, the \texttt{labs} parentheses (stands for labels), and the \texttt{theme\_classic} is optional and used to make the histogram look more polished.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(college_grad_students, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ grad_median)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{30}\NormalTok{, }\DataTypeTok{fill =} \StringTok{'steelblue'}\NormalTok{, }\DataTypeTok{color =} \StringTok{'white'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{'Median earnings'}\NormalTok{, }\DataTypeTok{y =} \StringTok{'Count of graduate majors'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gradmedhistrep-1.pdf}
\caption{\label{fig:gradmedhistrep}Histogram of full-time median earnings for different graduate school majors}
\end{figure}

Here is the code for the same histogram without the optional code. This is all that is needed to generate a histogram. In general, all plots require very little code if we do not care what they look like.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(college_grad_students, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ grad_median)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-156-1.pdf}

\begin{learncheck}
\textbf{Exercise 1:} In the \texttt{countyComplete} dataset, there is a
variable named \texttt{bachelors\_2010} that measures the percent of the
county population with a bachelor's degree between 2006-2010. Suppose we
want to visualize the distribution of \texttt{bachelors} with a
histogram. Generate a simple, default histogram (no optional code unless
you want to add it).
\end{learncheck}

\hypertarget{box-plot}{%
\section{Box plot}\label{box-plot}}

Below is the code used for the box plot in Chapter \ref{data-visualization}. Like the histogram, a box plot visualizes the distribution of one variable but uses descriptive measures median, first and third quartile, and identifies extreme values. Therefore, we only need to tell R which variable to assign to the either the x or y axis. Note that I assign \texttt{grad\_median} to the y axis so that the box plot is vertical, which is merely a stylistic choice. Assigning \texttt{grad\_median} to the x axis would make the box plot horizontal. Then, \texttt{geom\_boxplot} is used to tell R to make a boxplot from \texttt{grad\_median}.

Again, all the code after \texttt{geom\_boxplot} is optional and was used to make the box plot look more polished. The \texttt{fill\ =\ \textquotesingle{}steelblue\textquotesingle{}} changes the color of the box, \texttt{labs} is used to help the reader understand what \texttt{grad\_median} measures, \texttt{theme\_classic} is one of several themes built within R that changes the look of a plot, and the code inside the \texttt{theme} function removes all of the ink related to the x axis due to it being unnecessary. The \texttt{theme} function allows you to control every element of a plot. Unique themes can be created and saved for replication, saving time and avoiding errors.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(college_grad_students, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ grad_median)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{fill =} \StringTok{'steelblue'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{y =} \StringTok{'Median earnings'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.line.x =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{axis.text.x =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{axis.ticks.x =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{data-apps-text_files/figure-latex/gradmedboxrep-1.pdf}
\caption{\label{fig:gradmedboxrep}Box plot of full-time median earnings for different graduate school majors}
\end{figure}

Again, if we do not care how the box plot looks, all we need to make the plot is shown in the code below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(college_grad_students, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ grad_median)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-159-1.pdf}

\begin{learncheck}
\textbf{Exercise 2:} Generate a simple, default box plot for
\texttt{bachelors\_2010} (no optional code unless you want to add it).
\end{learncheck}

\hypertarget{bar-chart}{%
\section{Bar chart}\label{bar-chart}}

There are two functions that make bar graphs: \texttt{geom\_bar} and \texttt{geom\_col}. Recall that a bar chart is used to show counts or proportions of levels within a categorical variable. In Chapter \ref{data-visualization}, a bar chart was used to show the counts and proportions of graduate majors defined as having either high or low unemployment. The table below shows a few rows and variables from the data. Note that these data are \textbf{disaggregated} with respect to the count of majors belonging to high or low unemployment. That is, we need our bar chart to count the number of rows with ``high'' and ``low'' in the \texttt{unemp\_cat} column. In this case, we should use \texttt{geom\_bar}.

major

grad\_total

grad\_unemployment\_rate

unemp\_cat

grad\_median

Public Administration

42661

0.059

high

75000

Political Science And Government

695725

0.039

low

92000

International Relations

69355

0.045

low

86000

Public Policy

15284

0.031

low

89000

Below is the code used to generate the side-by-side or dodged bar chart from Chapter \ref{data-visualization}. Note the use of \texttt{geom\_bar}, which requires either an x or y aesthetic to be defined. Here, I assign the categorical variable \texttt{unemp\_cat} to the x aesthetic, making the bar chart vertical. Assigning it to the y aesthetic would make the bar chart horizontal. Again, all the code past \texttt{geom\_bar} is optional.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gradschool, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ unemp_cat)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{fill =} \StringTok{'steelblue'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{y =} \StringTok{'Count of degrees'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.line.x =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{axis.ticks.x =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{axis.title.x =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-162-1.pdf}

Here is what the bar chart looks like without the optional code.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gradschool, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ unemp_cat)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-163-1.pdf}

Below is the code used to generate the stacked bar chart showing counts, Figure \ref{fig:empbar}. The code within \texttt{aes} is less intuitive. Since \texttt{geom\_bar} requires an x or y aesthetic to be defined, I have to assign x to nothing via the blank quotation marks. The \texttt{fill\ =\ unemp\_cat} tell R to stack the bar chart, filling the bar with the counts of high and low.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gradschool, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \StringTok{""}\NormalTok{, }\DataTypeTok{fill =}\NormalTok{ unemp_cat)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{y =} \StringTok{'Count of degrees'}\NormalTok{,}
       \DataTypeTok{fill =} \StringTok{'Unemployment'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.line.x =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{axis.text.x =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{axis.ticks.x =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{axis.title.x =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-164-1.pdf}

Below is what the stacked bar chart looks like by default.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gradschool, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \StringTok{""}\NormalTok{, }\DataTypeTok{fill =}\NormalTok{ unemp_cat)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-165-1.pdf}

Lastly, the code below is used to show proportions rather than counts. The only substantive difference between this code and the code above is the use of \texttt{position=\textquotesingle{}fill\textquotesingle{}} within the \texttt{geom\_bar} function. This tells R to show proportions.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gradschool, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \StringTok{""}\NormalTok{, }\DataTypeTok{fill =}\NormalTok{ unemp_cat)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{'fill'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{y =} \StringTok{'Proportion of degrees'}\NormalTok{,}
       \DataTypeTok{fill =} \StringTok{'Unemployment'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.line.x =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{axis.text.x =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{axis.ticks.x =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{axis.title.x =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-166-1.pdf}

\begin{learncheck}
\textbf{Exercise 3:} Suppose we want to visualize how many counties each
state has. That is, we want to count how many rows belong to each
\texttt{state} in the \texttt{countyComplete} data using a bar chart.
Generate a bar chart that achieves this. Choose the type of bar chart
you deem best.
\end{learncheck}

When should we use \texttt{geom\_col} instead? When our counts are already aggregated in our data. Refer back to the table above. Note that \texttt{grad\_total} and \texttt{grad\_median} contain the count of graduates within each major and their median pay, respectively. Therefore, we do not need R to count the number of rows in our data, but rather report each number already included in the data. The \texttt{geom\_col} function takes these counts and visualizes them using a bar (or column) chart.

The below code shows how to generate Figure \ref{fig:spiapay}, which visualized the median pay for the four majors in the data related to those offered by SPIA. Median pay is simply a number in the data that does not need counting, thus the code uses \texttt{geom\_col}, which requires both an x and y aesthetic to be defined. To allow room for the long names of each major, I made the bar chart horizontal by assigning \texttt{major} to the y aesthetic. Each bar visually represents the numbers for \texttt{grad\_median} in the above table.

There is a new piece of code in the below chunk that can be used to reorder bars in ascending or descending order, which is generally preferred over random order of peaks and valleys. The \texttt{reorder(major,\ -grad\_median)} code tells R to reorder the majors in the plot in ascending because of the minus sign; removing it would reverse the order to descending.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gradschool_spia, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =} \KeywordTok{reorder}\NormalTok{(major, }\OperatorTok{-}\NormalTok{grad_median), }\DataTypeTok{x =}\NormalTok{ grad_median)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{fill =} \StringTok{'steelblue'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{'Median pay'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.title.y =} \KeywordTok{element_blank}\NormalTok{(),}
    \DataTypeTok{axis.line.y =} \KeywordTok{element_blank}\NormalTok{(),}
    \DataTypeTok{axis.ticks.y =} \KeywordTok{element_blank}\NormalTok{()}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-168-1.pdf}

Below is what the bar chart looks like without the optional code.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gradschool_spia, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ major, }\DataTypeTok{x =}\NormalTok{ grad_median)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-169-1.pdf}

\hypertarget{scatter-plot}{%
\section{Scatter plot}\label{scatter-plot}}

The code below shows how the scatter plot in Chapter \ref{data-visualization} was generated. This scatter plot actually contains two geometric objects. The \texttt{geom\_point} function generates the scatter plot, and the \texttt{geom\_smooth} function generates the regression/trend line. Note the assignment of x and y aesthetics that every scatter plot requires. Everything beyond \texttt{geom\_point} is optional.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gradschool, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ grad_median, }\DataTypeTok{y =}\NormalTok{ grad_total)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{color =} \StringTok{'steelblue'}\NormalTok{, }\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{'lm'}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }
              \DataTypeTok{linetype =} \StringTok{'dashed'}\NormalTok{, }\DataTypeTok{color =} \StringTok{'black'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_log10}\NormalTok{(}\DataTypeTok{label=}\NormalTok{scales}\OperatorTok{::}\KeywordTok{comma_format}\NormalTok{()) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{y =} \StringTok{'Total degrees'}\NormalTok{,}
       \DataTypeTok{x =} \StringTok{'Median pay'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-170-1.pdf}

Below is the scatter plot without optional code.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gradschool, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ grad_median, }\DataTypeTok{y =}\NormalTok{ grad_total)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-171-1.pdf}

\begin{learncheck}
\textbf{Exercise 4:} Pick two variables in the \texttt{countyComplete}
data and plot their relationship using a simple scatter plot.
\end{learncheck}

\hypertarget{line-graph}{%
\section{Line graph}\label{line-graph}}

Line graphs are best for visualizing variables over time (i.e.~time series). The \texttt{prisonLF} data separate prisoner counts by male vs.~female, remanded vs.~sentenced, and state. Therefore, there are four times series for each state. The code below generates a line graph for the time series of female prisoners who were sentenced in each Australia state.

Note how this code is different from the code before because I need to manipulate it before creating the plot. Specifically, I pipe the \texttt{prisonLF} data into the \texttt{filter} verb, which keeps only the rows with \texttt{Female} and \texttt{Sentenced}. Then, I pipe that result into the typical \texttt{ggplot}. However, I do not need to specify the dataset because it is already being piped into \texttt{ggplot}. Therefore, \texttt{ggplot} only needs \texttt{aes} and \texttt{geom} to be defined. Time \texttt{t} is assigned to the \texttt{x} aesthetic, \texttt{count} is assigned to the \texttt{y} aesthetic, and \texttt{state} is assigned to the \texttt{color} aesthetic. The \texttt{color} aesthetic is a common way to plot multiple groups. It also provides a legend by default. The \texttt{geom\_line} function generates a line graph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prisonLF }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(gender }\OperatorTok{==}\StringTok{ 'Female'} \OperatorTok{&}\StringTok{ }\NormalTok{legal }\OperatorTok{==}\StringTok{ 'Sentenced'}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ t, }\DataTypeTok{y =}\NormalTok{ count, }\DataTypeTok{color =}\NormalTok{ state)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{color =} \StringTok{'State'}\NormalTok{, }\DataTypeTok{y =} \StringTok{'Female Sentenced Prisoners'}\NormalTok{, }\DataTypeTok{x =} \StringTok{''}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-173-1.pdf}

\begin{learncheck}
\textbf{Exercise 5:} Create a line graph for male prisoners who were
sentenced by state.
\end{learncheck}

\hypertarget{submit}{%
\section{Submit}\label{submit}}

Save your notebook and submit to eLC. Once you submit, answers will become available for download.

\hypertarget{r-regression}{%
\chapter{R Regression}\label{r-regression}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Run a regression model
\item
  Have R report key results
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Open a script
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(moderndive)}
\KeywordTok{library}\NormalTok{(Stat2Data)}
\end{Highlighting}
\end{Shaded}

We will use the \texttt{TeenPregnancy} dataset within the \texttt{Stat2Data} package. We need to manually load specific datasets from \texttt{Stat2Data} in order to use them. Therefore, run the following code, and the dataset should show up in your \texttt{Environment} pane in the top-right.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"TeenPregnancy"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar.

\hypertarget{running-regression}{%
\section{Running Regression}\label{running-regression}}

Chapters \ref{simple-and-multiple-regression} and \ref{categorical-variables-and-interactions} cover the following regression models:

\begin{itemize}
\tightlist
\item
  Simple linear regression with two numerical variables
\item
  Multiple linear regression with all numerical variables
\item
  Including a categorical explanatory variable (parallel slopes)
\item
  Regression with a categorical explanatory interacted with a numerical variable
\item
  Regression with a binary categorical outcome (linear probability model)
\end{itemize}

While our interpretation of results changes depending on the kind of regression model we use, the code to run a standard linear regression is the same regardless of the number and type of explanatory variables and whether the outcome variable is numerical or binary. With the exception of including an interaction, running the regression models listed above can be done with the same code structure shown below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new_object_name <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{exp_}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{exp_}\DecValTok{2} \OperatorTok{+}\StringTok{ }\NormalTok{... }\OperatorTok{+}\StringTok{ }\NormalTok{exp_k, }\DataTypeTok{data =}\NormalTok{ name_of_dataset)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  We name a new object that will hold the results of our regression
\item
  \texttt{lm} is the function for linear regression (acronym for linear model)
\item
  We replace \texttt{outcome} with the name of our outcome variable that should be either numerical or binary
\item
  The tilde \texttt{\textasciitilde{}} separates the outcome from the explanatory variables
\item
  We replace the \texttt{exp\_1} to \texttt{exp\_k} with the names of how ever many explanatory variables we wish to include, each separated by a plus sign \texttt{+}
\item
  We replace \texttt{name\_of\_dataset} with the name of the dataset that contains the variables for the regression model.
\end{itemize}

Recall the following multiple regression model from Chapter \ref{simple-and-multiple-regression}.

\begin{equation}
FedSpend = \beta_0 + \beta_1Poverty + \beta_2HomeOwn + \beta_3Income + \epsilon
\label{eq:multregexrep}
\end{equation}

I ran this regression using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fedpov2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(fed_spend }\OperatorTok{~}\StringTok{ }\NormalTok{poverty }\OperatorTok{+}\StringTok{ }\NormalTok{homeownership }\OperatorTok{+}\StringTok{ }\NormalTok{income, }\DataTypeTok{data =}\NormalTok{ selcounty)}
\end{Highlighting}
\end{Shaded}

That's all there is to it. I named the model \texttt{fedpov2} to remind myself it was the second model I ran to examine the relationship between federal spending and poverty rate. Note that the code within the \texttt{lm} function mimics Equation \eqref{eq:multregexrep}. No matter if the explanatory variables happen to be numerical or categorical, the regression works the same in R. Lastly, I did some behind-the-scenes cleaning of the original \texttt{county} data discussed in Chapter \ref{simple-and-multiple-regression} and named it \texttt{selcounty}. Therefore, I told R to use that dataset when running the regression.

\begin{learncheck}
\textbf{Exercise 1:} Suppose we want to use the \texttt{TeenPregnancy}
dataset to examine whether state teen pregnancy rates are associated
with church attendance and a state's role in the Civil War (admittedly
an odd variable to include but let's think of it as a proxy for region).
The model would be represented using the following formula

\begin{equation}
Teen = \beta_0 + \beta_1Church + \beta_2CivilWar + \epsilon
\end{equation}

Run this regression model.
\end{learncheck}

\hypertarget{reporting-results}{%
\section{Reporting Results}\label{reporting-results}}

This section presents two ways to obtain results after running a regression. The first uses functions that load with the \texttt{moderndive} package and the second uses functions that load with R by default (i.e.~Base R). The \texttt{moderndive} functions are somewhat more intuitive and produce results that look nicer, but the base R functions are more robust to any variety of regression model.

\hypertarget{moderndive}{%
\subsection{Moderndive}\label{moderndive}}

To get a standard table of regression results from using \texttt{moderndive}, we can use the \texttt{get\_regression\_table} function on our saved regression model results like so

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_regression_table}\NormalTok{(fedpov2)}
\end{Highlighting}
\end{Shaded}

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

23.519

1.333

17.645

0.000

20.905

26.132

poverty

-0.056

0.021

-2.674

0.008

-0.097

-0.015

homeownership

-0.126

0.012

-10.736

0.000

-0.149

-0.103

income

-0.086

0.011

-7.723

0.000

-0.108

-0.064

And to get goodness-of-fit measures, we can use the \texttt{get\_regression\_summaries} function like so

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_regression_summaries}\NormalTok{(fedpov2)}
\end{Highlighting}
\end{Shaded}

r\_squared

adj\_r\_squared

mse

rmse

sigma

statistic

p\_value

df

nobs

0.064

0.063

20.72216

4.55216

4.555

71.055

0

3

3123

and if I only want the R-squared, Adjusted R-squared, and RMSE from this table, I can add the \texttt{select} function to the above code chunk like so

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_regression_summaries}\NormalTok{(fedpov2) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(r_squared, adj_r_squared, rmse)}
\end{Highlighting}
\end{Shaded}

r\_squared

adj\_r\_squared

rmse

0.064

0.063

4.55216

\begin{learncheck}
\textbf{Exercise 2:} Produce a standard table of regression results and
goodness-of-fit measures for your regression model for teen pregnancy
rates using the \texttt{moderndive} functions.
\end{learncheck}

\hypertarget{base-r}{%
\subsection{Base R}\label{base-r}}

A comprehensive set of regression results can be obtained using the \texttt{summary} function on our regression model like so

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{options}\NormalTok{(}\DataTypeTok{scipen =} \DecValTok{999}\NormalTok{) }\CommentTok{# turns off scientific notation if necessary}
\KeywordTok{summary}\NormalTok{(fedpov2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = fed_spend ~ poverty + homeownership + income, data = selcounty)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -9.463 -2.502 -1.007  1.015 39.327 
## 
## Coefficients:
##               Estimate Std. Error t value             Pr(>|t|)    
## (Intercept)   23.51860    1.33290  17.645 < 0.0000000000000002 ***
## poverty       -0.05597    0.02093  -2.674              0.00752 ** 
## homeownership -0.12582    0.01172 -10.736 < 0.0000000000000002 ***
## income        -0.08593    0.01113  -7.723   0.0000000000000152 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4.555 on 3119 degrees of freedom
## Multiple R-squared:  0.06397,	Adjusted R-squared:  0.06307 
## F-statistic: 71.06 on 3 and 3119 DF,  p-value: < 0.00000000000000022
\end{verbatim}

which gives us most of the information from \texttt{get\_regression\_table} except for the confidence intervals as well as R-squared and Adjusted R-squared. The Residual standard error is not exactly the same as the RMSE above--it is actually equal to \texttt{sigma} in the full table from \texttt{get\_regression\_summaries}--but you can treat them the same. The difference between the two is statistically technical and obtaining the same RMSE as the table above requires too much new code.

To get the confidence intervals, we can use the \texttt{confint} function like so,

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(fedpov2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                     2.5 %      97.5 %
## (Intercept)   20.90515522 26.13203853
## poverty       -0.09700107 -0.01493693
## homeownership -0.14880274 -0.10284564
## income        -0.10774450 -0.06411382
\end{verbatim}

which uses the 95\% confidence level by default.

\begin{learncheck}
\textbf{Exercise 3:} Generate the key regression results and
goodness-of-fit measures for your regression model for teen pregnancy
rates using the Base R functions.
\end{learncheck}

\hypertarget{submit}{%
\section{Submit}\label{submit}}

Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download.

\hypertarget{r-interactions}{%
\chapter{R Interactions}\label{r-interactions}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Include an interaction of two explanatory variables in a regression model
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Open a script
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(moderndive)}
\KeywordTok{library}\NormalTok{(carData)}
\end{Highlighting}
\end{Shaded}

We will use the \texttt{Salaries} dataset within the \texttt{carData} package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar.

\hypertarget{including-an-interaction}{%
\section{Including an interaction}\label{including-an-interaction}}

Though we only cover interacting a numerical variable with a categorical variable in this course, we can interact two variables of any type using the same code. In theory, we can interact more than two variables. In any case, an interaction requires us to multiply the variables within the \texttt{lm} function.

Recall the regression model from Chapter \ref{categorical-variables-and-interactions} where \texttt{mrall} is traffic fatality rate, \texttt{vmiles} is the average miles driven, and \texttt{jaild} is whether a state imposes mandatory jail for drunk driving.

\begin{equation}
mrall = \beta_0 + \beta_1 vmiles + \beta_2 jaild + \beta_3 vmiles \times jaild + \epsilon
\end{equation}

We can run this regression using the following code

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{interactmod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(mrall }\OperatorTok{~}\StringTok{ }\NormalTok{vmiles }\OperatorTok{+}\StringTok{ }\NormalTok{jaild }\OperatorTok{+}\StringTok{ }\NormalTok{vmiles}\OperatorTok{*}\NormalTok{jaild, }\DataTypeTok{data =}\NormalTok{ trdeath)}
\end{Highlighting}
\end{Shaded}

Note that the only difference from the code in the previous chapter is \texttt{vmiles*jaild}, which tells R to interact the two variables. Once again, the code reflects the equation.

We can obtain the results of this regression using the functions we learned in the previous chapter.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_regression_table}\NormalTok{(interactmod)}
\end{Highlighting}
\end{Shaded}

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

-0.384

0.221

-1.741

0.083

-0.819

0.050

vmiles

0.300

0.028

10.634

0.000

0.245

0.356

jaildyes

0.731

0.399

1.831

0.068

-0.054

1.516

vmiles:jaildyes

-0.058

0.050

-1.178

0.240

-0.156

0.039

\begin{learncheck}
\textbf{Exercise 1:} Suppose we want to use the \texttt{Salaries}
dataset to examine whether professor salary is associated with their sex
and how long they have worked at the institution. Furthermore, suppose
we theorize that the association between salary and how long they have
worked at the insitution differs by sex, thus requiring an interaction.
Therefore, we have the following model.

\begin{equation}
salary = \beta_0 + \beta_1sex + \beta_2yrs.service + \beta_3 sex \times yrs.service + \epsilon
\end{equation}

Run this regression model and obtain the results.
\end{learncheck}

\hypertarget{submit}{%
\section{Submit}\label{submit}}

Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download.

\hypertarget{r-nonlinear-regression}{%
\chapter{R Nonlinear Regression}\label{r-nonlinear-regression}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Run a regression with a quadratic term
\item
  Run a regression with log transformations
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Open a script
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(moderndive)}
\KeywordTok{library}\NormalTok{(carData)}
\end{Highlighting}
\end{Shaded}

We will use the \texttt{Mroz} dataset within the \texttt{carData} package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar.

\hypertarget{quadratic-term}{%
\section{Quadratic term}\label{quadratic-term}}

Recall the below regression model from Chapter \ref{nonlinear-variables} that includes a squared term for \texttt{Age}, which allows our regression line to change directions once as \texttt{Age} changes. We included this term because Figure \ref{fig:quadscatter} suggested wages initially increase with age, then decreases.

\begin{equation}
Wage = \beta_0 + \beta_1Age + \beta_2Age^2 + \beta_3Educ + \epsilon
\end{equation}

The below code demonstrates how to include a quadratic term within the \texttt{lm} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quad_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Wage }\OperatorTok{~}\StringTok{ }\NormalTok{Age }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(Age}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{Educ, }\DataTypeTok{data =}\NormalTok{ wages)}
\end{Highlighting}
\end{Shaded}

In this case, the code reflects the equation only somewhat; the \texttt{I()} is necessary to tell R that \texttt{Age\^{}2} is the squared version of \texttt{Age}. Otherwise, R would not recognize \texttt{Age\^{}2} in the data, thus excluding it from the regression.

Now we can obtain results in the usual manner.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_regression_table}\NormalTok{(quad_mod)}
\end{Highlighting}
\end{Shaded}

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

-22.722

3.023

-7.517

0

-28.742

-16.701

Age

1.350

0.134

10.077

0

1.083

1.617

I(Age\^{}2)

-0.013

0.001

-9.840

0

-0.016

-0.011

Educ

1.254

0.090

13.990

0

1.075

1.432

We need to alter the \texttt{Mroz} data slightly before running a regression. Run the following code that creates a new variable that equals 1 if \texttt{lfp} equals ``yes'' and 0 if \texttt{lfp} equals ``no.'' This is necessary because our outcome variable--even though categorical--must be represented numerically in order for the regression to work.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_Mroz <-}\StringTok{ }\NormalTok{Mroz }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{lfp_numeric =} \KeywordTok{if_else}\NormalTok{(lfp }\OperatorTok{==}\StringTok{ "yes"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{learncheck}
\textbf{Exercise 1:} Suppose we want to examine factors that explain
whether married women participate in the labor force, which is a binary
outcome. We use the following model:

\begin{equation}
lfp = \beta_0 + \beta_1k5 + \beta_2age + \beta_3age^2 + \beta_4wc + \beta_5lwg + \beta_6inc + \epsilon
\end{equation}

Run this regression model and obtain the results.
\end{learncheck}

\hypertarget{log-transformation}{%
\section{Log Transformation}\label{log-transformation}}

In Chapter \ref{nonlinear-variables}, the following log-log regression model was run.

\begin{equation}
ln(LifeExp)=\beta_0 + \beta_1ln(GDPpercap) + \beta_2Continent + \epsilon
\end{equation}

The below code demonstrates how to transform a variable into its natural log within the \texttt{lm} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loglog <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(lifeExp) }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(gdpPercap) }\OperatorTok{+}\StringTok{ }\NormalTok{continent, }\DataTypeTok{data =}\NormalTok{ gapminder)}
\end{Highlighting}
\end{Shaded}

Note that all we need to do is place the appropriate variables within the \texttt{log()} function, which R interprets as the natural log. This \emph{temporarily} transforms the variables; it does not create new variables in the dataset equal to the natural log of the variables.

Now we can obtain results in the usual manner.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_regression_table}\NormalTok{(loglog)}
\end{Highlighting}
\end{Shaded}

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

3.062

0.026

117.692

0

3.011

3.113

log(gdpPercap)

0.112

0.004

31.843

0

0.105

0.119

continentAmericas

0.133

0.011

12.519

0

0.112

0.154

continentAsia

0.110

0.009

12.037

0

0.092

0.128

continentEurope

0.166

0.012

14.357

0

0.143

0.189

continentOceania

0.152

0.029

5.187

0

0.095

0.210

\begin{learncheck}
\textbf{Exercise 2:} Suppose we decide we want to use the natural log of
family income exclusive of wife's income, \texttt{inc}, resulting in the
following model

\begin{equation}
lfp = \beta_0 + \beta_1k5 + \beta_2age + \beta_3age^2 + \beta_4wc + \beta_5lwg + \beta_6ln(inc) + \epsilon
\end{equation}

Run this regression model and obtain the results.
\end{learncheck}

\hypertarget{submit}{%
\section{Submit}\label{submit}}

Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download.

\hypertarget{r-evaluations}{%
\chapter{R Evaluations}\label{r-evaluations}}

\hypertarget{learning-outcomes}{%
\section{Learning Outcomes}\label{learning-outcomes}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Conduct a chi-square test
\item
  Conduct an independent t-test
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Open a script
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(carData)}
\KeywordTok{library}\NormalTok{(MASS)}
\end{Highlighting}
\end{Shaded}

For the chi-square test, we will use the \texttt{MplsStops} dataset within the \texttt{carData} package. For the t-tests, we will use the \texttt{UScrime} dataset within the \texttt{MASS} package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar.

\hypertarget{chi-square-test}{%
\section{Chi-square test}\label{chi-square-test}}

A chi-square test, like the one demonstrated in Chapter \ref{surveys-and-evaluations}, requires two steps:

\begin{itemize}
\tightlist
\item
  Create a cross-tabulation table using the \texttt{table} function
\item
  Run the chi-square on the cross-tabulation using the \texttt{chisq.test} function
\end{itemize}

\hypertarget{cross-tab}{%
\subsection{Cross-tab}\label{cross-tab}}

Below is the code used to produce the cross-tab from Chapter \ref{surveys-and-evaluations}. I save the new table as \texttt{polltable}. Using the \texttt{table} function, I tell R which two variables from the \texttt{poll} dataset to cross-tabulate. The \texttt{\$} is how we identify a specific variable within a dataset. The levels of the first variable, \texttt{response}, will be tabulated by row, while the frequency of the levels of the second variable, \texttt{party}, will be tabulated by column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{polltable <-}\StringTok{ }\KeywordTok{table}\NormalTok{(poll}\OperatorTok{$}\NormalTok{response, poll}\OperatorTok{$}\NormalTok{party)}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-208}Response by political party

Republican

Democrat

Independent

Apply for citizenship

57

101

120

Guest worker

121

28

113

Leave the country

179

45

126

\hypertarget{run-chi-square}{%
\subsection{Run chi-square}\label{run-chi-square}}

Now that we have a cross-tabulation table, we can run the chi-square test. The code below demonstrates how.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{chisq.test}\NormalTok{(immigration_poll)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 	Pearson's Chi-squared test
## 
## data:  immigration_poll
## X-squared = 100.95, df = 4, p-value < 0.00000000000000022
\end{verbatim}

Then, it is simply a matter of interpreting the results.

\begin{learncheck}
\textbf{Exercise 1:} Using the \texttt{MplsStops} data, suppose we
wanted to test whether receiving a citation after being stopped by the
police, \texttt{citationIssued}, is independent of \texttt{race.} Both
are nominal variables, so a chi-square test can be used. Run this
chi-square test.
\end{learncheck}

\begin{learncheck}
\textbf{Exercise 2:} Are the two variables independent? Why?
\end{learncheck}

\hypertarget{t-tests}{%
\section{T-tests}\label{t-tests}}

To reiterate, if the two groups in a t-test are comprised of different subjects, we use an independent t-test. If they are comprised of the same subjects, then we use a dependent t-test.

\hypertarget{independent-t-test}{%
\subsection{Independent t-test}\label{independent-t-test}}

The code below demonstrates how the independent t-test from Chapter \ref{surveys-and-evaluations} was conducted. The \texttt{t.test} function works a lot like the \texttt{lm} function in that the outcome is entered first, then we input the variable that identifies the groups, which is essentially an explanatory variable. The two variables are separated by \texttt{\textasciitilde{}}. Then, we tell R which dataset to use, which is called \texttt{jobtrain} in this case.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(earnings }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data =}\NormalTok{ jobtrain)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 	Welch Two Sample t-test
## 
## data:  earnings by treatment
## t = -1.1921, df = 275.58, p-value = 0.2342
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -11629.708   2856.939
## sample estimates:
## mean in group 0 mean in group 1 
##        21645.10        26031.49
\end{verbatim}

\begin{learncheck}
\textbf{Exercise 3:} Using the \texttt{UScrimes} data, suppose we wanted
to test whether the probability of imprisonment, \texttt{Prob}, is
independent of between Southern and non-Southern states, \texttt{So}.
The outcome is numerical and the explanatory is nominal. Therefore, a
t-test can be used. Run this t-test.
\end{learncheck}

\begin{learncheck}
\textbf{Exercise 4:} Is there an association between the two variables?
Why?
\end{learncheck}

\hypertarget{dependent-t-test}{%
\subsection{Dependent t-test}\label{dependent-t-test}}

To conduct a dependent t-test, add the option \texttt{paired=TRUE} inside the \texttt{t.test} code like so

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(earnings }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data =}\NormalTok{ jobtrain, }\DataTypeTok{paired =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

However, this code will not work because the number of observations in the treatment and control groups are not equal. If we truly had a paired sample where the same subjects measured twice, then we should have the same number of observations in both groups.

\hypertarget{submit}{%
\section{Submit}\label{submit}}

Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download.

\hypertarget{r-regression-diagnostics}{%
\chapter{R Regression Diagnostics}\label{r-regression-diagnostics}}

\hypertarget{learning-outcomes}{%
\section{Learning Outcomes}\label{learning-outcomes}}

In this chapter, you will learn how to:

\begin{itemize}
\tightlist
\item
  Produce residual vs.~fitted (RVFP) and residual vs.~leverage plots (RVLP)
\item
  Check for multicollinearity using variance inflation factor (VIF)
\item
  Exclude observations from a regression model
\end{itemize}

\hypertarget{set-up}{%
\section{Set-up}\label{set-up}}

To complete this chapter, you need to

\begin{itemize}
\tightlist
\item
  Open a script
\item
  Load the following packages
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(car)}
\KeywordTok{library}\NormalTok{(gvlma)}
\KeywordTok{library}\NormalTok{(carData)}
\end{Highlighting}
\end{Shaded}

We will use the \texttt{States} dataset within the \texttt{carData} package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar.

\hypertarget{rvf-and-rvl}{%
\section{RVF and RVL}\label{rvf-and-rvl}}

Producing RVF and RVL plots is very easy. In Chapter \ref{regression-diagnostics}, the following regression was run.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{murder_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(murder }\OperatorTok{~}\StringTok{ }\NormalTok{pctmetro }\OperatorTok{+}\StringTok{ }\NormalTok{pcths }\OperatorTok{+}\StringTok{ }\NormalTok{poverty, }\DataTypeTok{data =}\NormalTok{ statecrime1997)}
\KeywordTok{get_regression_table}\NormalTok{(murder_mod)}
\end{Highlighting}
\end{Shaded}

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

-81.538

27.317

-2.985

0.004

-136.493

-26.583

pctmetro

0.180

0.051

3.541

0.001

0.078

0.282

pcths

0.657

0.298

2.203

0.032

0.057

1.256

poverty

1.971

0.364

5.413

0.000

1.238

2.703

Once we have our regression results saved to an object, all we need to do is use the \texttt{plot} function, as shown in the code below. The \texttt{plot} function produces four plots. The first plot is the RVFP and the fourth plot is the RVLP. You do not need to concern yourself with the second and third plots.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(murder_mod)}
\end{Highlighting}
\end{Shaded}

\includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-220-1.pdf} \includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-220-2.pdf} \includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-220-3.pdf} \includegraphics{data-apps-text_files/figure-latex/unnamed-chunk-220-4.pdf}

In both plots, we see that observation 51 (D.C.) is particularly problematic.

\begin{learncheck}
\textbf{Exercise 1:} Using the \texttt{States} data, run a regression
model where either \texttt{SATV} or \texttt{SATM} is the outcome. Once
you have the model, produce the RVFP and RVLP. Do any observations
appear problematic?
\end{learncheck}

\hypertarget{vif}{%
\section{VIF}\label{vif}}

VIF is a common way to check for excessive multicollinearity. There is no strict rule for identifying multicollinearity, but a VIF between 5 and 10 signals a potential problem. To obtain the VIF, we can use the \texttt{vif} function from the \texttt{car} package like so.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vif}\NormalTok{(murder_mod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## pctmetro    pcths  poverty 
##   1.0091   2.2515   2.2598
\end{verbatim}

None of the VIF values for the explanatory variables come close to 5. Therefore, we can be confident that multicollinearity is not an issue.

\begin{learncheck}
\textbf{Exercise 2:} Obtain VIF values for your regression model. Might
multicollinearity be a concern?
\end{learncheck}

\hypertarget{exclude-observations}{%
\section{Exclude observations}\label{exclude-observations}}

We should be judicious and transparent when deciding to exclude observations from an analysis. When in doubt, do not exclude observations. If we decide an exclusion is defensible, then we can exclude observations directly within the \texttt{lm} function to avoid the need to create a new dataset. In the code below, I exclude observation 51 from the regression model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{murder_mod2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(murder }\OperatorTok{~}\StringTok{ }\NormalTok{pctmetro }\OperatorTok{+}\StringTok{ }\NormalTok{pcths }\OperatorTok{+}\StringTok{ }\NormalTok{poverty, }\DataTypeTok{data =}\NormalTok{ statecrime1997[}\OperatorTok{-}\DecValTok{51}\NormalTok{,])}
\end{Highlighting}
\end{Shaded}

Now I have different results that are more representative of state murder rates.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_regression_table}\NormalTok{(murder_mod)}
\end{Highlighting}
\end{Shaded}

term

estimate

std\_error

statistic

p\_value

lower\_ci

upper\_ci

intercept

-81.538

27.317

-2.985

0.004

-136.493

-26.583

pctmetro

0.180

0.051

3.541

0.001

0.078

0.282

pcths

0.657

0.298

2.203

0.032

0.057

1.256

poverty

1.971

0.364

5.413

0.000

1.238

2.703

\begin{learncheck}
\textbf{Exercise 3:} Exclude one or more observations from your
regression model.
\end{learncheck}

\hypertarget{submit}{%
\section{Submit}\label{submit}}

Please save your script using your last name and submit to eLC. Once you submit, answers will become available for download.

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{wrangle-and-tidy-reference}{%
\chapter{Wrangle and Tidy Reference}\label{wrangle-and-tidy-reference}}

Unless data are already perfectly prepared, the most time consuming part of data analysis is wrangling and tidying data. It is impossible to cover all scenarios one may encounter when preparing raw data for an analysis. Even for advanced users of R, it is not uncommon to search for an unknown solution to a new problem via the web, texts, or manuals. Attempting to memorize the plethora of functions in R that could serve as solutions would quickly result in diminishing returns. Instead, it is more realistic to obtain enough familiarity with basic wrangle and tidy problems and solutions that one knows where how to effectively search for the solution.

\hypertarget{cheatsheets}{%
\section{Cheatsheets}\label{cheatsheets}}

RStudio provides numerous \href{https://rstudio.com/resources/cheatsheets/}{cheatsheets} to help R users reference commonly used and helpful functions. Below is a list of cheatsheets that pertain to wrangling and tidying and mostly accessible for new users.

This is the most relevant cheatsheet for what you will encounter in the course:

\begin{itemize}
\tightlist
\item
  \href{https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf}{Data transformation}
\end{itemize}

Here are some more that are less relevant:

\begin{itemize}
\tightlist
\item
  \href{https://github.com/rstudio/cheatsheets/raw/master/factors.pdf}{Factors}
\item
  \href{https://github.com/rstudio/cheatsheets/raw/master/strings.pdf}{Working with string variables}
\item
  \href{https://github.com/rstudio/cheatsheets/raw/master/lubridate.pdf}{Dates and times}
\end{itemize}

\hypertarget{common-functions}{%
\section{Common functions}\label{common-functions}}

Knowing just a handful of functions can help you make considerable progress in many situations. The remainder of this chapter serves as a sort of cheatsheet that distills some of the most common and relevant functions useful for problems you may encounter during the course. Functions are demonstrated using the friendly \texttt{gapminder} data.

The \texttt{tidyverse} package is actually a collection of several \texttt{packages} designed to make the wrangle, tidy, and data exploration process as intuitive and consistent as possible. You should almost always load \texttt{tidyverse}, as it contains every function you may need to wrangle and tidy data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\hypertarget{wrangle-verbs}{%
\subsection{Wrangle Verbs}\label{wrangle-verbs}}

\begin{itemize}
\tightlist
\item
  filter: extract rows/cases
\item
  select: extract columns/variables
\item
  mutate: alter existing variables or create new variables
\item
  arrange: reorder rows in ascending or descending order of one or more variables
\item
  head/tail: extract the top/bottom number of rows
\item
  summarize: collapses data into a table of summary statistics
\item
  group\_by: tells R to apply functions to each group separately; common to use with summarize
\end{itemize}

\hypertarget{filter}{%
\subsubsection*{Filter}\label{filter}}
\addcontentsline{toc}{subsubsection}{Filter}

Use \texttt{filter} to extract rows from a dataset. Inversely, one can think of \texttt{filter} as a way to remove rows. However, remember that \texttt{filter} keeps the rows that meet the condition on which you filter. Therefore, you want to use a condition that keeps the rows you want.

Note there are 1,704 rows in the \texttt{gapminder} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(gapminder)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,704
## Columns: 6
## $ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afgha...
## $ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi...
## $ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 199...
## $ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 4...
## $ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 14880372,...
## $ gdpPercap <dbl> 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.113...
\end{verbatim}

Suppose I want to keep only countries in Asia. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(continent }\OperatorTok{==}\StringTok{ 'Asia'}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 396
## Columns: 6
## $ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afgha...
## $ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi...
## $ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 199...
## $ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 4...
## $ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 14880372,...
## $ gdpPercap <dbl> 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.113...
\end{verbatim}

The result is a new dataset with 396 rows. Note the use of double equal signs \texttt{==} to tell R it is a conditional (``if equal to'') rather than setting something equal to something else, which would not make sense in this case.

Suppose I want countries in Asia \textbf{AND} in the year 1952. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(continent }\OperatorTok{==}\StringTok{ 'Asia'} \OperatorTok{&}\StringTok{ }\NormalTok{year }\OperatorTok{==}\StringTok{ }\DecValTok{1952}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 33
## Columns: 6
## $ country   <fct> "Afghanistan", "Bahrain", "Bangladesh", "Cambodia", "Chin...
## $ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi...
## $ year      <int> 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 195...
## $ lifeExp   <dbl> 28.801, 50.939, 37.484, 39.417, 44.000, 60.960, 37.373, 3...
## $ pop       <int> 8425333, 120447, 46886859, 4693836, 556263527, 2125900, 3...
## $ gdpPercap <dbl> 779.4453, 9867.0848, 684.2442, 368.4693, 400.4486, 3054.4...
\end{verbatim}

This results in a new dataset with 33 rows. Note the use of the ampersand \texttt{\&} to code the ``and'' conditional.

Suppose I want countries in Asia with a life expectancy less than or equal to 40 in 1952. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(continent }\OperatorTok{==}\StringTok{ 'Asia'} \OperatorTok{&}\StringTok{ }\NormalTok{year }\OperatorTok{==}\StringTok{ }\DecValTok{1952} \OperatorTok{&}\StringTok{ }\NormalTok{lifeExp }\OperatorTok{<=}\StringTok{ }\DecValTok{40}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 10
## Columns: 6
## $ country   <fct> "Afghanistan", "Bangladesh", "Cambodia", "India", "Indone...
## $ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia
## $ year      <int> 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952
## $ lifeExp   <dbl> 28.801, 37.484, 39.417, 37.373, 37.468, 36.319, 36.157, 3...
## $ pop       <int> 8425333, 46886859, 4693836, 372000000, 82052000, 20092996...
## $ gdpPercap <dbl> 779.4453, 684.2442, 368.4693, 546.5657, 749.6817, 331.000...
\end{verbatim}

Suppose I all countries in 1952 except those in Asia. There are a few options to do this. Which option is most efficient depends on the specific case. In this case:

\textbf{Option 1: Using the ``or'' conditional \texttt{\textbar{}} (least efficient)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(continent }\OperatorTok{==}\StringTok{ 'Africa'} \OperatorTok{|}\StringTok{ }\NormalTok{continent }\OperatorTok{==}\StringTok{ 'Americas'} \OperatorTok{|}\StringTok{ }\NormalTok{continent }\OperatorTok{==}\StringTok{ 'Europe'} \OperatorTok{|}\StringTok{ }\NormalTok{continent }\OperatorTok{==}\StringTok{ 'Oceania'}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,308
## Columns: 6
## $ country   <fct> Albania, Albania, Albania, Albania, Albania, Albania, Alb...
## $ continent <fct> Europe, Europe, Europe, Europe, Europe, Europe, Europe, E...
## $ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 199...
## $ lifeExp   <dbl> 55.230, 59.280, 64.820, 66.220, 67.690, 68.930, 70.420, 7...
## $ pop       <int> 1282697, 1476505, 1728137, 1984060, 2263554, 2509048, 278...
## $ gdpPercap <dbl> 1601.056, 1942.284, 2312.889, 2760.197, 3313.422, 3533.00...
\end{verbatim}

\textbf{Option 2: Using the shortcut \texttt{\%in\%} for multiple ``or'' conditionals (moderately efficient)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(continent }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'Africa'}\NormalTok{, }\StringTok{'Americas'}\NormalTok{, }\StringTok{'Europe'}\NormalTok{, }\StringTok{'Oceania'}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,308
## Columns: 6
## $ country   <fct> Albania, Albania, Albania, Albania, Albania, Albania, Alb...
## $ continent <fct> Europe, Europe, Europe, Europe, Europe, Europe, Europe, E...
## $ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 199...
## $ lifeExp   <dbl> 55.230, 59.280, 64.820, 66.220, 67.690, 68.930, 70.420, 7...
## $ pop       <int> 1282697, 1476505, 1728137, 1984060, 2263554, 2509048, 278...
## $ gdpPercap <dbl> 1601.056, 1942.284, 2312.889, 2760.197, 3313.422, 3533.00...
\end{verbatim}

\textbf{Option 3: Use the ``not equal to'' conditional \texttt{!=} (most efficient)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(continent }\OperatorTok{!=}\StringTok{ 'Asia'}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,308
## Columns: 6
## $ country   <fct> Albania, Albania, Albania, Albania, Albania, Albania, Alb...
## $ continent <fct> Europe, Europe, Europe, Europe, Europe, Europe, Europe, E...
## $ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 199...
## $ lifeExp   <dbl> 55.230, 59.280, 64.820, 66.220, 67.690, 68.930, 70.420, 7...
## $ pop       <int> 1282697, 1476505, 1728137, 1984060, 2263554, 2509048, 278...
## $ gdpPercap <dbl> 1601.056, 1942.284, 2312.889, 2760.197, 3313.422, 3533.00...
\end{verbatim}

\hypertarget{select}{%
\subsubsection*{Select}\label{select}}
\addcontentsline{toc}{subsubsection}{Select}

Suppose I want a dataset that contains only country, continent, year, and life expectancy. There are multiple options. Which is more efficient depends on the specific case. In this case:

\textbf{Option 1: List the variables I want to keep (least efficient)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(country, continent, year, lifeExp) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,704
## Columns: 4
## $ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afgha...
## $ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi...
## $ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 199...
## $ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 4...
\end{verbatim}

\textbf{Option 2: List the variables I don't want to keep (moderately efficient)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{pop, }\OperatorTok{-}\NormalTok{gdpPercap) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,704
## Columns: 4
## $ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afgha...
## $ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi...
## $ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 199...
## $ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 4...
\end{verbatim}

\textbf{Option 3: Use \texttt{:} to specify the range of variables, which only works because the variables I want happen to be stored next to each other (most efficient)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(country}\OperatorTok{:}\NormalTok{lifeExp) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,704
## Columns: 4
## $ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afgha...
## $ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi...
## $ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 199...
## $ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 4...
\end{verbatim}

\hypertarget{mutate}{%
\subsubsection*{Mutate}\label{mutate}}
\addcontentsline{toc}{subsubsection}{Mutate}

The \texttt{mutate} function allows you to mutate your dataset by either changing an existing variable or creating a new one.

Suppose I wanted to change GDP per capita so that it is expressed in thousands of dollars instead of dollars. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{gdpPercap =}\NormalTok{ gdpPercap}\OperatorTok{/}\DecValTok{1000}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,704
## Columns: 6
## $ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afgha...
## $ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi...
## $ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 199...
## $ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 4...
## $ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 14880372,...
## $ gdpPercap <dbl> 0.7794453, 0.8208530, 0.8531007, 0.8361971, 0.7399811, 0....
\end{verbatim}

Note that I use the name of an existing variable on the left-hand side of the equation. This overwrites the data according to the function I have specified. You can scroll up to previous glimpses to confirm that gdpPercap has indeed been divided by 1,000.

Suppose I wanted a new variable that measures total GDP to have in addition to GDP per capita expressed in thousands. Since GDP per capita equals GDP divided by population, I can simply use the inverse of this calculation. Thus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{gdpPercap =}\NormalTok{ gdpPercap}\OperatorTok{/}\DecValTok{1000}\NormalTok{,}
         \DataTypeTok{gdp =}\NormalTok{ gdpPercap}\OperatorTok{*}\NormalTok{pop) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,704
## Columns: 7
## $ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afgha...
## $ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi...
## $ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 199...
## $ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 4...
## $ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 14880372,...
## $ gdpPercap <dbl> 0.7794453, 0.8208530, 0.8531007, 0.8361971, 0.7399811, 0....
## $ gdp       <dbl> 6567086, 7585449, 8758856, 9648014, 9678553, 11697659, 12...
\end{verbatim}

Since \texttt{mutate} applies mathematical functions, there are way too many possible uses to cover here. The second page of the \href{https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf}{Data transformation} cheatsheet lists numerous common functions used with \texttt{mutate} under the ``Vector Functions'' header. Also, the first page of the cheatsheet lists a few different versions of \texttt{mutate} that can come in handy. One particularly helpful variation is \texttt{mutate\_at}.

Suppose there are multiple variables you want to mutate using the same formula. A common example is when a bunch of variables are expressed as proportions between 0 and 1 when you want them all to be expressed as percentages between 0 and 100. You could list each mutate individually, but this quickly becomes tedious. Instead, you can use \texttt{mutate\_at} to list the variables you want to mutate, then define the function you want applied to them.

For example, suppose I wanted to multiply all of the numerical variables in \texttt{gapminder} by 100 (doesn't make sense but just go with it). Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate_at}\NormalTok{(}\KeywordTok{vars}\NormalTok{(year, lifeExp, pop, gdpPercap), }\KeywordTok{funs}\NormalTok{(.}\OperatorTok{*}\DecValTok{100}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,704
## Columns: 6
## $ country   <fct> Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afgha...
## $ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi...
## $ year      <dbl> 195200, 195700, 196200, 196700, 197200, 197700, 198200, 1...
## $ lifeExp   <dbl> 2880.1, 3033.2, 3199.7, 3402.0, 3608.8, 3843.8, 3985.4, 4...
## $ pop       <dbl> 842533300, 924093400, 1026708300, 1153796600, 1307946000,...
## $ gdpPercap <dbl> 77944.53, 82085.30, 85310.07, 83619.71, 73998.11, 78611.3...
\end{verbatim}

The period \texttt{.} inside \texttt{funs} is a generic placeholder, telling R to multiply each of the variables inside \texttt{vars} by 100.

\hypertarget{combining-filter-select-and-mutate}{%
\subsubsection*{Combining filter, select, and mutate}\label{combining-filter-select-and-mutate}}
\addcontentsline{toc}{subsubsection}{Combining filter, select, and mutate}

You can do some serious wrangling efficiently with filter, select, and mutate. Suppose I wanted a new dataset of GDP (in billions) for European countries in 2007.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{euro_gdp07 <-}\StringTok{ }\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(continent }\OperatorTok{==}\StringTok{ 'Europe'} \OperatorTok{&}\StringTok{ }\NormalTok{year }\OperatorTok{==}\StringTok{ }\DecValTok{2007}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{gdp =}\NormalTok{ (gdpPercap}\OperatorTok{*}\NormalTok{pop)}\OperatorTok{/}\DecValTok{1000000000}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(country, year, gdp)}

\KeywordTok{glimpse}\NormalTok{(euro_gdp07)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 30
## Columns: 3
## $ country <fct> Albania, Austria, Belgium, Bosnia and Herzegovina, Bulgaria...
## $ year    <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,...
## $ gdp     <dbl> 21.376411, 296.229401, 350.141167, 33.897027, 78.213929, 65...
\end{verbatim}

\hypertarget{arrange}{%
\subsubsection*{Arrange}\label{arrange}}
\addcontentsline{toc}{subsubsection}{Arrange}

The \texttt{arrange} verb is useful if you want to identify cases that have the highest or lowest values for one or more variables. By default, \texttt{arrange} reorders rows in ascending order (i.e.~lowest to highest). In the previous glimpse, countries are arranged in alphabetical order. Suppose I wanted them arranged based on GDP.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{euro_gdp07 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(gdp) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 30
## Columns: 3
## $ country <fct> Montenegro, Iceland, Albania, Bosnia and Herzegovina, Slove...
## $ year    <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,...
## $ gdp     <dbl> 6.336476, 10.924102, 21.376411, 33.897027, 51.774743, 65.68...
\end{verbatim}

Now we see a few countries with the lowest GDP. If instead I wanted GDP arranged from highest to lowest, then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{euro_gdp07 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(gdp)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 30
## Columns: 3
## $ country <fct> Germany, United Kingdom, France, Italy, Spain, Netherlands,...
## $ year    <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,...
## $ gdp     <dbl> 2650.87089, 2017.96931, 1861.22794, 1661.26443, 1165.75989,...
\end{verbatim}

Now we see some of the wealthiest European countries.

\hypertarget{headtail}{%
\subsubsection*{Head/Tail}\label{headtail}}
\addcontentsline{toc}{subsubsection}{Head/Tail}

By default, the \texttt{head} and \texttt{tail} verbs extract the top and bottom 6 rows of a dataset, respectively. These verbs are useful if we want to show a reader a sample of the data in a familiar spreadsheet form, which can be useful. Though the output from \texttt{glimpse} is very useful, it does not look good in a report. The \texttt{head} and \texttt{tail} verbs allow us to provide similar information in a much more presentable format.

Suppose we wanted to show a reader the three wealthiest and poorest European countries (in absolute terms). We can specify the number of rows \texttt{head} or \texttt{tail} extract using \texttt{n=\#}. Thus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{euro_gdp07 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(gdp)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DataTypeTok{n=}\DecValTok{3}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{format =} \StringTok{'html'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

country

year

gdp

Germany

2007

2650.871

United Kingdom

2007

2017.969

France

2007

1861.228

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{euro_gdp07 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(gdp) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DataTypeTok{n=}\DecValTok{3}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{format =} \StringTok{'html'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

country

year

gdp

Montenegro

2007

6.336476

Iceland

2007

10.924102

Albania

2007

21.376411

\hypertarget{summarize}{%
\subsubsection*{Summarize}\label{summarize}}
\addcontentsline{toc}{subsubsection}{Summarize}

Summarize creates a new dataset by collapsing all of the cases of a dataset into one or more summary statistics. It is useful for providing quick summary stat calculations in a somewhat presentable format. I do not recommend using \texttt{summarize} to produce the kind of summary stats table commonly found in reports because it can become tedious and the formatting is not good enough. I recommend using the \texttt{arsenal} package instead.

Suppose I wanted to report the average gdpPercap and lifeExp for 2007 in a rough and ready table. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(year }\OperatorTok{==}\StringTok{ }\DecValTok{2007}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\StringTok{'Average GDP per capita'}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(gdpPercap), }
            \StringTok{'Average life expectance'}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(lifeExp)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{format =} \StringTok{'html'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Average GDP per capita

Average life expectance

11680.07

67.00742

The \texttt{summarize} verb works with numerous summary functions listed on the second page of the \href{https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf}{Data transformation} cheatsheet under the heading ``Summary Functions.''

\hypertarget{group_by}{%
\subsubsection*{Group\_By}\label{group_by}}
\addcontentsline{toc}{subsubsection}{Group\_By}

The \texttt{group\_by} verb is most commonly used in tandem with \texttt{summarize}. If instead of calculating a summary stat for the entire dataset, you wanted to calculate the summary stat for each group of a categorical variable separately, use \texttt{group\_by} before using \texttt{summarize}.

Suppose I wanted average GDP per capita and life expectancy in 2007 for each continent. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(year }\OperatorTok{==}\StringTok{ }\DecValTok{2007}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(continent) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\StringTok{'Average GDP per capita'}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(gdpPercap), }
            \StringTok{'Average life expectance'}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(lifeExp)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{format =} \StringTok{'html'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` ungrouping output (override with `.groups` argument)
\end{verbatim}

continent

Average GDP per capita

Average life expectance

Africa

3089.033

54.80604

Americas

11003.032

73.60812

Asia

12473.027

70.72848

Europe

25054.482

77.64860

Oceania

29810.188

80.71950

Pretty powerful! You can also use multiple grouping variables. Suppose I wanted these summary stats for each continent each year since 1997. Then:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(year }\OperatorTok{>=}\StringTok{ }\DecValTok{1997}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(continent, year) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\StringTok{'Average GDP per capita'}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(gdpPercap), }
            \StringTok{'Average life expectance'}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(lifeExp)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{format =} \StringTok{'html'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` regrouping output by 'continent' (override with `.groups` argument)
\end{verbatim}

continent

year

Average GDP per capita

Average life expectance

Africa

1997

2378.760

53.59827

Africa

2002

2599.385

53.32523

Africa

2007

3089.033

54.80604

Americas

1997

8889.301

71.15048

Americas

2002

9287.677

72.42204

Americas

2007

11003.032

73.60812

Asia

1997

9834.093

68.02052

Asia

2002

10174.090

69.23388

Asia

2007

12473.027

70.72848

Europe

1997

19076.782

75.50517

Europe

2002

21711.732

76.70060

Europe

2007

25054.482

77.64860

Oceania

1997

24024.175

78.19000

Oceania

2002

26938.778

79.74000

Oceania

2007

29810.188

80.71950

\hypertarget{tidy-verbs}{%
\subsection{Tidy Verbs}\label{tidy-verbs}}

As with wrangling, one can encounter numerous different tidying scenarios. However, most of the time tidying involves converting a wide dataset to a long dataset. The most common untidy data one encounters is a time series or panel data where each time period is stored across columns (i.e.~wide) rather than down rows (i.e.~long).

Let's begin with a simple time series of population taken from the \texttt{gapminder} data. Suppose we downloaded a dataset named \texttt{uspop} for U.S. population.

country

1997

2002

2007

United States

272911760

287675526

301139947

We don't want each year to be a variable. Rather, we want year to be one variable with separate levels/rows for each period. We can achieve this with \texttt{pivot\_longer}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{uspop }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}\DataTypeTok{cols =} \StringTok{'1997'}\OperatorTok{:}\StringTok{'2007'}\NormalTok{, }
               \DataTypeTok{names_to =} \StringTok{'year'}\NormalTok{,}
               \DataTypeTok{values_to =} \StringTok{'pop'}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{format =} \StringTok{'html'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

country

year

pop

United States

1997

272911760

United States

2002

287675526

United States

2007

301139947

Note that \texttt{pivot\_longer} tries to make the code as intuitive as possible using natural language. First, we tell R which columns to pivot, then we tell R to name the new column `year', then we tell R to name the new column with the values for population `pop'.

Suppose we encountered a more difficult wide version of the \texttt{gapminder} data named \texttt{gap\_wide} shown below. This one has multiple variables listed wide for each year.

\hypertarget{htmlwidget-5b9a6b2b38f01f96e463}{}
\begin{datatables}

\end{datatables}

Tidying \texttt{gap\_wide} will take two steps. First, we can separate the variable names \texttt{pop/lifeExp/gdpPercap} from the numeric year into two columns using \texttt{pivot\_longer}. This will result in a column that contains all three variables that precede the year and a column that contains year. We will also need to name a third column that will contain the values that the pivoted columns contained.

In the code below, I tell R which columns to pivot using \texttt{cols} and to name the two new columns `var' and `year'. I use \texttt{names\_sep} to tell that each of the columns should be separated using the underscore. Then, I give the new column that will contain the values the generic name `value' since this is an temporary column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gap_long1 <-}\StringTok{ }\NormalTok{gap_wide }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}\DataTypeTok{cols =}\NormalTok{ pop_}\DecValTok{1997}\OperatorTok{:}\NormalTok{gdpPercap_}\DecValTok{2007}\NormalTok{,}
               \DataTypeTok{names_to =} \KeywordTok{c}\NormalTok{(}\StringTok{'var'}\NormalTok{, }\StringTok{'year'}\NormalTok{),}
               \DataTypeTok{names_sep =} \StringTok{'_'}\NormalTok{,}
               \DataTypeTok{values_to =} \StringTok{'value'}\NormalTok{)}

\NormalTok{DT}\OperatorTok{::}\KeywordTok{datatable}\NormalTok{(gap_long1, }\DataTypeTok{rownames =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{options =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{pageLength =} \DecValTok{5}\NormalTok{, }\DataTypeTok{scrollX=}\NormalTok{T))}
\end{Highlighting}
\end{Shaded}

\hypertarget{htmlwidget-7fc88bba3c0a6a827e77}{}
\begin{datatables}

\end{datatables}

Now we need to convert the \texttt{var} column to wide using \texttt{pivot\_wider}. This will create new columns for each of the unique values contained in the `var' column. Since there are three unique values, the result will be three new columns. We also need to specify which column contains the values that will be transferred over to the three new columns.

In the code below, I tell R to pivot the `var' column wide and take the values from the `value' column. And voila; we are back to having our original, tidy data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gap_long2 <-}\StringTok{ }\NormalTok{gap_long1 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{pivot_wider}\NormalTok{(}\DataTypeTok{names_from =}\NormalTok{ var,}
              \DataTypeTok{values_from =}\NormalTok{ value)}

\NormalTok{DT}\OperatorTok{::}\KeywordTok{datatable}\NormalTok{(gap_long2, }\DataTypeTok{rownames =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{options =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{pageLength =} \DecValTok{5}\NormalTok{, }\DataTypeTok{scrollX=}\NormalTok{T))}
\end{Highlighting}
\end{Shaded}

\hypertarget{htmlwidget-12eb49391c984bdcbaff}{}
\begin{datatables}

\end{datatables}

  \bibliography{book.bib,packages.bib}

\end{document}
